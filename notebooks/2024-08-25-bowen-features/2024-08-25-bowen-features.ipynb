{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring meta-analyzer features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bowen_df = pd.read_json(\"../../data/helpsteer2_train_tags.jsonl\", lines=True)\n",
    "input_df = pd.read_json(\n",
    "    \"../../data/helpsteer2_human_vs_gpt4_weighted_for_llama.jsonl\", lines=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe, instead of doing the data processing in the feature extractor, we just update the input dataset to include these tags (in terms of fields).\n",
    "However, there are things you need to check: are the tags act on the prompt only? Or does it include the response?\n",
    "Also, you need to find the code o generating the prompt_hash from the initial repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "bowen_df_normalized = pd.concat(\n",
    "    [bowen_df.drop(columns=[\"tags\"]), pd.json_normalize(bowen_df[\"tags\"])],\n",
    "    axis=1,\n",
    ")\n",
    "bowen_df_normalized[\"prompt_hash\"] = bowen_df_normalized[\"prompt\"].apply(\n",
    "    lambda x: hashlib.md5(x.encode(\"utf-8\")).hexdigest()\n",
    ")\n",
    "bowen_df_normalized = bowen_df_normalized.drop_duplicates(subset=[\"prompt_hash\"])\n",
    "# Remove unnecessary columns\n",
    "bowen_df_normalized = bowen_df_normalized.drop(\n",
    "    columns=[\n",
    "        \"response\",\n",
    "        \"helpfulness\",\n",
    "        \"correctness\",\n",
    "        \"coherence\",\n",
    "        \"complexity\",\n",
    "        \"verbosity\",\n",
    "    ]\n",
    ").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bowen_df_normalized.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df = input_df.merge(\n",
    "    bowen_df_normalized.drop(columns=[\"prompt\"]), how=\"left\", on=\"prompt_hash\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df.to_json(\n",
    "    \"helpsteer2_human_vs_gpt4_weighted_for_llama.jsonl\", lines=True, orient=\"records\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data types\n",
    "\n",
    "- `subject_of_expertise`: list[str]\n",
    "- `expertise_level`: str (3 labels) (basic domain knowledge, general public, expert domain knowledge)\n",
    "- `languages`: list[str]\n",
    "- `open_endedness`: str (4 labels?) (moderate, high, low, no)\n",
    "- `safety_concern`: str (safe, low, moderate, high)\n",
    "- `complexity_of_intents`: str (simple, moderate, complex, high)\n",
    "- `type_of_in_context_material`: list[str]\n",
    "- `format_constraints`: list[str]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the data type is a `list[str]`. The domains of this prompt is \"ICT\" and \"Sociology\". Our constraints should be something like: at least one of this domain falls under what we specified. Then perhaps there's a `strict` option, if we want that the domains are exactly what we specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_subjects = []\n",
    "for subjects in updated_df.subject_of_expertise.to_list():\n",
    "    for subject in all_subjects:\n",
    "        all_subjects.extend(subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_of_in_context_material = []\n",
    "for values in updated_df.type_of_in_context_material.to_list():\n",
    "    type_of_in_context_material.extend(values)\n",
    "\n",
    "\n",
    "format_constraints = []\n",
    "for values in updated_df.format_constraints.to_list():\n",
    "    if values:\n",
    "        format_constraints.extend(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
