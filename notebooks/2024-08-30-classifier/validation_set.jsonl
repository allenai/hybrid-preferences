{"hash":"e30383c5ba2d5942666b453a426a179e","model_type":"Seq. Classifier","chat_template":"tulu","bertscore__min_val=0.33|max_val=0.67":0.0,"bertscore__min_val=0.67|max_val=1.0":0.0,"bertscore_length__min_val=0.0|max_val=0.33":0.0,"bertscore_length__min_val=0.33|max_val=0.67":0.0,"bertscore_length__min_val=0.67|max_val=1.0":0.0,"complexity_of_intents=complex":0.0,"complexity_of_intents=moderate":0.0,"complexity_of_intents=simple":0.0,"cosine_sim__min_val=0.0|max_val=0.33":0.0,"cosine_sim__min_val=0.33|max_val=0.67":0.0,"cosine_sim__min_val=0.67|max_val=1.0":0.0,"entity_sim__min_val=0.0|max_val=0.33":0.0,"entity_sim__min_val=0.33|max_val=0.67":0.0,"entity_sim__min_val=0.67|max_val=1.0":0.0,"expertise_level=basic_domain_knowledge":0.0,"expertise_level=expert_domain_knowledge":0.0,"expertise_level=general_public":0.0,"format_constraints=1":0.282,"languages=English":0.0,"len_longer__min_val=0.0|max_val=0.33":0.0,"len_longer__min_val=0.33|max_val=0.67":0.0,"len_longer__min_val=0.67|max_val=1.0":0.0,"len_shorter__min_val=0.0|max_val=0.33":0.0,"len_shorter__min_val=0.33|max_val=0.67":0.0,"len_shorter__min_val=0.67|max_val=1.0":0.0,"open_endedness=high":0.0,"open_endedness=low":0.0,"open_endedness=moderate":0.0,"open_endedness=no":0.0,"prompt_len__min_val=0.0|max_val=0.33":0.0,"prompt_len__min_val=0.33|max_val=0.67":0.0,"prompt_len__min_val=0.67|max_val=1.0":0.0,"rouge__min_val=0.0|max_val=0.33":0.0,"rouge__min_val=0.33|max_val=0.67":0.0,"rouge__min_val=0.67|max_val=1.0":0.0,"safety_concern=high":0.0,"safety_concern=low":0.0,"safety_concern=moderate":0,"safety_concern=safe":0.0,"subject_of_expertise=Agriculture":0.0,"subject_of_expertise=Business":0.0,"subject_of_expertise=Computer_sciences":0,"subject_of_expertise=Earth_sciences":0,"subject_of_expertise=Economics":0,"subject_of_expertise=Electrical_engineering":0,"subject_of_expertise=Family_and_consumer_science":0,"subject_of_expertise=Geography":0.0,"subject_of_expertise=Human_physical_performance_and_recreation":0.0,"subject_of_expertise=Linguistics_and_language":0.0,"subject_of_expertise=Literature":0,"subject_of_expertise=Materials_science_and_engineering":0.0,"subject_of_expertise=Mechanical_engineering":0,"subject_of_expertise=Media_studies_and_communication":0,"subject_of_expertise=Medicine_and_health":0.0,"subject_of_expertise=Military_sciences":0.0,"subject_of_expertise=Philosophy":0,"subject_of_expertise=Political_science":0.0,"subject_of_expertise=Social_work":0.0,"subject_of_expertise=Sociology":0,"subject_of_expertise=Space_sciences":0,"subject_of_expertise=System_science":0,"token_len_difference__min_val=0.0|max_val=0.33":0.0,"token_len_difference__min_val=0.33|max_val=0.67":0.0,"token_len_difference__min_val=0.67|max_val=1.0":0.0,"type_of_in_context_material=1":0.0,"num_swaps":1974,"Chat":0.8268156425,"Chat Hard":0.6381578947,"Safety":0.5700831169,"Reasoning":0.7956737327,"Overall":0.7076825967,"alpacaeval-easy":0.81,"alpacaeval-hard":0.8,"alpacaeval-length":0.8315789474,"donotanswer":0.5294117647,"hep-cpp":0.7682926829,"hep-go":0.737804878,"hep-java":0.8170731707,"hep-js":0.8109756098,"hep-python":0.7865853659,"hep-rust":0.7682926829,"llmbar-adver-GPTInst":0.6413043478,"llmbar-adver-GPTOut":0.4042553191,"llmbar-adver-manual":0.5869565217,"llmbar-adver-neighbor":0.6492537313,"llmbar-natural":0.76,"math-prm":0.8098434004,"mt-bench-easy":0.8928571429,"mt-bench-hard":0.6216216216,"mt-bench-med":0.875,"refusals-dangerous":0.46,"refusals-offensive":0.35,"xstest-should-refuse":0.525974026,"xstest-should-respond":0.892,"label":1}
{"hash":"d4976d93b68703bb4874cbafdbb998cc","model_type":"Seq. Classifier","chat_template":"tulu","bertscore__min_val=0.33|max_val=0.67":0.0,"bertscore__min_val=0.67|max_val=1.0":0.0,"bertscore_length__min_val=0.0|max_val=0.33":0.0,"bertscore_length__min_val=0.33|max_val=0.67":0.0,"bertscore_length__min_val=0.67|max_val=1.0":0.2097142857,"complexity_of_intents=complex":0.0,"complexity_of_intents=moderate":0.0,"complexity_of_intents=simple":0.0,"cosine_sim__min_val=0.0|max_val=0.33":0.0538571429,"cosine_sim__min_val=0.33|max_val=0.67":0.0,"cosine_sim__min_val=0.67|max_val=1.0":0.0,"entity_sim__min_val=0.0|max_val=0.33":0.0,"entity_sim__min_val=0.33|max_val=0.67":0.2141428571,"entity_sim__min_val=0.67|max_val=1.0":0.0,"expertise_level=basic_domain_knowledge":0.0,"expertise_level=expert_domain_knowledge":0.0,"expertise_level=general_public":0.0,"format_constraints=1":0.0,"languages=English":0.0,"len_longer__min_val=0.0|max_val=0.33":0.0,"len_longer__min_val=0.33|max_val=0.67":0.0,"len_longer__min_val=0.67|max_val=1.0":0.0,"len_shorter__min_val=0.0|max_val=0.33":0.0,"len_shorter__min_val=0.33|max_val=0.67":0.0,"len_shorter__min_val=0.67|max_val=1.0":0.001,"open_endedness=high":0.0,"open_endedness=low":0.0,"open_endedness=moderate":0.0,"open_endedness=no":0.0,"prompt_len__min_val=0.0|max_val=0.33":0.0,"prompt_len__min_val=0.33|max_val=0.67":0.0,"prompt_len__min_val=0.67|max_val=1.0":0.0,"rouge__min_val=0.0|max_val=0.33":0.2812857143,"rouge__min_val=0.33|max_val=0.67":0.0,"rouge__min_val=0.67|max_val=1.0":0.0,"safety_concern=high":0.0,"safety_concern=low":0.0,"safety_concern=moderate":0,"safety_concern=safe":0.0,"subject_of_expertise=Agriculture":0.0,"subject_of_expertise=Business":0.0,"subject_of_expertise=Computer_sciences":0,"subject_of_expertise=Earth_sciences":0,"subject_of_expertise=Economics":0,"subject_of_expertise=Electrical_engineering":0,"subject_of_expertise=Family_and_consumer_science":0,"subject_of_expertise=Geography":0.0,"subject_of_expertise=Human_physical_performance_and_recreation":0.0,"subject_of_expertise=Linguistics_and_language":0.0,"subject_of_expertise=Literature":0,"subject_of_expertise=Materials_science_and_engineering":0.0,"subject_of_expertise=Mechanical_engineering":0,"subject_of_expertise=Media_studies_and_communication":0,"subject_of_expertise=Medicine_and_health":0.0,"subject_of_expertise=Military_sciences":0.0,"subject_of_expertise=Philosophy":0,"subject_of_expertise=Political_science":0.0,"subject_of_expertise=Social_work":0.0,"subject_of_expertise=Sociology":0,"subject_of_expertise=Space_sciences":0,"subject_of_expertise=System_science":0,"token_len_difference__min_val=0.0|max_val=0.33":0.0,"token_len_difference__min_val=0.33|max_val=0.67":0.0,"token_len_difference__min_val=0.67|max_val=1.0":0.0,"type_of_in_context_material=1":0.0,"num_swaps":1,"Chat":0.7653631285,"Chat Hard":0.6293859649,"Safety":0.4495121095,"Reasoning":0.81326049,"Overall":0.6643804232,"alpacaeval-easy":0.82,"alpacaeval-hard":0.7263157895,"alpacaeval-length":0.7157894737,"donotanswer":0.4779411765,"hep-cpp":0.7743902439,"hep-go":0.7987804878,"hep-java":0.7804878049,"hep-js":0.7987804878,"hep-python":0.8231707317,"hep-rust":0.8170731707,"llmbar-adver-GPTInst":0.6086956522,"llmbar-adver-GPTOut":0.4255319149,"llmbar-adver-manual":0.5869565217,"llmbar-adver-neighbor":0.6194029851,"llmbar-natural":0.78,"math-prm":0.8277404922,"mt-bench-easy":0.8571428571,"mt-bench-hard":0.6216216216,"mt-bench-med":0.775,"refusals-dangerous":0.18,"refusals-offensive":0.25,"xstest-should-refuse":0.3441558442,"xstest-should-respond":0.9,"label":1}
{"hash":"07ecf2a9caa729c6bd854596b88c2dbc","model_type":"Seq. Classifier","chat_template":"tulu","bertscore__min_val=0.33|max_val=0.67":0.0,"bertscore__min_val=0.67|max_val=1.0":0.0,"bertscore_length__min_val=0.0|max_val=0.33":0.0,"bertscore_length__min_val=0.33|max_val=0.67":0.0,"bertscore_length__min_val=0.67|max_val=1.0":0.0,"complexity_of_intents=complex":0.0,"complexity_of_intents=moderate":0.0,"complexity_of_intents=simple":0.0,"cosine_sim__min_val=0.0|max_val=0.33":0.0,"cosine_sim__min_val=0.33|max_val=0.67":0.0,"cosine_sim__min_val=0.67|max_val=1.0":0.7087142857,"entity_sim__min_val=0.0|max_val=0.33":0.0,"entity_sim__min_val=0.33|max_val=0.67":0.0,"entity_sim__min_val=0.67|max_val=1.0":0.2818571429,"expertise_level=basic_domain_knowledge":0.0,"expertise_level=expert_domain_knowledge":0.0,"expertise_level=general_public":0.0,"format_constraints=1":0.0,"languages=English":0.9985714286,"len_longer__min_val=0.0|max_val=0.33":0.0,"len_longer__min_val=0.33|max_val=0.67":0.0,"len_longer__min_val=0.67|max_val=1.0":0.0,"len_shorter__min_val=0.0|max_val=0.33":0.0,"len_shorter__min_val=0.33|max_val=0.67":0.0,"len_shorter__min_val=0.67|max_val=1.0":0.0,"open_endedness=high":0.0,"open_endedness=low":0.0,"open_endedness=moderate":0.0,"open_endedness=no":0.0,"prompt_len__min_val=0.0|max_val=0.33":0.0,"prompt_len__min_val=0.33|max_val=0.67":0.0,"prompt_len__min_val=0.67|max_val=1.0":0.0,"rouge__min_val=0.0|max_val=0.33":0.0,"rouge__min_val=0.33|max_val=0.67":0.6908571429,"rouge__min_val=0.67|max_val=1.0":0.0,"safety_concern=high":0.0,"safety_concern=low":0.0,"safety_concern=moderate":0,"safety_concern=safe":0.0,"subject_of_expertise=Agriculture":0.0,"subject_of_expertise=Business":0.0,"subject_of_expertise=Computer_sciences":0,"subject_of_expertise=Earth_sciences":0,"subject_of_expertise=Economics":0,"subject_of_expertise=Electrical_engineering":0,"subject_of_expertise=Family_and_consumer_science":0,"subject_of_expertise=Geography":0.0,"subject_of_expertise=Human_physical_performance_and_recreation":0.0,"subject_of_expertise=Linguistics_and_language":0.0,"subject_of_expertise=Literature":0,"subject_of_expertise=Materials_science_and_engineering":0.0,"subject_of_expertise=Mechanical_engineering":0,"subject_of_expertise=Media_studies_and_communication":0,"subject_of_expertise=Medicine_and_health":0.0,"subject_of_expertise=Military_sciences":0.0,"subject_of_expertise=Philosophy":0,"subject_of_expertise=Political_science":0.0,"subject_of_expertise=Social_work":0.0,"subject_of_expertise=Sociology":0,"subject_of_expertise=Space_sciences":0,"subject_of_expertise=System_science":0,"token_len_difference__min_val=0.0|max_val=0.33":0.0,"token_len_difference__min_val=0.33|max_val=0.67":0.0,"token_len_difference__min_val=0.67|max_val=1.0":0.0,"type_of_in_context_material=1":0.0,"num_swaps":1134,"Chat":0.7793296089,"Chat Hard":0.6315789474,"Safety":0.4652791857,"Reasoning":0.8440756807,"Overall":0.6800658557,"alpacaeval-easy":0.74,"alpacaeval-hard":0.7473684211,"alpacaeval-length":0.7789473684,"donotanswer":0.5147058824,"hep-cpp":0.8048780488,"hep-go":0.7682926829,"hep-java":0.7804878049,"hep-js":0.7865853659,"hep-python":0.7926829268,"hep-rust":0.7865853659,"llmbar-adver-GPTInst":0.5652173913,"llmbar-adver-GPTOut":0.4042553191,"llmbar-adver-manual":0.6304347826,"llmbar-adver-neighbor":0.6417910448,"llmbar-natural":0.81,"math-prm":0.9015659955,"mt-bench-easy":0.9285714286,"mt-bench-hard":0.5675675676,"mt-bench-med":0.85,"refusals-dangerous":0.19,"refusals-offensive":0.23,"xstest-should-refuse":0.3896103896,"xstest-should-respond":0.876,"label":1}
{"hash":"5cb0c855815eadd33f80f446110f7f0f","model_type":"Seq. Classifier","chat_template":"tulu","bertscore__min_val=0.33|max_val=0.67":0.0,"bertscore__min_val=0.67|max_val=1.0":0.0,"bertscore_length__min_val=0.0|max_val=0.33":0.0,"bertscore_length__min_val=0.33|max_val=0.67":0.0,"bertscore_length__min_val=0.67|max_val=1.0":0.0,"complexity_of_intents=complex":0.0941428571,"complexity_of_intents=moderate":0.0,"complexity_of_intents=simple":0.0,"cosine_sim__min_val=0.0|max_val=0.33":0.0,"cosine_sim__min_val=0.33|max_val=0.67":0.0,"cosine_sim__min_val=0.67|max_val=1.0":0.0,"entity_sim__min_val=0.0|max_val=0.33":0.0,"entity_sim__min_val=0.33|max_val=0.67":0.0,"entity_sim__min_val=0.67|max_val=1.0":0.0,"expertise_level=basic_domain_knowledge":0.0,"expertise_level=expert_domain_knowledge":0.0,"expertise_level=general_public":0.0,"format_constraints=1":0.0,"languages=English":0.0,"len_longer__min_val=0.0|max_val=0.33":0.0,"len_longer__min_val=0.33|max_val=0.67":0.0,"len_longer__min_val=0.67|max_val=1.0":0.0,"len_shorter__min_val=0.0|max_val=0.33":0.0,"len_shorter__min_val=0.33|max_val=0.67":0.0,"len_shorter__min_val=0.67|max_val=1.0":0.0,"open_endedness=high":0.0,"open_endedness=low":0.0,"open_endedness=moderate":0.0,"open_endedness=no":0.0,"prompt_len__min_val=0.0|max_val=0.33":0.0,"prompt_len__min_val=0.33|max_val=0.67":0.0,"prompt_len__min_val=0.67|max_val=1.0":0.0,"rouge__min_val=0.0|max_val=0.33":0.0,"rouge__min_val=0.33|max_val=0.67":0.0,"rouge__min_val=0.67|max_val=1.0":0.0,"safety_concern=high":0.0,"safety_concern=low":0.0,"safety_concern=moderate":0,"safety_concern=safe":0.0,"subject_of_expertise=Agriculture":0.0,"subject_of_expertise=Business":0.0,"subject_of_expertise=Computer_sciences":0,"subject_of_expertise=Earth_sciences":0,"subject_of_expertise=Economics":0,"subject_of_expertise=Electrical_engineering":0,"subject_of_expertise=Family_and_consumer_science":0,"subject_of_expertise=Geography":0.0,"subject_of_expertise=Human_physical_performance_and_recreation":0.0142857143,"subject_of_expertise=Linguistics_and_language":0.0,"subject_of_expertise=Literature":0,"subject_of_expertise=Materials_science_and_engineering":0.0,"subject_of_expertise=Mechanical_engineering":0,"subject_of_expertise=Media_studies_and_communication":0,"subject_of_expertise=Medicine_and_health":0.0,"subject_of_expertise=Military_sciences":0.0,"subject_of_expertise=Philosophy":0,"subject_of_expertise=Political_science":0.0,"subject_of_expertise=Social_work":0.0,"subject_of_expertise=Sociology":0,"subject_of_expertise=Space_sciences":0,"subject_of_expertise=System_science":0,"token_len_difference__min_val=0.0|max_val=0.33":0.0,"token_len_difference__min_val=0.33|max_val=0.67":0.0,"token_len_difference__min_val=0.67|max_val=1.0":0.0,"type_of_in_context_material=1":0.0,"num_swaps":10,"Chat":0.6927374302,"Chat Hard":0.6688596491,"Safety":0.5442993331,"Reasoning":0.8624672614,"Overall":0.6920909185,"alpacaeval-easy":0.71,"alpacaeval-hard":0.6526315789,"alpacaeval-length":0.6105263158,"donotanswer":0.5808823529,"hep-cpp":0.8231707317,"hep-go":0.7926829268,"hep-java":0.8902439024,"hep-js":0.8292682927,"hep-python":0.8170731707,"hep-rust":0.8414634146,"llmbar-adver-GPTInst":0.6847826087,"llmbar-adver-GPTOut":0.4468085106,"llmbar-adver-manual":0.6304347826,"llmbar-adver-neighbor":0.6791044776,"llmbar-natural":0.79,"math-prm":0.8926174497,"mt-bench-easy":0.8571428571,"mt-bench-hard":0.5945945946,"mt-bench-med":0.825,"refusals-dangerous":0.29,"refusals-offensive":0.29,"xstest-should-refuse":0.525974026,"xstest-should-respond":0.872,"label":1}
{"hash":"de29f569e4985f516b5d4a43f62003fc","model_type":"Seq. Classifier","chat_template":"tulu","bertscore__min_val=0.33|max_val=0.67":0.0,"bertscore__min_val=0.67|max_val=1.0":0.0,"bertscore_length__min_val=0.0|max_val=0.33":0.0,"bertscore_length__min_val=0.33|max_val=0.67":0.0,"bertscore_length__min_val=0.67|max_val=1.0":0.0,"complexity_of_intents=complex":0.0,"complexity_of_intents=moderate":0.0,"complexity_of_intents=simple":0.0,"cosine_sim__min_val=0.0|max_val=0.33":0.0,"cosine_sim__min_val=0.33|max_val=0.67":0.0,"cosine_sim__min_val=0.67|max_val=1.0":0.0,"entity_sim__min_val=0.0|max_val=0.33":0.0,"entity_sim__min_val=0.33|max_val=0.67":0.0,"entity_sim__min_val=0.67|max_val=1.0":0.0,"expertise_level=basic_domain_knowledge":0.0,"expertise_level=expert_domain_knowledge":0.0,"expertise_level=general_public":0.0,"format_constraints=1":0.2772857143,"languages=English":0.9984285714,"len_longer__min_val=0.0|max_val=0.33":0.0,"len_longer__min_val=0.33|max_val=0.67":0.0,"len_longer__min_val=0.67|max_val=1.0":0.0,"len_shorter__min_val=0.0|max_val=0.33":0.0,"len_shorter__min_val=0.33|max_val=0.67":0.0,"len_shorter__min_val=0.67|max_val=1.0":0.0,"open_endedness=high":0.0,"open_endedness=low":0.0,"open_endedness=moderate":0.0,"open_endedness=no":0.0,"prompt_len__min_val=0.0|max_val=0.33":0.0,"prompt_len__min_val=0.33|max_val=0.67":0.0,"prompt_len__min_val=0.67|max_val=1.0":0.0,"rouge__min_val=0.0|max_val=0.33":0.0,"rouge__min_val=0.33|max_val=0.67":0.0,"rouge__min_val=0.67|max_val=1.0":0.0,"safety_concern=high":0.0,"safety_concern=low":0.0,"safety_concern=moderate":0,"safety_concern=safe":0.0,"subject_of_expertise=Agriculture":0.0,"subject_of_expertise=Business":0.0,"subject_of_expertise=Computer_sciences":0,"subject_of_expertise=Earth_sciences":0,"subject_of_expertise=Economics":0,"subject_of_expertise=Electrical_engineering":0,"subject_of_expertise=Family_and_consumer_science":0,"subject_of_expertise=Geography":0.0,"subject_of_expertise=Human_physical_performance_and_recreation":0.0,"subject_of_expertise=Linguistics_and_language":0.0,"subject_of_expertise=Literature":0,"subject_of_expertise=Materials_science_and_engineering":0.0,"subject_of_expertise=Mechanical_engineering":0,"subject_of_expertise=Media_studies_and_communication":0,"subject_of_expertise=Medicine_and_health":0.0,"subject_of_expertise=Military_sciences":0.0,"subject_of_expertise=Philosophy":0,"subject_of_expertise=Political_science":0.0,"subject_of_expertise=Social_work":0.0,"subject_of_expertise=Sociology":0,"subject_of_expertise=Space_sciences":0,"subject_of_expertise=System_science":0,"token_len_difference__min_val=0.0|max_val=0.33":0.0,"token_len_difference__min_val=0.33|max_val=0.67":0.0,"token_len_difference__min_val=0.67|max_val=1.0":0.0,"type_of_in_context_material=1":0.0,"num_swaps":1938,"Chat":0.7625698324,"Chat Hard":0.6315789474,"Safety":0.5848422604,"Reasoning":0.8125613848,"Overall":0.6978881063,"alpacaeval-easy":0.77,"alpacaeval-hard":0.7894736842,"alpacaeval-length":0.6947368421,"donotanswer":0.4705882353,"hep-cpp":0.7317073171,"hep-go":0.743902439,"hep-java":0.762195122,"hep-js":0.756097561,"hep-python":0.7682926829,"hep-rust":0.7804878049,"llmbar-adver-GPTInst":0.597826087,"llmbar-adver-GPTOut":0.4680851064,"llmbar-adver-manual":0.6086956522,"llmbar-adver-neighbor":0.6492537313,"llmbar-natural":0.75,"math-prm":0.8680089485,"mt-bench-easy":0.8214285714,"mt-bench-hard":0.5675675676,"mt-bench-med":0.8,"refusals-dangerous":0.48,"refusals-offensive":0.4,"xstest-should-refuse":0.5909090909,"xstest-should-respond":0.864,"label":1}
{"hash":"098dc0471dde428b991422a97513168e","model_type":"Seq. Classifier","chat_template":"tulu","bertscore__min_val=0.33|max_val=0.67":0.0,"bertscore__min_val=0.67|max_val=1.0":0.0,"bertscore_length__min_val=0.0|max_val=0.33":0.4251428571,"bertscore_length__min_val=0.33|max_val=0.67":0.0,"bertscore_length__min_val=0.67|max_val=1.0":0.0,"complexity_of_intents=complex":0.0,"complexity_of_intents=moderate":0.0,"complexity_of_intents=simple":0.0,"cosine_sim__min_val=0.0|max_val=0.33":0.0,"cosine_sim__min_val=0.33|max_val=0.67":0.0,"cosine_sim__min_val=0.67|max_val=1.0":0.0,"entity_sim__min_val=0.0|max_val=0.33":0.0,"entity_sim__min_val=0.33|max_val=0.67":0.0,"entity_sim__min_val=0.67|max_val=1.0":0.0,"expertise_level=basic_domain_knowledge":0.0,"expertise_level=expert_domain_knowledge":0.0,"expertise_level=general_public":0.0,"format_constraints=1":0.0,"languages=English":0.0,"len_longer__min_val=0.0|max_val=0.33":0.0,"len_longer__min_val=0.33|max_val=0.67":0.0,"len_longer__min_val=0.67|max_val=1.0":0.0,"len_shorter__min_val=0.0|max_val=0.33":0.0,"len_shorter__min_val=0.33|max_val=0.67":0.0,"len_shorter__min_val=0.67|max_val=1.0":0.0012857143,"open_endedness=high":0.0,"open_endedness=low":0.0,"open_endedness=moderate":0.0,"open_endedness=no":0.0,"prompt_len__min_val=0.0|max_val=0.33":0.0,"prompt_len__min_val=0.33|max_val=0.67":0.0,"prompt_len__min_val=0.67|max_val=1.0":0.0008571429,"rouge__min_val=0.0|max_val=0.33":0.0,"rouge__min_val=0.33|max_val=0.67":0.6891428571,"rouge__min_val=0.67|max_val=1.0":0.0,"safety_concern=high":0.0,"safety_concern=low":0.0,"safety_concern=moderate":0,"safety_concern=safe":0.0,"subject_of_expertise=Agriculture":0.0,"subject_of_expertise=Business":0.0,"subject_of_expertise=Computer_sciences":0,"subject_of_expertise=Earth_sciences":0,"subject_of_expertise=Economics":0,"subject_of_expertise=Electrical_engineering":0,"subject_of_expertise=Family_and_consumer_science":0,"subject_of_expertise=Geography":0.0,"subject_of_expertise=Human_physical_performance_and_recreation":0.0,"subject_of_expertise=Linguistics_and_language":0.0,"subject_of_expertise=Literature":0,"subject_of_expertise=Materials_science_and_engineering":0.0,"subject_of_expertise=Mechanical_engineering":0,"subject_of_expertise=Media_studies_and_communication":0,"subject_of_expertise=Medicine_and_health":0.0,"subject_of_expertise=Military_sciences":0.0,"subject_of_expertise=Philosophy":0,"subject_of_expertise=Political_science":0.0,"subject_of_expertise=Social_work":0.0,"subject_of_expertise=Sociology":0,"subject_of_expertise=Space_sciences":0,"subject_of_expertise=System_science":0,"token_len_difference__min_val=0.0|max_val=0.33":0.0,"token_len_difference__min_val=0.33|max_val=0.67":0.0,"token_len_difference__min_val=0.67|max_val=1.0":0.0,"type_of_in_context_material=1":0.0,"num_swaps":1222,"Chat":0.7681564246,"Chat Hard":0.625,"Safety":0.4647602668,"Reasoning":0.8463094069,"Overall":0.6760565246,"alpacaeval-easy":0.76,"alpacaeval-hard":0.7789473684,"alpacaeval-length":0.7157894737,"donotanswer":0.5147058824,"hep-cpp":0.7804878049,"hep-go":0.7682926829,"hep-java":0.8170731707,"hep-js":0.8292682927,"hep-python":0.8109756098,"hep-rust":0.7804878049,"llmbar-adver-GPTInst":0.6304347826,"llmbar-adver-GPTOut":0.4042553191,"llmbar-adver-manual":0.5869565217,"llmbar-adver-neighbor":0.6044776119,"llmbar-natural":0.76,"math-prm":0.8948545861,"mt-bench-easy":0.8571428571,"mt-bench-hard":0.6486486486,"mt-bench-med":0.825,"refusals-dangerous":0.21,"refusals-offensive":0.2,"xstest-should-refuse":0.3896103896,"xstest-should-respond":0.88,"label":1}
{"hash":"3421c61f28da40ff906276bad8240caa","model_type":"Seq. Classifier","chat_template":"tulu","bertscore__min_val=0.33|max_val=0.67":0.0,"bertscore__min_val=0.67|max_val=1.0":0.9675714286,"bertscore_length__min_val=0.0|max_val=0.33":0.0,"bertscore_length__min_val=0.33|max_val=0.67":0.0,"bertscore_length__min_val=0.67|max_val=1.0":0.0,"complexity_of_intents=complex":0.0,"complexity_of_intents=moderate":0.0,"complexity_of_intents=simple":0.0,"cosine_sim__min_val=0.0|max_val=0.33":0.0568571429,"cosine_sim__min_val=0.33|max_val=0.67":0.0,"cosine_sim__min_val=0.67|max_val=1.0":0.0,"entity_sim__min_val=0.0|max_val=0.33":0.0,"entity_sim__min_val=0.33|max_val=0.67":0.0,"entity_sim__min_val=0.67|max_val=1.0":0.0,"expertise_level=basic_domain_knowledge":0.0,"expertise_level=expert_domain_knowledge":0.0,"expertise_level=general_public":0.0,"format_constraints=1":0.0,"languages=English":0.0,"len_longer__min_val=0.0|max_val=0.33":0.0,"len_longer__min_val=0.33|max_val=0.67":0.0,"len_longer__min_val=0.67|max_val=1.0":0.0,"len_shorter__min_val=0.0|max_val=0.33":0.0,"len_shorter__min_val=0.33|max_val=0.67":0.0,"len_shorter__min_val=0.67|max_val=1.0":0.0,"open_endedness=high":0.0,"open_endedness=low":0.0,"open_endedness=moderate":0.0,"open_endedness=no":0.0,"prompt_len__min_val=0.0|max_val=0.33":0.0,"prompt_len__min_val=0.33|max_val=0.67":0.0,"prompt_len__min_val=0.67|max_val=1.0":0.0,"rouge__min_val=0.0|max_val=0.33":0.0,"rouge__min_val=0.33|max_val=0.67":0.0,"rouge__min_val=0.67|max_val=1.0":0.0,"safety_concern=high":0.0,"safety_concern=low":0.0,"safety_concern=moderate":0,"safety_concern=safe":0.0,"subject_of_expertise=Agriculture":0.0,"subject_of_expertise=Business":0.0,"subject_of_expertise=Computer_sciences":0,"subject_of_expertise=Earth_sciences":0,"subject_of_expertise=Economics":0,"subject_of_expertise=Electrical_engineering":0,"subject_of_expertise=Family_and_consumer_science":0,"subject_of_expertise=Geography":0.0,"subject_of_expertise=Human_physical_performance_and_recreation":0.0,"subject_of_expertise=Linguistics_and_language":0.0,"subject_of_expertise=Literature":0,"subject_of_expertise=Materials_science_and_engineering":0.0,"subject_of_expertise=Mechanical_engineering":0,"subject_of_expertise=Media_studies_and_communication":0,"subject_of_expertise=Medicine_and_health":0.0,"subject_of_expertise=Military_sciences":0.0,"subject_of_expertise=Philosophy":0,"subject_of_expertise=Political_science":0.0,"subject_of_expertise=Social_work":0.0,"subject_of_expertise=Sociology":0,"subject_of_expertise=Space_sciences":0,"subject_of_expertise=System_science":0,"token_len_difference__min_val=0.0|max_val=0.33":0.0018571429,"token_len_difference__min_val=0.33|max_val=0.67":0.0,"token_len_difference__min_val=0.67|max_val=1.0":0.0,"type_of_in_context_material=1":0.0,"num_swaps":252,"Chat":0.8100558659,"Chat Hard":0.6162280702,"Safety":0.4603911548,"Reasoning":0.8348133901,"Overall":0.6803721202,"alpacaeval-easy":0.84,"alpacaeval-hard":0.7789473684,"alpacaeval-length":0.7789473684,"donotanswer":0.5294117647,"hep-cpp":0.8353658537,"hep-go":0.8170731707,"hep-java":0.8109756098,"hep-js":0.8048780488,"hep-python":0.8292682927,"hep-rust":0.7926829268,"llmbar-adver-GPTInst":0.5760869565,"llmbar-adver-GPTOut":0.4893617021,"llmbar-adver-manual":0.5869565217,"llmbar-adver-neighbor":0.5373134328,"llmbar-natural":0.82,"math-prm":0.8545861298,"mt-bench-easy":0.9285714286,"mt-bench-hard":0.6486486486,"mt-bench-med":0.8,"refusals-dangerous":0.3,"refusals-offensive":0.15,"xstest-should-refuse":0.3181818182,"xstest-should-respond":0.936,"label":1}
{"hash":"cd4f92195657a925382da9805f6d54f1","model_type":"Seq. Classifier","chat_template":"tulu","bertscore__min_val=0.33|max_val=0.67":0.0,"bertscore__min_val=0.67|max_val=1.0":0.0,"bertscore_length__min_val=0.0|max_val=0.33":0.0,"bertscore_length__min_val=0.33|max_val=0.67":0.0,"bertscore_length__min_val=0.67|max_val=1.0":0.0,"complexity_of_intents=complex":0.0,"complexity_of_intents=moderate":0.1885714286,"complexity_of_intents=simple":0.0,"cosine_sim__min_val=0.0|max_val=0.33":0.0,"cosine_sim__min_val=0.33|max_val=0.67":0.0,"cosine_sim__min_val=0.67|max_val=1.0":0.0,"entity_sim__min_val=0.0|max_val=0.33":0.0,"entity_sim__min_val=0.33|max_val=0.67":0.0,"entity_sim__min_val=0.67|max_val=1.0":0.0,"expertise_level=basic_domain_knowledge":0.0,"expertise_level=expert_domain_knowledge":0.1071428571,"expertise_level=general_public":0.0,"format_constraints=1":0.0,"languages=English":0.0,"len_longer__min_val=0.0|max_val=0.33":0.0,"len_longer__min_val=0.33|max_val=0.67":0.0,"len_longer__min_val=0.67|max_val=1.0":0.0,"len_shorter__min_val=0.0|max_val=0.33":0.0,"len_shorter__min_val=0.33|max_val=0.67":0.0,"len_shorter__min_val=0.67|max_val=1.0":0.0,"open_endedness=high":0.0,"open_endedness=low":0.0,"open_endedness=moderate":0.0,"open_endedness=no":0.0,"prompt_len__min_val=0.0|max_val=0.33":0.0,"prompt_len__min_val=0.33|max_val=0.67":0.0,"prompt_len__min_val=0.67|max_val=1.0":0.0,"rouge__min_val=0.0|max_val=0.33":0.0,"rouge__min_val=0.33|max_val=0.67":0.0,"rouge__min_val=0.67|max_val=1.0":0.0,"safety_concern=high":0.0,"safety_concern=low":0.0,"safety_concern=moderate":0,"safety_concern=safe":0.0,"subject_of_expertise=Agriculture":0.0,"subject_of_expertise=Business":0.0,"subject_of_expertise=Computer_sciences":0,"subject_of_expertise=Earth_sciences":0,"subject_of_expertise=Economics":0,"subject_of_expertise=Electrical_engineering":0,"subject_of_expertise=Family_and_consumer_science":0,"subject_of_expertise=Geography":0.0,"subject_of_expertise=Human_physical_performance_and_recreation":0.0,"subject_of_expertise=Linguistics_and_language":0.0,"subject_of_expertise=Literature":0,"subject_of_expertise=Materials_science_and_engineering":0.0,"subject_of_expertise=Mechanical_engineering":0,"subject_of_expertise=Media_studies_and_communication":0,"subject_of_expertise=Medicine_and_health":0.0,"subject_of_expertise=Military_sciences":0.0,"subject_of_expertise=Philosophy":0,"subject_of_expertise=Political_science":0.0,"subject_of_expertise=Social_work":0.0,"subject_of_expertise=Sociology":0,"subject_of_expertise=Space_sciences":0,"subject_of_expertise=System_science":0,"token_len_difference__min_val=0.0|max_val=0.33":0.0,"token_len_difference__min_val=0.33|max_val=0.67":0.0,"token_len_difference__min_val=0.67|max_val=1.0":0.0,"type_of_in_context_material=1":0.0,"num_swaps":138,"Chat":0.7206703911,"Chat Hard":0.6337719298,"Safety":0.3798831871,"Reasoning":0.8287226496,"Overall":0.6407620394,"alpacaeval-easy":0.72,"alpacaeval-hard":0.6842105263,"alpacaeval-length":0.6947368421,"donotanswer":0.4705882353,"hep-cpp":0.7804878049,"hep-go":0.7317073171,"hep-java":0.8475609756,"hep-js":0.7804878049,"hep-python":0.7682926829,"hep-rust":0.7743902439,"llmbar-adver-GPTInst":0.6195652174,"llmbar-adver-GPTOut":0.4680851064,"llmbar-adver-manual":0.6086956522,"llmbar-adver-neighbor":0.6044776119,"llmbar-natural":0.75,"math-prm":0.8769574944,"mt-bench-easy":0.8571428571,"mt-bench-hard":0.7027027027,"mt-bench-med":0.775,"refusals-dangerous":0.15,"refusals-offensive":0.1,"xstest-should-refuse":0.2337662338,"xstest-should-respond":0.868,"label":0}
{"hash":"9ad7674524326668fd2767e9e4597be8","model_type":"Seq. Classifier","chat_template":"tulu","bertscore__min_val=0.33|max_val=0.67":0.0,"bertscore__min_val=0.67|max_val=1.0":0.0,"bertscore_length__min_val=0.0|max_val=0.33":0.0,"bertscore_length__min_val=0.33|max_val=0.67":0.3672857143,"bertscore_length__min_val=0.67|max_val=1.0":0.0,"complexity_of_intents=complex":0.0,"complexity_of_intents=moderate":0.0,"complexity_of_intents=simple":0.0,"cosine_sim__min_val=0.0|max_val=0.33":0.0551428571,"cosine_sim__min_val=0.33|max_val=0.67":0.0,"cosine_sim__min_val=0.67|max_val=1.0":0.0,"entity_sim__min_val=0.0|max_val=0.33":0.0,"entity_sim__min_val=0.33|max_val=0.67":0.2148571429,"entity_sim__min_val=0.67|max_val=1.0":0.0,"expertise_level=basic_domain_knowledge":0.0,"expertise_level=expert_domain_knowledge":0.0,"expertise_level=general_public":0.0,"format_constraints=1":0.0,"languages=English":0.0,"len_longer__min_val=0.0|max_val=0.33":0.0,"len_longer__min_val=0.33|max_val=0.67":0.0,"len_longer__min_val=0.67|max_val=1.0":0.0,"len_shorter__min_val=0.0|max_val=0.33":0.0,"len_shorter__min_val=0.33|max_val=0.67":0.0,"len_shorter__min_val=0.67|max_val=1.0":0.0,"open_endedness=high":0.0,"open_endedness=low":0.0,"open_endedness=moderate":0.0,"open_endedness=no":0.0,"prompt_len__min_val=0.0|max_val=0.33":0.0,"prompt_len__min_val=0.33|max_val=0.67":0.0,"prompt_len__min_val=0.67|max_val=1.0":0.0,"rouge__min_val=0.0|max_val=0.33":0.0,"rouge__min_val=0.33|max_val=0.67":0.0,"rouge__min_val=0.67|max_val=1.0":0.0,"safety_concern=high":0.0,"safety_concern=low":0.0,"safety_concern=moderate":0,"safety_concern=safe":0.0,"subject_of_expertise=Agriculture":0.0,"subject_of_expertise=Business":0.0,"subject_of_expertise=Computer_sciences":0,"subject_of_expertise=Earth_sciences":0,"subject_of_expertise=Economics":0,"subject_of_expertise=Electrical_engineering":0,"subject_of_expertise=Family_and_consumer_science":0,"subject_of_expertise=Geography":0.0,"subject_of_expertise=Human_physical_performance_and_recreation":0.0,"subject_of_expertise=Linguistics_and_language":0.0,"subject_of_expertise=Literature":0,"subject_of_expertise=Materials_science_and_engineering":0.0,"subject_of_expertise=Mechanical_engineering":0,"subject_of_expertise=Media_studies_and_communication":0,"subject_of_expertise=Medicine_and_health":0.0,"subject_of_expertise=Military_sciences":0.0,"subject_of_expertise=Philosophy":0,"subject_of_expertise=Political_science":0.0,"subject_of_expertise=Social_work":0.0,"subject_of_expertise=Sociology":0,"subject_of_expertise=Space_sciences":0,"subject_of_expertise=System_science":0,"token_len_difference__min_val=0.0|max_val=0.33":0.0,"token_len_difference__min_val=0.33|max_val=0.67":0.0,"token_len_difference__min_val=0.67|max_val=1.0":0.0,"type_of_in_context_material=1":0.0,"num_swaps":3,"Chat":0.751396648,"Chat Hard":0.6271929825,"Safety":0.4789981046,"Reasoning":0.7904833033,"Overall":0.6620177596,"alpacaeval-easy":0.75,"alpacaeval-hard":0.7052631579,"alpacaeval-length":0.7368421053,"donotanswer":0.5147058824,"hep-cpp":0.7926829268,"hep-go":0.8048780488,"hep-java":0.8475609756,"hep-js":0.756097561,"hep-python":0.7926829268,"hep-rust":0.7804878049,"llmbar-adver-GPTInst":0.5543478261,"llmbar-adver-GPTOut":0.4680851064,"llmbar-adver-manual":0.5869565217,"llmbar-adver-neighbor":0.6641791045,"llmbar-natural":0.76,"math-prm":0.7852348993,"mt-bench-easy":0.8928571429,"mt-bench-hard":0.5675675676,"mt-bench-med":0.8,"refusals-dangerous":0.35,"refusals-offensive":0.19,"xstest-should-refuse":0.3896103896,"xstest-should-respond":0.864,"label":1}
{"hash":"5ec871bbaf74f83bd221444addb008d6","model_type":"Seq. Classifier","chat_template":"tulu","bertscore__min_val=0.33|max_val=0.67":0.0,"bertscore__min_val=0.67|max_val=1.0":0.9677142857,"bertscore_length__min_val=0.0|max_val=0.33":0.0,"bertscore_length__min_val=0.33|max_val=0.67":0.0,"bertscore_length__min_val=0.67|max_val=1.0":0.0,"complexity_of_intents=complex":0.0,"complexity_of_intents=moderate":0.0,"complexity_of_intents=simple":0.0,"cosine_sim__min_val=0.0|max_val=0.33":0.0568571429,"cosine_sim__min_val=0.33|max_val=0.67":0.0,"cosine_sim__min_val=0.67|max_val=1.0":0.0,"entity_sim__min_val=0.0|max_val=0.33":0.0,"entity_sim__min_val=0.33|max_val=0.67":0.0,"entity_sim__min_val=0.67|max_val=1.0":0.0,"expertise_level=basic_domain_knowledge":0.0,"expertise_level=expert_domain_knowledge":0.0,"expertise_level=general_public":0.0,"format_constraints=1":0.0,"languages=English":0.0,"len_longer__min_val=0.0|max_val=0.33":0.0,"len_longer__min_val=0.33|max_val=0.67":0.0,"len_longer__min_val=0.67|max_val=1.0":0.0,"len_shorter__min_val=0.0|max_val=0.33":0.0,"len_shorter__min_val=0.33|max_val=0.67":0.0,"len_shorter__min_val=0.67|max_val=1.0":0.0,"open_endedness=high":0.0,"open_endedness=low":0.0,"open_endedness=moderate":0.0,"open_endedness=no":0.0,"prompt_len__min_val=0.0|max_val=0.33":0.0,"prompt_len__min_val=0.33|max_val=0.67":0.0,"prompt_len__min_val=0.67|max_val=1.0":0.001,"rouge__min_val=0.0|max_val=0.33":0.0,"rouge__min_val=0.33|max_val=0.67":0.6875714286,"rouge__min_val=0.67|max_val=1.0":0.0,"safety_concern=high":0.0,"safety_concern=low":0.0,"safety_concern=moderate":0,"safety_concern=safe":0.0,"subject_of_expertise=Agriculture":0.0,"subject_of_expertise=Business":0.0,"subject_of_expertise=Computer_sciences":0,"subject_of_expertise=Earth_sciences":0,"subject_of_expertise=Economics":0,"subject_of_expertise=Electrical_engineering":0,"subject_of_expertise=Family_and_consumer_science":0,"subject_of_expertise=Geography":0.0,"subject_of_expertise=Human_physical_performance_and_recreation":0.0,"subject_of_expertise=Linguistics_and_language":0.0,"subject_of_expertise=Literature":0,"subject_of_expertise=Materials_science_and_engineering":0.0,"subject_of_expertise=Mechanical_engineering":0,"subject_of_expertise=Media_studies_and_communication":0,"subject_of_expertise=Medicine_and_health":0.0,"subject_of_expertise=Military_sciences":0.0,"subject_of_expertise=Philosophy":0,"subject_of_expertise=Political_science":0.0,"subject_of_expertise=Social_work":0.0,"subject_of_expertise=Sociology":0,"subject_of_expertise=Space_sciences":0,"subject_of_expertise=System_science":0,"token_len_difference__min_val=0.0|max_val=0.33":0.0015714286,"token_len_difference__min_val=0.33|max_val=0.67":0.0,"token_len_difference__min_val=0.67|max_val=1.0":0.0,"type_of_in_context_material=1":0.0,"num_swaps":25,"Chat":0.782122905,"Chat Hard":0.6096491228,"Safety":0.5292167076,"Reasoning":0.829322857,"Overall":0.6875778981,"alpacaeval-easy":0.82,"alpacaeval-hard":0.7473684211,"alpacaeval-length":0.7368421053,"donotanswer":0.5147058824,"hep-cpp":0.7926829268,"hep-go":0.8414634146,"hep-java":0.8414634146,"hep-js":0.7987804878,"hep-python":0.7926829268,"hep-rust":0.8109756098,"llmbar-adver-GPTInst":0.5543478261,"llmbar-adver-GPTOut":0.4042553191,"llmbar-adver-manual":0.5869565217,"llmbar-adver-neighbor":0.6044776119,"llmbar-natural":0.78,"math-prm":0.8456375839,"mt-bench-easy":0.7857142857,"mt-bench-hard":0.5945945946,"mt-bench-med":0.875,"refusals-dangerous":0.38,"refusals-offensive":0.32,"xstest-should-refuse":0.4545454545,"xstest-should-respond":0.896,"label":1}
{"hash":"817e2b332c8bb457947520f617c406b4","model_type":"Seq. Classifier","chat_template":"tulu","bertscore__min_val=0.33|max_val=0.67":0.0,"bertscore__min_val=0.67|max_val=1.0":0.0,"bertscore_length__min_val=0.0|max_val=0.33":0.0,"bertscore_length__min_val=0.33|max_val=0.67":0.0,"bertscore_length__min_val=0.67|max_val=1.0":0.0,"complexity_of_intents=complex":0.0,"complexity_of_intents=moderate":0.0,"complexity_of_intents=simple":0.0,"cosine_sim__min_val=0.0|max_val=0.33":0.0,"cosine_sim__min_val=0.33|max_val=0.67":0.0,"cosine_sim__min_val=0.67|max_val=1.0":0.0,"entity_sim__min_val=0.0|max_val=0.33":0.5194285714,"entity_sim__min_val=0.33|max_val=0.67":0.0,"entity_sim__min_val=0.67|max_val=1.0":0.0,"expertise_level=basic_domain_knowledge":0.0,"expertise_level=expert_domain_knowledge":0.0,"expertise_level=general_public":0.0,"format_constraints=1":0.0,"languages=English":0.0,"len_longer__min_val=0.0|max_val=0.33":0.0,"len_longer__min_val=0.33|max_val=0.67":0.0,"len_longer__min_val=0.67|max_val=1.0":0.0,"len_shorter__min_val=0.0|max_val=0.33":0.0,"len_shorter__min_val=0.33|max_val=0.67":0.0,"len_shorter__min_val=0.67|max_val=1.0":0.0011428571,"open_endedness=high":0.0,"open_endedness=low":0.0,"open_endedness=moderate":0.0,"open_endedness=no":0.0,"prompt_len__min_val=0.0|max_val=0.33":0.0,"prompt_len__min_val=0.33|max_val=0.67":0.0,"prompt_len__min_val=0.67|max_val=1.0":0.0,"rouge__min_val=0.0|max_val=0.33":0.0,"rouge__min_val=0.33|max_val=0.67":0.6875714286,"rouge__min_val=0.67|max_val=1.0":0.0,"safety_concern=high":0.0,"safety_concern=low":0.0,"safety_concern=moderate":0,"safety_concern=safe":0.0,"subject_of_expertise=Agriculture":0.0,"subject_of_expertise=Business":0.0,"subject_of_expertise=Computer_sciences":0,"subject_of_expertise=Earth_sciences":0,"subject_of_expertise=Economics":0,"subject_of_expertise=Electrical_engineering":0,"subject_of_expertise=Family_and_consumer_science":0,"subject_of_expertise=Geography":0.0,"subject_of_expertise=Human_physical_performance_and_recreation":0.0,"subject_of_expertise=Linguistics_and_language":0.0,"subject_of_expertise=Literature":0,"subject_of_expertise=Materials_science_and_engineering":0.0,"subject_of_expertise=Mechanical_engineering":0,"subject_of_expertise=Media_studies_and_communication":0,"subject_of_expertise=Medicine_and_health":0.0,"subject_of_expertise=Military_sciences":0.0,"subject_of_expertise=Philosophy":0,"subject_of_expertise=Political_science":0.0,"subject_of_expertise=Social_work":0.0,"subject_of_expertise=Sociology":0,"subject_of_expertise=Space_sciences":0,"subject_of_expertise=System_science":0,"token_len_difference__min_val=0.0|max_val=0.33":0.0021428571,"token_len_difference__min_val=0.33|max_val=0.67":0.0,"token_len_difference__min_val=0.67|max_val=1.0":0.0,"type_of_in_context_material=1":0.0,"num_swaps":2385,"Chat":0.843575419,"Chat Hard":0.6228070175,"Safety":0.5157530362,"Reasoning":0.8005606482,"Overall":0.6956740302,"alpacaeval-easy":0.9,"alpacaeval-hard":0.8105263158,"alpacaeval-length":0.8,"donotanswer":0.5294117647,"hep-cpp":0.7256097561,"hep-go":0.743902439,"hep-java":0.7743902439,"hep-js":0.7865853659,"hep-python":0.7804878049,"hep-rust":0.762195122,"llmbar-adver-GPTInst":0.6413043478,"llmbar-adver-GPTOut":0.2978723404,"llmbar-adver-manual":0.4782608696,"llmbar-adver-neighbor":0.6492537313,"llmbar-natural":0.77,"math-prm":0.8389261745,"mt-bench-easy":0.8928571429,"mt-bench-hard":0.6756756757,"mt-bench-med":0.85,"refusals-dangerous":0.28,"refusals-offensive":0.25,"xstest-should-refuse":0.487012987,"xstest-should-respond":0.876,"label":1}
{"hash":"5eb23d1fb8c50fa67ee910928fad92ab","model_type":"Seq. Classifier","chat_template":"tulu","bertscore__min_val=0.33|max_val=0.67":0.0,"bertscore__min_val=0.67|max_val=1.0":0.9685714286,"bertscore_length__min_val=0.0|max_val=0.33":0.0,"bertscore_length__min_val=0.33|max_val=0.67":0.0,"bertscore_length__min_val=0.67|max_val=1.0":0.0,"complexity_of_intents=complex":0.0,"complexity_of_intents=moderate":0.0,"complexity_of_intents=simple":0.0,"cosine_sim__min_val=0.0|max_val=0.33":0.0,"cosine_sim__min_val=0.33|max_val=0.67":0.0,"cosine_sim__min_val=0.67|max_val=1.0":0.0,"entity_sim__min_val=0.0|max_val=0.33":0.0,"entity_sim__min_val=0.33|max_val=0.67":0.0,"entity_sim__min_val=0.67|max_val=1.0":0.0,"expertise_level=basic_domain_knowledge":0.0,"expertise_level=expert_domain_knowledge":0.0,"expertise_level=general_public":0.0,"format_constraints=1":0.0,"languages=English":0.0,"len_longer__min_val=0.0|max_val=0.33":0.0,"len_longer__min_val=0.33|max_val=0.67":0.0,"len_longer__min_val=0.67|max_val=1.0":0.0,"len_shorter__min_val=0.0|max_val=0.33":0.0,"len_shorter__min_val=0.33|max_val=0.67":0.0,"len_shorter__min_val=0.67|max_val=1.0":0.0,"open_endedness=high":0.0,"open_endedness=low":0.0,"open_endedness=moderate":0.0,"open_endedness=no":0.0,"prompt_len__min_val=0.0|max_val=0.33":0.0,"prompt_len__min_val=0.33|max_val=0.67":0.0,"prompt_len__min_val=0.67|max_val=1.0":0.0,"rouge__min_val=0.0|max_val=0.33":0.0,"rouge__min_val=0.33|max_val=0.67":0.0,"rouge__min_val=0.67|max_val=1.0":0.0,"safety_concern=high":0.0,"safety_concern=low":0.0,"safety_concern=moderate":0,"safety_concern=safe":0.0,"subject_of_expertise=Agriculture":0.0,"subject_of_expertise=Business":0.0,"subject_of_expertise=Computer_sciences":0,"subject_of_expertise=Earth_sciences":0,"subject_of_expertise=Economics":0,"subject_of_expertise=Electrical_engineering":0,"subject_of_expertise=Family_and_consumer_science":0,"subject_of_expertise=Geography":0.0,"subject_of_expertise=Human_physical_performance_and_recreation":0.0,"subject_of_expertise=Linguistics_and_language":0.0,"subject_of_expertise=Literature":0,"subject_of_expertise=Materials_science_and_engineering":0.0,"subject_of_expertise=Mechanical_engineering":0,"subject_of_expertise=Media_studies_and_communication":0,"subject_of_expertise=Medicine_and_health":0.0,"subject_of_expertise=Military_sciences":0.0,"subject_of_expertise=Philosophy":0,"subject_of_expertise=Political_science":0.0,"subject_of_expertise=Social_work":0.0,"subject_of_expertise=Sociology":0,"subject_of_expertise=Space_sciences":0,"subject_of_expertise=System_science":0,"token_len_difference__min_val=0.0|max_val=0.33":0.0,"token_len_difference__min_val=0.33|max_val=0.67":0.0,"token_len_difference__min_val=0.67|max_val=1.0":0.0,"type_of_in_context_material=1":0.0,"num_swaps":6780,"Chat":0.9273743017,"Chat Hard":0.5723684211,"Safety":0.7366914707,"Reasoning":0.8204800295,"Overall":0.7642285557,"alpacaeval-easy":0.91,"alpacaeval-hard":0.9368421053,"alpacaeval-length":0.9368421053,"donotanswer":0.5955882353,"hep-cpp":0.7926829268,"hep-go":0.7926829268,"hep-java":0.7926829268,"hep-js":0.8048780488,"hep-python":0.7987804878,"hep-rust":0.8170731707,"llmbar-adver-GPTInst":0.2934782609,"llmbar-adver-GPTOut":0.3829787234,"llmbar-adver-manual":0.4347826087,"llmbar-adver-neighbor":0.6865671642,"llmbar-natural":0.8,"math-prm":0.841163311,"mt-bench-easy":0.9642857143,"mt-bench-hard":0.6486486486,"mt-bench-med":0.9,"refusals-dangerous":0.62,"refusals-offensive":0.66,"xstest-should-refuse":0.7532467532,"xstest-should-respond":0.96,"label":1}
{"hash":"746902b8133c1ba2fc067d364cf418fc","model_type":"Seq. Classifier","chat_template":"tulu","bertscore__min_val=0.33|max_val=0.67":0.0,"bertscore__min_val=0.67|max_val=1.0":0.0,"bertscore_length__min_val=0.0|max_val=0.33":0.0,"bertscore_length__min_val=0.33|max_val=0.67":0.0,"bertscore_length__min_val=0.67|max_val=1.0":0.0,"complexity_of_intents=complex":0.0,"complexity_of_intents=moderate":0.0,"complexity_of_intents=simple":0.0,"cosine_sim__min_val=0.0|max_val=0.33":0.0,"cosine_sim__min_val=0.33|max_val=0.67":0.0,"cosine_sim__min_val=0.67|max_val=1.0":0.0,"entity_sim__min_val=0.0|max_val=0.33":0.0,"entity_sim__min_val=0.33|max_val=0.67":0.0,"entity_sim__min_val=0.67|max_val=1.0":0.0,"expertise_level=basic_domain_knowledge":0.493,"expertise_level=expert_domain_knowledge":0.0,"expertise_level=general_public":0.0,"format_constraints=1":0.0,"languages=English":0.999,"len_longer__min_val=0.0|max_val=0.33":0.0,"len_longer__min_val=0.33|max_val=0.67":0.0,"len_longer__min_val=0.67|max_val=1.0":0.0,"len_shorter__min_val=0.0|max_val=0.33":0.0,"len_shorter__min_val=0.33|max_val=0.67":0.0,"len_shorter__min_val=0.67|max_val=1.0":0.0,"open_endedness=high":0.0,"open_endedness=low":0.0,"open_endedness=moderate":0.0,"open_endedness=no":0.0,"prompt_len__min_val=0.0|max_val=0.33":0.0,"prompt_len__min_val=0.33|max_val=0.67":0.0,"prompt_len__min_val=0.67|max_val=1.0":0.0,"rouge__min_val=0.0|max_val=0.33":0.0,"rouge__min_val=0.33|max_val=0.67":0.0,"rouge__min_val=0.67|max_val=1.0":0.0,"safety_concern=high":0.0,"safety_concern=low":0.0,"safety_concern=moderate":0,"safety_concern=safe":0.0,"subject_of_expertise=Agriculture":0.0,"subject_of_expertise=Business":0.0,"subject_of_expertise=Computer_sciences":0,"subject_of_expertise=Earth_sciences":0,"subject_of_expertise=Economics":0,"subject_of_expertise=Electrical_engineering":0,"subject_of_expertise=Family_and_consumer_science":0,"subject_of_expertise=Geography":0.0,"subject_of_expertise=Human_physical_performance_and_recreation":0.0,"subject_of_expertise=Linguistics_and_language":0.0,"subject_of_expertise=Literature":0,"subject_of_expertise=Materials_science_and_engineering":0.0,"subject_of_expertise=Mechanical_engineering":0,"subject_of_expertise=Media_studies_and_communication":0,"subject_of_expertise=Medicine_and_health":0.0,"subject_of_expertise=Military_sciences":0.0,"subject_of_expertise=Philosophy":0,"subject_of_expertise=Political_science":0.0,"subject_of_expertise=Social_work":0.0,"subject_of_expertise=Sociology":0,"subject_of_expertise=Space_sciences":0,"subject_of_expertise=System_science":0,"token_len_difference__min_val=0.0|max_val=0.33":0.0,"token_len_difference__min_val=0.33|max_val=0.67":0.0,"token_len_difference__min_val=0.67|max_val=1.0":0.0,"type_of_in_context_material=1":0.0,"num_swaps":3449,"Chat":0.8575418994,"Chat Hard":0.6359649123,"Safety":0.553768761,"Reasoning":0.8297354995,"Overall":0.7192527681,"alpacaeval-easy":0.9,"alpacaeval-hard":0.8315789474,"alpacaeval-length":0.8315789474,"donotanswer":0.5147058824,"hep-cpp":0.7987804878,"hep-go":0.7987804878,"hep-java":0.8414634146,"hep-js":0.7073170732,"hep-python":0.8048780488,"hep-rust":0.8109756098,"llmbar-adver-GPTInst":0.5652173913,"llmbar-adver-GPTOut":0.3829787234,"llmbar-adver-manual":0.5434782609,"llmbar-adver-neighbor":0.6791044776,"llmbar-natural":0.8,"math-prm":0.8657718121,"mt-bench-easy":0.8571428571,"mt-bench-hard":0.6486486486,"mt-bench-med":0.875,"refusals-dangerous":0.43,"refusals-offensive":0.3,"xstest-should-refuse":0.5324675325,"xstest-should-respond":0.868,"label":1}
{"hash":"905bc88981d21eeff17b977bb6b8f563","model_type":"Seq. Classifier","chat_template":"tulu","bertscore__min_val=0.33|max_val=0.67":0.0,"bertscore__min_val=0.67|max_val=1.0":0.0,"bertscore_length__min_val=0.0|max_val=0.33":0.0,"bertscore_length__min_val=0.33|max_val=0.67":0.0,"bertscore_length__min_val=0.67|max_val=1.0":0.0,"complexity_of_intents=complex":0.0,"complexity_of_intents=moderate":0.0,"complexity_of_intents=simple":0.0,"cosine_sim__min_val=0.0|max_val=0.33":0.0,"cosine_sim__min_val=0.33|max_val=0.67":0.0,"cosine_sim__min_val=0.67|max_val=1.0":0.0,"entity_sim__min_val=0.0|max_val=0.33":0.0,"entity_sim__min_val=0.33|max_val=0.67":0.0,"entity_sim__min_val=0.67|max_val=1.0":0.0,"expertise_level=basic_domain_knowledge":0.0,"expertise_level=expert_domain_knowledge":0.0,"expertise_level=general_public":0.0,"format_constraints=1":0.2732857143,"languages=English":0.0,"len_longer__min_val=0.0|max_val=0.33":0.0,"len_longer__min_val=0.33|max_val=0.67":0.0,"len_longer__min_val=0.67|max_val=1.0":0.0,"len_shorter__min_val=0.0|max_val=0.33":0.0,"len_shorter__min_val=0.33|max_val=0.67":0.0,"len_shorter__min_val=0.67|max_val=1.0":0.0,"open_endedness=high":0.0,"open_endedness=low":0.0,"open_endedness=moderate":0.0,"open_endedness=no":0.0438571429,"prompt_len__min_val=0.0|max_val=0.33":0.0,"prompt_len__min_val=0.33|max_val=0.67":0.0,"prompt_len__min_val=0.67|max_val=1.0":0.0,"rouge__min_val=0.0|max_val=0.33":0.0,"rouge__min_val=0.33|max_val=0.67":0.0,"rouge__min_val=0.67|max_val=1.0":0.0,"safety_concern=high":0.0,"safety_concern=low":0.0,"safety_concern=moderate":0,"safety_concern=safe":0.0,"subject_of_expertise=Agriculture":0.0,"subject_of_expertise=Business":0.0,"subject_of_expertise=Computer_sciences":0,"subject_of_expertise=Earth_sciences":0,"subject_of_expertise=Economics":0,"subject_of_expertise=Electrical_engineering":0,"subject_of_expertise=Family_and_consumer_science":0,"subject_of_expertise=Geography":0.0,"subject_of_expertise=Human_physical_performance_and_recreation":0.0,"subject_of_expertise=Linguistics_and_language":0.0,"subject_of_expertise=Literature":0,"subject_of_expertise=Materials_science_and_engineering":0.0,"subject_of_expertise=Mechanical_engineering":0,"subject_of_expertise=Media_studies_and_communication":0,"subject_of_expertise=Medicine_and_health":0.0,"subject_of_expertise=Military_sciences":0.0,"subject_of_expertise=Philosophy":0,"subject_of_expertise=Political_science":0.0,"subject_of_expertise=Social_work":0.0,"subject_of_expertise=Sociology":0,"subject_of_expertise=Space_sciences":0,"subject_of_expertise=System_science":0,"token_len_difference__min_val=0.0|max_val=0.33":0.0,"token_len_difference__min_val=0.33|max_val=0.67":0.0,"token_len_difference__min_val=0.67|max_val=1.0":0.0,"type_of_in_context_material=1":0.1114285714,"num_swaps":59,"Chat":0.7458100559,"Chat Hard":0.6622807018,"Safety":0.4693839242,"Reasoning":0.8273994653,"Overall":0.6762185368,"alpacaeval-easy":0.73,"alpacaeval-hard":0.6947368421,"alpacaeval-length":0.7473684211,"donotanswer":0.5,"hep-cpp":0.7256097561,"hep-go":0.743902439,"hep-java":0.7804878049,"hep-js":0.8170731707,"hep-python":0.8536585366,"hep-rust":0.7865853659,"llmbar-adver-GPTInst":0.6304347826,"llmbar-adver-GPTOut":0.4680851064,"llmbar-adver-manual":0.5652173913,"llmbar-adver-neighbor":0.6641791045,"llmbar-natural":0.82,"math-prm":0.870246085,"mt-bench-easy":0.8571428571,"mt-bench-hard":0.6756756757,"mt-bench-med":0.825,"refusals-dangerous":0.23,"refusals-offensive":0.12,"xstest-should-refuse":0.4155844156,"xstest-should-respond":0.912,"label":1}
{"hash":"392e3d1f6deb421dd807a3f40530ed39","model_type":"Seq. Classifier","chat_template":"tulu","bertscore__min_val=0.33|max_val=0.67":0.0,"bertscore__min_val=0.67|max_val=1.0":0.9682857143,"bertscore_length__min_val=0.0|max_val=0.33":0.0,"bertscore_length__min_val=0.33|max_val=0.67":0.0,"bertscore_length__min_val=0.67|max_val=1.0":0.0,"complexity_of_intents=complex":0.0,"complexity_of_intents=moderate":0.0,"complexity_of_intents=simple":0.0,"cosine_sim__min_val=0.0|max_val=0.33":0.0,"cosine_sim__min_val=0.33|max_val=0.67":0.0,"cosine_sim__min_val=0.67|max_val=1.0":0.7052857143,"entity_sim__min_val=0.0|max_val=0.33":0.0,"entity_sim__min_val=0.33|max_val=0.67":0.2222857143,"entity_sim__min_val=0.67|max_val=1.0":0.0,"expertise_level=basic_domain_knowledge":0.0,"expertise_level=expert_domain_knowledge":0.0,"expertise_level=general_public":0.0,"format_constraints=1":0.0,"languages=English":0.0,"len_longer__min_val=0.0|max_val=0.33":0.0,"len_longer__min_val=0.33|max_val=0.67":0.0,"len_longer__min_val=0.67|max_val=1.0":0.0,"len_shorter__min_val=0.0|max_val=0.33":0.0,"len_shorter__min_val=0.33|max_val=0.67":0.0,"len_shorter__min_val=0.67|max_val=1.0":0.0,"open_endedness=high":0.0,"open_endedness=low":0.0,"open_endedness=moderate":0.0,"open_endedness=no":0.0,"prompt_len__min_val=0.0|max_val=0.33":0.0,"prompt_len__min_val=0.33|max_val=0.67":0.0,"prompt_len__min_val=0.67|max_val=1.0":0.0,"rouge__min_val=0.0|max_val=0.33":0.0,"rouge__min_val=0.33|max_val=0.67":0.0,"rouge__min_val=0.67|max_val=1.0":0.0,"safety_concern=high":0.0,"safety_concern=low":0.0,"safety_concern=moderate":0,"safety_concern=safe":0.0,"subject_of_expertise=Agriculture":0.0,"subject_of_expertise=Business":0.0,"subject_of_expertise=Computer_sciences":0,"subject_of_expertise=Earth_sciences":0,"subject_of_expertise=Economics":0,"subject_of_expertise=Electrical_engineering":0,"subject_of_expertise=Family_and_consumer_science":0,"subject_of_expertise=Geography":0.0,"subject_of_expertise=Human_physical_performance_and_recreation":0.0,"subject_of_expertise=Linguistics_and_language":0.0,"subject_of_expertise=Literature":0,"subject_of_expertise=Materials_science_and_engineering":0.0,"subject_of_expertise=Mechanical_engineering":0,"subject_of_expertise=Media_studies_and_communication":0,"subject_of_expertise=Medicine_and_health":0.0,"subject_of_expertise=Military_sciences":0.0,"subject_of_expertise=Philosophy":0,"subject_of_expertise=Political_science":0.0,"subject_of_expertise=Social_work":0.0,"subject_of_expertise=Sociology":0,"subject_of_expertise=Space_sciences":0,"subject_of_expertise=System_science":0,"token_len_difference__min_val=0.0|max_val=0.33":0.0017142857,"token_len_difference__min_val=0.33|max_val=0.67":0.0,"token_len_difference__min_val=0.67|max_val=1.0":0.0,"type_of_in_context_material=1":0.0,"num_swaps":1319,"Chat":0.8687150838,"Chat Hard":0.649122807,"Safety":0.4526970867,"Reasoning":0.8318635074,"Overall":0.7005996212,"alpacaeval-easy":0.85,"alpacaeval-hard":0.8315789474,"alpacaeval-length":0.8947368421,"donotanswer":0.5073529412,"hep-cpp":0.7682926829,"hep-go":0.8109756098,"hep-java":0.8841463415,"hep-js":0.8048780488,"hep-python":0.8231707317,"hep-rust":0.8170731707,"llmbar-adver-GPTInst":0.6086956522,"llmbar-adver-GPTOut":0.4255319149,"llmbar-adver-manual":0.6304347826,"llmbar-adver-neighbor":0.6417910448,"llmbar-natural":0.81,"math-prm":0.8456375839,"mt-bench-easy":0.8928571429,"mt-bench-hard":0.6486486486,"mt-bench-med":0.925,"refusals-dangerous":0.08,"refusals-offensive":0.16,"xstest-should-refuse":0.3766233766,"xstest-should-respond":0.96,"label":1}
{"hash":"1aa6f4e50f88f360c0726ec27c8c3c24","model_type":"Seq. Classifier","chat_template":"tulu","bertscore__min_val=0.33|max_val=0.67":0.0308571429,"bertscore__min_val=0.67|max_val=1.0":0.0,"bertscore_length__min_val=0.0|max_val=0.33":0.0,"bertscore_length__min_val=0.33|max_val=0.67":0.0,"bertscore_length__min_val=0.67|max_val=1.0":0.0,"complexity_of_intents=complex":0.0,"complexity_of_intents=moderate":0.0,"complexity_of_intents=simple":0.0,"cosine_sim__min_val=0.0|max_val=0.33":0.0,"cosine_sim__min_val=0.33|max_val=0.67":0.2368571429,"cosine_sim__min_val=0.67|max_val=1.0":0.0,"entity_sim__min_val=0.0|max_val=0.33":0.0,"entity_sim__min_val=0.33|max_val=0.67":0.0,"entity_sim__min_val=0.67|max_val=1.0":0.0,"expertise_level=basic_domain_knowledge":0.0,"expertise_level=expert_domain_knowledge":0.0,"expertise_level=general_public":0.0,"format_constraints=1":0.0,"languages=English":0.0,"len_longer__min_val=0.0|max_val=0.33":0.0,"len_longer__min_val=0.33|max_val=0.67":0.0,"len_longer__min_val=0.67|max_val=1.0":0.0,"len_shorter__min_val=0.0|max_val=0.33":0.0,"len_shorter__min_val=0.33|max_val=0.67":0.0,"len_shorter__min_val=0.67|max_val=1.0":0.0,"open_endedness=high":0.0,"open_endedness=low":0.0,"open_endedness=moderate":0.0,"open_endedness=no":0.0,"prompt_len__min_val=0.0|max_val=0.33":0.0,"prompt_len__min_val=0.33|max_val=0.67":0.0,"prompt_len__min_val=0.67|max_val=1.0":0.0,"rouge__min_val=0.0|max_val=0.33":0.2828571429,"rouge__min_val=0.33|max_val=0.67":0.0,"rouge__min_val=0.67|max_val=1.0":0.0,"safety_concern=high":0.0,"safety_concern=low":0.0,"safety_concern=moderate":0,"safety_concern=safe":0.0,"subject_of_expertise=Agriculture":0.0,"subject_of_expertise=Business":0.0,"subject_of_expertise=Computer_sciences":0,"subject_of_expertise=Earth_sciences":0,"subject_of_expertise=Economics":0,"subject_of_expertise=Electrical_engineering":0,"subject_of_expertise=Family_and_consumer_science":0,"subject_of_expertise=Geography":0.0,"subject_of_expertise=Human_physical_performance_and_recreation":0.0,"subject_of_expertise=Linguistics_and_language":0.0,"subject_of_expertise=Literature":0,"subject_of_expertise=Materials_science_and_engineering":0.0,"subject_of_expertise=Mechanical_engineering":0,"subject_of_expertise=Media_studies_and_communication":0,"subject_of_expertise=Medicine_and_health":0.0,"subject_of_expertise=Military_sciences":0.0,"subject_of_expertise=Philosophy":0,"subject_of_expertise=Political_science":0.0,"subject_of_expertise=Social_work":0.0,"subject_of_expertise=Sociology":0,"subject_of_expertise=Space_sciences":0,"subject_of_expertise=System_science":0,"token_len_difference__min_val=0.0|max_val=0.33":0.0,"token_len_difference__min_val=0.33|max_val=0.67":0.0,"token_len_difference__min_val=0.67|max_val=1.0":0.0,"type_of_in_context_material=1":0.0,"num_swaps":44,"Chat":0.6592178771,"Chat Hard":0.6973684211,"Safety":0.5264158652,"Reasoning":0.8237504774,"Overall":0.6766881602,"alpacaeval-easy":0.58,"alpacaeval-hard":0.6105263158,"alpacaeval-length":0.6736842105,"donotanswer":0.5220588235,"hep-cpp":0.7134146341,"hep-go":0.7195121951,"hep-java":0.7682926829,"hep-js":0.7256097561,"hep-python":0.7865853659,"hep-rust":0.762195122,"llmbar-adver-GPTInst":0.7717391304,"llmbar-adver-GPTOut":0.5531914894,"llmbar-adver-manual":0.6739130435,"llmbar-adver-neighbor":0.671641791,"llmbar-natural":0.78,"math-prm":0.9015659955,"mt-bench-easy":0.8928571429,"mt-bench-hard":0.5945945946,"mt-bench-med":0.775,"refusals-dangerous":0.37,"refusals-offensive":0.32,"xstest-should-refuse":0.461038961,"xstest-should-respond":0.872,"label":1}
{"hash":"bc2d7ce3000fd8a227e1bce2455e1f96","model_type":"Seq. Classifier","chat_template":"tulu","bertscore__min_val=0.33|max_val=0.67":0.0,"bertscore__min_val=0.67|max_val=1.0":0.0,"bertscore_length__min_val=0.0|max_val=0.33":0.0,"bertscore_length__min_val=0.33|max_val=0.67":0.0,"bertscore_length__min_val=0.67|max_val=1.0":0.0,"complexity_of_intents=complex":0.0932857143,"complexity_of_intents=moderate":0.0,"complexity_of_intents=simple":0.0,"cosine_sim__min_val=0.0|max_val=0.33":0.0,"cosine_sim__min_val=0.33|max_val=0.67":0.0,"cosine_sim__min_val=0.67|max_val=1.0":0.0,"entity_sim__min_val=0.0|max_val=0.33":0.0,"entity_sim__min_val=0.33|max_val=0.67":0.0,"entity_sim__min_val=0.67|max_val=1.0":0.0,"expertise_level=basic_domain_knowledge":0.0,"expertise_level=expert_domain_knowledge":0.105,"expertise_level=general_public":0.0,"format_constraints=1":0.0,"languages=English":0.0,"len_longer__min_val=0.0|max_val=0.33":0.0,"len_longer__min_val=0.33|max_val=0.67":0.0,"len_longer__min_val=0.67|max_val=1.0":0.0,"len_shorter__min_val=0.0|max_val=0.33":0.0,"len_shorter__min_val=0.33|max_val=0.67":0.0,"len_shorter__min_val=0.67|max_val=1.0":0.0,"open_endedness=high":0.0,"open_endedness=low":0.0,"open_endedness=moderate":0.0,"open_endedness=no":0.0,"prompt_len__min_val=0.0|max_val=0.33":0.0,"prompt_len__min_val=0.33|max_val=0.67":0.0,"prompt_len__min_val=0.67|max_val=1.0":0.0,"rouge__min_val=0.0|max_val=0.33":0.0,"rouge__min_val=0.33|max_val=0.67":0.0,"rouge__min_val=0.67|max_val=1.0":0.0,"safety_concern=high":0.0,"safety_concern=low":0.0,"safety_concern=moderate":0,"safety_concern=safe":0.0,"subject_of_expertise=Agriculture":0.0087142857,"subject_of_expertise=Business":0.0,"subject_of_expertise=Computer_sciences":0,"subject_of_expertise=Earth_sciences":0,"subject_of_expertise=Economics":0,"subject_of_expertise=Electrical_engineering":0,"subject_of_expertise=Family_and_consumer_science":0,"subject_of_expertise=Geography":0.0,"subject_of_expertise=Human_physical_performance_and_recreation":0.0,"subject_of_expertise=Linguistics_and_language":0.0,"subject_of_expertise=Literature":0,"subject_of_expertise=Materials_science_and_engineering":0.0,"subject_of_expertise=Mechanical_engineering":0,"subject_of_expertise=Media_studies_and_communication":0,"subject_of_expertise=Medicine_and_health":0.0,"subject_of_expertise=Military_sciences":0.0,"subject_of_expertise=Philosophy":0,"subject_of_expertise=Political_science":0.0,"subject_of_expertise=Social_work":0.0,"subject_of_expertise=Sociology":0,"subject_of_expertise=Space_sciences":0,"subject_of_expertise=System_science":0,"token_len_difference__min_val=0.0|max_val=0.33":0.0,"token_len_difference__min_val=0.33|max_val=0.67":0.0,"token_len_difference__min_val=0.67|max_val=1.0":0.0,"type_of_in_context_material=1":0.0,"num_swaps":5,"Chat":0.7290502793,"Chat Hard":0.6644736842,"Safety":0.6544304668,"Reasoning":0.8277063895,"Overall":0.718915205,"alpacaeval-easy":0.71,"alpacaeval-hard":0.7263157895,"alpacaeval-length":0.7052631579,"donotanswer":0.6102941176,"hep-cpp":0.7926829268,"hep-go":0.7195121951,"hep-java":0.8353658537,"hep-js":0.7317073171,"hep-python":0.8231707317,"hep-rust":0.7682926829,"llmbar-adver-GPTInst":0.6847826087,"llmbar-adver-GPTOut":0.4680851064,"llmbar-adver-manual":0.6086956522,"llmbar-adver-neighbor":0.6791044776,"llmbar-natural":0.77,"math-prm":0.8769574944,"mt-bench-easy":0.7857142857,"mt-bench-hard":0.5945945946,"mt-bench-med":0.8,"refusals-dangerous":0.56,"refusals-offensive":0.43,"xstest-should-refuse":0.6818181818,"xstest-should-respond":0.856,"label":1}
{"hash":"48734f7154d2bc24fee0b19f9bb648a8","model_type":"Seq. Classifier","chat_template":"tulu","bertscore__min_val=0.33|max_val=0.67":0.0,"bertscore__min_val=0.67|max_val=1.0":0.0,"bertscore_length__min_val=0.0|max_val=0.33":0.0,"bertscore_length__min_val=0.33|max_val=0.67":0.3631428571,"bertscore_length__min_val=0.67|max_val=1.0":0.0,"complexity_of_intents=complex":0.0,"complexity_of_intents=moderate":0.0,"complexity_of_intents=simple":0.0,"cosine_sim__min_val=0.0|max_val=0.33":0.0,"cosine_sim__min_val=0.33|max_val=0.67":0.0,"cosine_sim__min_val=0.67|max_val=1.0":0.7055714286,"entity_sim__min_val=0.0|max_val=0.33":0.513,"entity_sim__min_val=0.33|max_val=0.67":0.0,"entity_sim__min_val=0.67|max_val=1.0":0.0,"expertise_level=basic_domain_knowledge":0.0,"expertise_level=expert_domain_knowledge":0.0,"expertise_level=general_public":0.0,"format_constraints=1":0.0,"languages=English":0.0,"len_longer__min_val=0.0|max_val=0.33":0.0,"len_longer__min_val=0.33|max_val=0.67":0.0,"len_longer__min_val=0.67|max_val=1.0":0.0,"len_shorter__min_val=0.0|max_val=0.33":0.0,"len_shorter__min_val=0.33|max_val=0.67":0.0,"len_shorter__min_val=0.67|max_val=1.0":0.0011428571,"open_endedness=high":0.0,"open_endedness=low":0.0,"open_endedness=moderate":0.0,"open_endedness=no":0.0,"prompt_len__min_val=0.0|max_val=0.33":0.0,"prompt_len__min_val=0.33|max_val=0.67":0.0,"prompt_len__min_val=0.67|max_val=1.0":0.0,"rouge__min_val=0.0|max_val=0.33":0.2858571429,"rouge__min_val=0.33|max_val=0.67":0.0,"rouge__min_val=0.67|max_val=1.0":0.0,"safety_concern=high":0.0,"safety_concern=low":0.0,"safety_concern=moderate":0,"safety_concern=safe":0.0,"subject_of_expertise=Agriculture":0.0,"subject_of_expertise=Business":0.0,"subject_of_expertise=Computer_sciences":0,"subject_of_expertise=Earth_sciences":0,"subject_of_expertise=Economics":0,"subject_of_expertise=Electrical_engineering":0,"subject_of_expertise=Family_and_consumer_science":0,"subject_of_expertise=Geography":0.0,"subject_of_expertise=Human_physical_performance_and_recreation":0.0,"subject_of_expertise=Linguistics_and_language":0.0,"subject_of_expertise=Literature":0,"subject_of_expertise=Materials_science_and_engineering":0.0,"subject_of_expertise=Mechanical_engineering":0,"subject_of_expertise=Media_studies_and_communication":0,"subject_of_expertise=Medicine_and_health":0.0,"subject_of_expertise=Military_sciences":0.0,"subject_of_expertise=Philosophy":0,"subject_of_expertise=Political_science":0.0,"subject_of_expertise=Social_work":0.0,"subject_of_expertise=Sociology":0,"subject_of_expertise=Space_sciences":0,"subject_of_expertise=System_science":0,"token_len_difference__min_val=0.0|max_val=0.33":0.0017142857,"token_len_difference__min_val=0.33|max_val=0.67":0.0,"token_len_difference__min_val=0.67|max_val=1.0":0.0,"type_of_in_context_material=1":0.0,"num_swaps":50,"Chat":0.687150838,"Chat Hard":0.6403508772,"Safety":0.47642106,"Reasoning":0.8048234845,"Overall":0.6521865649,"alpacaeval-easy":0.68,"alpacaeval-hard":0.5684210526,"alpacaeval-length":0.6947368421,"donotanswer":0.5661764706,"hep-cpp":0.7987804878,"hep-go":0.7987804878,"hep-java":0.8475609756,"hep-js":0.762195122,"hep-python":0.7682926829,"hep-rust":0.756097561,"llmbar-adver-GPTInst":0.6956521739,"llmbar-adver-GPTOut":0.3829787234,"llmbar-adver-manual":0.6304347826,"llmbar-adver-neighbor":0.6119402985,"llmbar-natural":0.77,"math-prm":0.8210290828,"mt-bench-easy":0.8928571429,"mt-bench-hard":0.5945945946,"mt-bench-med":0.825,"refusals-dangerous":0.31,"refusals-offensive":0.24,"xstest-should-refuse":0.3376623377,"xstest-should-respond":0.884,"label":0}
{"hash":"992f59b217c13a92875124e2bd77adc6","model_type":"Seq. Classifier","chat_template":"tulu","bertscore__min_val=0.33|max_val=0.67":0.0,"bertscore__min_val=0.67|max_val=1.0":0.0,"bertscore_length__min_val=0.0|max_val=0.33":0.0,"bertscore_length__min_val=0.33|max_val=0.67":0.0,"bertscore_length__min_val=0.67|max_val=1.0":0.0,"complexity_of_intents=complex":0.0,"complexity_of_intents=moderate":0.0,"complexity_of_intents=simple":0.0,"cosine_sim__min_val=0.0|max_val=0.33":0.0,"cosine_sim__min_val=0.33|max_val=0.67":0.0,"cosine_sim__min_val=0.67|max_val=1.0":0.0,"entity_sim__min_val=0.0|max_val=0.33":0.0,"entity_sim__min_val=0.33|max_val=0.67":0.0,"entity_sim__min_val=0.67|max_val=1.0":0.0,"expertise_level=basic_domain_knowledge":0.0,"expertise_level=expert_domain_knowledge":0.0,"expertise_level=general_public":0.0,"format_constraints=1":0.0,"languages=English":0.0,"len_longer__min_val=0.0|max_val=0.33":0.0,"len_longer__min_val=0.33|max_val=0.67":0.0,"len_longer__min_val=0.67|max_val=1.0":0.0,"len_shorter__min_val=0.0|max_val=0.33":0.0,"len_shorter__min_val=0.33|max_val=0.67":0.0,"len_shorter__min_val=0.67|max_val=1.0":0.0,"open_endedness=high":0.0,"open_endedness=low":0.0,"open_endedness=moderate":0.0,"open_endedness=no":0.0,"prompt_len__min_val=0.0|max_val=0.33":0.0,"prompt_len__min_val=0.33|max_val=0.67":0.0,"prompt_len__min_val=0.67|max_val=1.0":0.0,"rouge__min_val=0.0|max_val=0.33":0.2841428571,"rouge__min_val=0.33|max_val=0.67":0.0,"rouge__min_val=0.67|max_val=1.0":0.0,"safety_concern=high":0.0,"safety_concern=low":0.0,"safety_concern=moderate":0,"safety_concern=safe":0.0,"subject_of_expertise=Agriculture":0.0,"subject_of_expertise=Business":0.0,"subject_of_expertise=Computer_sciences":0,"subject_of_expertise=Earth_sciences":0,"subject_of_expertise=Economics":0,"subject_of_expertise=Electrical_engineering":0,"subject_of_expertise=Family_and_consumer_science":0,"subject_of_expertise=Geography":0.0,"subject_of_expertise=Human_physical_performance_and_recreation":0.0,"subject_of_expertise=Linguistics_and_language":0.0,"subject_of_expertise=Literature":0,"subject_of_expertise=Materials_science_and_engineering":0.0,"subject_of_expertise=Mechanical_engineering":0,"subject_of_expertise=Media_studies_and_communication":0,"subject_of_expertise=Medicine_and_health":0.0,"subject_of_expertise=Military_sciences":0.0,"subject_of_expertise=Philosophy":0,"subject_of_expertise=Political_science":0.0,"subject_of_expertise=Social_work":0.0,"subject_of_expertise=Sociology":0,"subject_of_expertise=Space_sciences":0,"subject_of_expertise=System_science":0,"token_len_difference__min_val=0.0|max_val=0.33":0.0,"token_len_difference__min_val=0.33|max_val=0.67":0.0,"token_len_difference__min_val=0.67|max_val=1.0":0.0,"type_of_in_context_material=1":0.0,"num_swaps":1989,"Chat":0.8715083799,"Chat Hard":0.6425438596,"Safety":0.5869746578,"Reasoning":0.8504767556,"Overall":0.7378759132,"alpacaeval-easy":0.86,"alpacaeval-hard":0.9052631579,"alpacaeval-length":0.8421052632,"donotanswer":0.6102941176,"hep-cpp":0.7804878049,"hep-go":0.8170731707,"hep-java":0.8292682927,"hep-js":0.7865853659,"hep-python":0.7926829268,"hep-rust":0.8170731707,"llmbar-adver-GPTInst":0.5108695652,"llmbar-adver-GPTOut":0.4893617021,"llmbar-adver-manual":0.652173913,"llmbar-adver-neighbor":0.6492537313,"llmbar-natural":0.82,"math-prm":0.8970917226,"mt-bench-easy":0.9285714286,"mt-bench-hard":0.6486486486,"mt-bench-med":0.85,"refusals-dangerous":0.19,"refusals-offensive":0.64,"xstest-should-refuse":0.487012987,"xstest-should-respond":0.952,"label":1}
{"hash":"14a45b2699325be0987c8c65b7335853","model_type":"Seq. Classifier","chat_template":"tulu","bertscore__min_val=0.33|max_val=0.67":0.0307142857,"bertscore__min_val=0.67|max_val=1.0":0.0,"bertscore_length__min_val=0.0|max_val=0.33":0.0,"bertscore_length__min_val=0.33|max_val=0.67":0.0,"bertscore_length__min_val=0.67|max_val=1.0":0.0,"complexity_of_intents=complex":0.0,"complexity_of_intents=moderate":0.0,"complexity_of_intents=simple":0.0,"cosine_sim__min_val=0.0|max_val=0.33":0.0,"cosine_sim__min_val=0.33|max_val=0.67":0.0,"cosine_sim__min_val=0.67|max_val=1.0":0.0,"entity_sim__min_val=0.0|max_val=0.33":0.0,"entity_sim__min_val=0.33|max_val=0.67":0.0,"entity_sim__min_val=0.67|max_val=1.0":0.0,"expertise_level=basic_domain_knowledge":0.0,"expertise_level=expert_domain_knowledge":0.0,"expertise_level=general_public":0.0,"format_constraints=1":0.0,"languages=English":0.0,"len_longer__min_val=0.0|max_val=0.33":0.0,"len_longer__min_val=0.33|max_val=0.67":0.0,"len_longer__min_val=0.67|max_val=1.0":0.0,"len_shorter__min_val=0.0|max_val=0.33":0.0,"len_shorter__min_val=0.33|max_val=0.67":0.0,"len_shorter__min_val=0.67|max_val=1.0":0.0,"open_endedness=high":0.0,"open_endedness=low":0.0,"open_endedness=moderate":0.0,"open_endedness=no":0.0,"prompt_len__min_val=0.0|max_val=0.33":0.0,"prompt_len__min_val=0.33|max_val=0.67":0.0,"prompt_len__min_val=0.67|max_val=1.0":0.0,"rouge__min_val=0.0|max_val=0.33":0.0,"rouge__min_val=0.33|max_val=0.67":0.0,"rouge__min_val=0.67|max_val=1.0":0.0,"safety_concern=high":0.0,"safety_concern=low":0.0,"safety_concern=moderate":0,"safety_concern=safe":0.0,"subject_of_expertise=Agriculture":0.0,"subject_of_expertise=Business":0.0,"subject_of_expertise=Computer_sciences":0,"subject_of_expertise=Earth_sciences":0,"subject_of_expertise=Economics":0,"subject_of_expertise=Electrical_engineering":0,"subject_of_expertise=Family_and_consumer_science":0,"subject_of_expertise=Geography":0.0,"subject_of_expertise=Human_physical_performance_and_recreation":0.0,"subject_of_expertise=Linguistics_and_language":0.0,"subject_of_expertise=Literature":0,"subject_of_expertise=Materials_science_and_engineering":0.0,"subject_of_expertise=Mechanical_engineering":0,"subject_of_expertise=Media_studies_and_communication":0,"subject_of_expertise=Medicine_and_health":0.0,"subject_of_expertise=Military_sciences":0.0,"subject_of_expertise=Philosophy":0,"subject_of_expertise=Political_science":0.0,"subject_of_expertise=Social_work":0.0,"subject_of_expertise=Sociology":0,"subject_of_expertise=Space_sciences":0,"subject_of_expertise=System_science":0,"token_len_difference__min_val=0.0|max_val=0.33":0.0,"token_len_difference__min_val=0.33|max_val=0.67":0.0,"token_len_difference__min_val=0.67|max_val=1.0":0.0,"type_of_in_context_material=1":0.0,"num_swaps":215,"Chat":0.7737430168,"Chat Hard":0.6666666667,"Safety":0.5201988066,"Reasoning":0.8335993343,"Overall":0.6985519561,"alpacaeval-easy":0.72,"alpacaeval-hard":0.7894736842,"alpacaeval-length":0.7368421053,"donotanswer":0.5220588235,"hep-cpp":0.7804878049,"hep-go":0.7743902439,"hep-java":0.8048780488,"hep-js":0.8536585366,"hep-python":0.7743902439,"hep-rust":0.7804878049,"llmbar-adver-GPTInst":0.6630434783,"llmbar-adver-GPTOut":0.5957446809,"llmbar-adver-manual":0.6086956522,"llmbar-adver-neighbor":0.6268656716,"llmbar-natural":0.82,"math-prm":0.8724832215,"mt-bench-easy":0.9285714286,"mt-bench-hard":0.5675675676,"mt-bench-med":0.85,"refusals-dangerous":0.31,"refusals-offensive":0.25,"xstest-should-refuse":0.4675324675,"xstest-should-respond":0.916,"label":1}
{"hash":"c4f2aa1d303f40c1c458f6d1ad673825","model_type":"Seq. Classifier","chat_template":"tulu","bertscore__min_val=0.33|max_val=0.67":0.0,"bertscore__min_val=0.67|max_val=1.0":0.0,"bertscore_length__min_val=0.0|max_val=0.33":0.0,"bertscore_length__min_val=0.33|max_val=0.67":0.0,"bertscore_length__min_val=0.67|max_val=1.0":0.2281428571,"complexity_of_intents=complex":0.0,"complexity_of_intents=moderate":0.0,"complexity_of_intents=simple":0.0,"cosine_sim__min_val=0.0|max_val=0.33":0.0,"cosine_sim__min_val=0.33|max_val=0.67":0.0,"cosine_sim__min_val=0.67|max_val=1.0":0.0,"entity_sim__min_val=0.0|max_val=0.33":0.0,"entity_sim__min_val=0.33|max_val=0.67":0.0,"entity_sim__min_val=0.67|max_val=1.0":0.0,"expertise_level=basic_domain_knowledge":0.0,"expertise_level=expert_domain_knowledge":0.0,"expertise_level=general_public":0.0,"format_constraints=1":0.0,"languages=English":0.0,"len_longer__min_val=0.0|max_val=0.33":0.0,"len_longer__min_val=0.33|max_val=0.67":0.0,"len_longer__min_val=0.67|max_val=1.0":0.0,"len_shorter__min_val=0.0|max_val=0.33":0.0,"len_shorter__min_val=0.33|max_val=0.67":0.0,"len_shorter__min_val=0.67|max_val=1.0":0.0,"open_endedness=high":0.0,"open_endedness=low":0.0,"open_endedness=moderate":0.0,"open_endedness=no":0.0,"prompt_len__min_val=0.0|max_val=0.33":0.0,"prompt_len__min_val=0.33|max_val=0.67":0.0,"prompt_len__min_val=0.67|max_val=1.0":0.0007142857,"rouge__min_val=0.0|max_val=0.33":0.0,"rouge__min_val=0.33|max_val=0.67":0.0,"rouge__min_val=0.67|max_val=1.0":0.0,"safety_concern=high":0.0,"safety_concern=low":0.0,"safety_concern=moderate":0,"safety_concern=safe":0.0,"subject_of_expertise=Agriculture":0.0,"subject_of_expertise=Business":0.0,"subject_of_expertise=Computer_sciences":0,"subject_of_expertise=Earth_sciences":0,"subject_of_expertise=Economics":0,"subject_of_expertise=Electrical_engineering":0,"subject_of_expertise=Family_and_consumer_science":0,"subject_of_expertise=Geography":0.0,"subject_of_expertise=Human_physical_performance_and_recreation":0.0,"subject_of_expertise=Linguistics_and_language":0.0,"subject_of_expertise=Literature":0,"subject_of_expertise=Materials_science_and_engineering":0.0,"subject_of_expertise=Mechanical_engineering":0,"subject_of_expertise=Media_studies_and_communication":0,"subject_of_expertise=Medicine_and_health":0.0,"subject_of_expertise=Military_sciences":0.0,"subject_of_expertise=Philosophy":0,"subject_of_expertise=Political_science":0.0,"subject_of_expertise=Social_work":0.0,"subject_of_expertise=Sociology":0,"subject_of_expertise=Space_sciences":0,"subject_of_expertise=System_science":0,"token_len_difference__min_val=0.0|max_val=0.33":0.0,"token_len_difference__min_val=0.33|max_val=0.67":0.0,"token_len_difference__min_val=0.67|max_val=1.0":0.0055714286,"type_of_in_context_material=1":0.0,"num_swaps":1597,"Chat":0.8016759777,"Chat Hard":0.6666666667,"Safety":0.6446009126,"Reasoning":0.8262774868,"Overall":0.7348052609,"alpacaeval-easy":0.8,"alpacaeval-hard":0.7894736842,"alpacaeval-length":0.7578947368,"donotanswer":0.6029411765,"hep-cpp":0.75,"hep-go":0.762195122,"hep-java":0.8292682927,"hep-js":0.8231707317,"hep-python":0.8048780488,"hep-rust":0.8048780488,"llmbar-adver-GPTInst":0.6413043478,"llmbar-adver-GPTOut":0.5319148936,"llmbar-adver-manual":0.6304347826,"llmbar-adver-neighbor":0.6119402985,"llmbar-natural":0.83,"math-prm":0.8568232662,"mt-bench-easy":0.8928571429,"mt-bench-hard":0.7027027027,"mt-bench-med":0.875,"refusals-dangerous":0.51,"refusals-offensive":0.27,"xstest-should-refuse":0.7012987013,"xstest-should-respond":0.92,"label":1}
{"hash":"c14eb4951569221ab6fae29e745256cb","model_type":"Seq. Classifier","chat_template":"tulu","bertscore__min_val=0.33|max_val=0.67":0.0,"bertscore__min_val=0.67|max_val=1.0":0.0,"bertscore_length__min_val=0.0|max_val=0.33":0.0,"bertscore_length__min_val=0.33|max_val=0.67":0.3665714286,"bertscore_length__min_val=0.67|max_val=1.0":0.0,"complexity_of_intents=complex":0.0,"complexity_of_intents=moderate":0.0,"complexity_of_intents=simple":0.0,"cosine_sim__min_val=0.0|max_val=0.33":0.0,"cosine_sim__min_val=0.33|max_val=0.67":0.0,"cosine_sim__min_val=0.67|max_val=1.0":0.0,"entity_sim__min_val=0.0|max_val=0.33":0.0,"entity_sim__min_val=0.33|max_val=0.67":0.0,"entity_sim__min_val=0.67|max_val=1.0":0.281,"expertise_level=basic_domain_knowledge":0.0,"expertise_level=expert_domain_knowledge":0.0,"expertise_level=general_public":0.0,"format_constraints=1":0.0,"languages=English":0.0,"len_longer__min_val=0.0|max_val=0.33":0.0,"len_longer__min_val=0.33|max_val=0.67":0.0,"len_longer__min_val=0.67|max_val=1.0":0.0,"len_shorter__min_val=0.0|max_val=0.33":0.0,"len_shorter__min_val=0.33|max_val=0.67":0.0,"len_shorter__min_val=0.67|max_val=1.0":0.0,"open_endedness=high":0.0,"open_endedness=low":0.0,"open_endedness=moderate":0.0,"open_endedness=no":0.0,"prompt_len__min_val=0.0|max_val=0.33":0.0,"prompt_len__min_val=0.33|max_val=0.67":0.0,"prompt_len__min_val=0.67|max_val=1.0":0.0,"rouge__min_val=0.0|max_val=0.33":0.0,"rouge__min_val=0.33|max_val=0.67":0.0,"rouge__min_val=0.67|max_val=1.0":0.0,"safety_concern=high":0.0,"safety_concern=low":0.0,"safety_concern=moderate":0,"safety_concern=safe":0.0,"subject_of_expertise=Agriculture":0.0,"subject_of_expertise=Business":0.0,"subject_of_expertise=Computer_sciences":0,"subject_of_expertise=Earth_sciences":0,"subject_of_expertise=Economics":0,"subject_of_expertise=Electrical_engineering":0,"subject_of_expertise=Family_and_consumer_science":0,"subject_of_expertise=Geography":0.0,"subject_of_expertise=Human_physical_performance_and_recreation":0.0,"subject_of_expertise=Linguistics_and_language":0.0,"subject_of_expertise=Literature":0,"subject_of_expertise=Materials_science_and_engineering":0.0,"subject_of_expertise=Mechanical_engineering":0,"subject_of_expertise=Media_studies_and_communication":0,"subject_of_expertise=Medicine_and_health":0.0,"subject_of_expertise=Military_sciences":0.0,"subject_of_expertise=Philosophy":0,"subject_of_expertise=Political_science":0.0,"subject_of_expertise=Social_work":0.0,"subject_of_expertise=Sociology":0,"subject_of_expertise=Space_sciences":0,"subject_of_expertise=System_science":0,"token_len_difference__min_val=0.0|max_val=0.33":0.0,"token_len_difference__min_val=0.33|max_val=0.67":0.0,"token_len_difference__min_val=0.67|max_val=1.0":0.0,"type_of_in_context_material=1":0.0,"num_swaps":682,"Chat":0.7877094972,"Chat Hard":0.6447368421,"Safety":0.4708509653,"Reasoning":0.7968025318,"Overall":0.6750249591,"alpacaeval-easy":0.76,"alpacaeval-hard":0.8105263158,"alpacaeval-length":0.7789473684,"donotanswer":0.5220588235,"hep-cpp":0.75,"hep-go":0.6890243902,"hep-java":0.7865853659,"hep-js":0.737804878,"hep-python":0.743902439,"hep-rust":0.7804878049,"llmbar-adver-GPTInst":0.6739130435,"llmbar-adver-GPTOut":0.4468085106,"llmbar-adver-manual":0.5869565217,"llmbar-adver-neighbor":0.5597014925,"llmbar-natural":0.85,"math-prm":0.8456375839,"mt-bench-easy":0.8571428571,"mt-bench-hard":0.6486486486,"mt-bench-med":0.775,"refusals-dangerous":0.25,"refusals-offensive":0.19,"xstest-should-refuse":0.3571428571,"xstest-should-respond":0.936,"label":1}
{"hash":"010fb8b84da911ae83921d53ccfd03fa","model_type":"Seq. Classifier","chat_template":"tulu","bertscore__min_val=0.33|max_val=0.67":0.0,"bertscore__min_val=0.67|max_val=1.0":0.0,"bertscore_length__min_val=0.0|max_val=0.33":0.0,"bertscore_length__min_val=0.33|max_val=0.67":0.3611428571,"bertscore_length__min_val=0.67|max_val=1.0":0.0,"complexity_of_intents=complex":0.0,"complexity_of_intents=moderate":0.0,"complexity_of_intents=simple":0.0,"cosine_sim__min_val=0.0|max_val=0.33":0.0,"cosine_sim__min_val=0.33|max_val=0.67":0.0,"cosine_sim__min_val=0.67|max_val=1.0":0.0,"entity_sim__min_val=0.0|max_val=0.33":0.0,"entity_sim__min_val=0.33|max_val=0.67":0.0,"entity_sim__min_val=0.67|max_val=1.0":0.2758571429,"expertise_level=basic_domain_knowledge":0.0,"expertise_level=expert_domain_knowledge":0.0,"expertise_level=general_public":0.0,"format_constraints=1":0.0,"languages=English":0.0,"len_longer__min_val=0.0|max_val=0.33":0.0,"len_longer__min_val=0.33|max_val=0.67":0.0,"len_longer__min_val=0.67|max_val=1.0":0.0,"len_shorter__min_val=0.0|max_val=0.33":0.0,"len_shorter__min_val=0.33|max_val=0.67":0.0,"len_shorter__min_val=0.67|max_val=1.0":0.0,"open_endedness=high":0.0,"open_endedness=low":0.0,"open_endedness=moderate":0.0,"open_endedness=no":0.0,"prompt_len__min_val=0.0|max_val=0.33":0.0,"prompt_len__min_val=0.33|max_val=0.67":0.0,"prompt_len__min_val=0.67|max_val=1.0":0.0,"rouge__min_val=0.0|max_val=0.33":0.2844285714,"rouge__min_val=0.33|max_val=0.67":0.0,"rouge__min_val=0.67|max_val=1.0":0.0,"safety_concern=high":0.0,"safety_concern=low":0.0,"safety_concern=moderate":0,"safety_concern=safe":0.0,"subject_of_expertise=Agriculture":0.0,"subject_of_expertise=Business":0.0,"subject_of_expertise=Computer_sciences":0,"subject_of_expertise=Earth_sciences":0,"subject_of_expertise=Economics":0,"subject_of_expertise=Electrical_engineering":0,"subject_of_expertise=Family_and_consumer_science":0,"subject_of_expertise=Geography":0.0,"subject_of_expertise=Human_physical_performance_and_recreation":0.0,"subject_of_expertise=Linguistics_and_language":0.0,"subject_of_expertise=Literature":0,"subject_of_expertise=Materials_science_and_engineering":0.0,"subject_of_expertise=Mechanical_engineering":0,"subject_of_expertise=Media_studies_and_communication":0,"subject_of_expertise=Medicine_and_health":0.0,"subject_of_expertise=Military_sciences":0.0,"subject_of_expertise=Philosophy":0,"subject_of_expertise=Political_science":0.0,"subject_of_expertise=Social_work":0.0,"subject_of_expertise=Sociology":0,"subject_of_expertise=Space_sciences":0,"subject_of_expertise=System_science":0,"token_len_difference__min_val=0.0|max_val=0.33":0.0,"token_len_difference__min_val=0.33|max_val=0.67":0.0,"token_len_difference__min_val=0.67|max_val=1.0":0.0,"type_of_in_context_material=1":0.0,"num_swaps":53,"Chat":0.782122905,"Chat Hard":0.6425438596,"Safety":0.4338447174,"Reasoning":0.8264786926,"Overall":0.6712475437,"alpacaeval-easy":0.77,"alpacaeval-hard":0.7894736842,"alpacaeval-length":0.7368421053,"donotanswer":0.4632352941,"hep-cpp":0.7926829268,"hep-go":0.8231707317,"hep-java":0.8414634146,"hep-js":0.8170731707,"hep-python":0.7682926829,"hep-rust":0.7743902439,"llmbar-adver-GPTInst":0.6195652174,"llmbar-adver-GPTOut":0.4680851064,"llmbar-adver-manual":0.5869565217,"llmbar-adver-neighbor":0.6417910448,"llmbar-natural":0.78,"math-prm":0.8501118568,"mt-bench-easy":0.8571428571,"mt-bench-hard":0.6216216216,"mt-bench-med":0.85,"refusals-dangerous":0.14,"refusals-offensive":0.17,"xstest-should-refuse":0.3636363636,"xstest-should-respond":0.884,"label":1}
{"hash":"3be011e834fa05bbe6ea47c3d76ab7ca","model_type":"Seq. Classifier","chat_template":"tulu","bertscore__min_val=0.33|max_val=0.67":0.0,"bertscore__min_val=0.67|max_val=1.0":0.0,"bertscore_length__min_val=0.0|max_val=0.33":0.0,"bertscore_length__min_val=0.33|max_val=0.67":0.0,"bertscore_length__min_val=0.67|max_val=1.0":0.0,"complexity_of_intents=complex":0.0,"complexity_of_intents=moderate":0.0,"complexity_of_intents=simple":0.0,"cosine_sim__min_val=0.0|max_val=0.33":0.0,"cosine_sim__min_val=0.33|max_val=0.67":0.2347142857,"cosine_sim__min_val=0.67|max_val=1.0":0.0,"entity_sim__min_val=0.0|max_val=0.33":0.0,"entity_sim__min_val=0.33|max_val=0.67":0.2064285714,"entity_sim__min_val=0.67|max_val=1.0":0.0,"expertise_level=basic_domain_knowledge":0.0,"expertise_level=expert_domain_knowledge":0.0,"expertise_level=general_public":0.0,"format_constraints=1":0.0,"languages=English":0.0,"len_longer__min_val=0.0|max_val=0.33":0.0,"len_longer__min_val=0.33|max_val=0.67":0.0,"len_longer__min_val=0.67|max_val=1.0":0.0,"len_shorter__min_val=0.0|max_val=0.33":0.0,"len_shorter__min_val=0.33|max_val=0.67":0.0,"len_shorter__min_val=0.67|max_val=1.0":0.0,"open_endedness=high":0.0,"open_endedness=low":0.0,"open_endedness=moderate":0.0,"open_endedness=no":0.0,"prompt_len__min_val=0.0|max_val=0.33":0.0,"prompt_len__min_val=0.33|max_val=0.67":0.0,"prompt_len__min_val=0.67|max_val=1.0":0.0,"rouge__min_val=0.0|max_val=0.33":0.2841428571,"rouge__min_val=0.33|max_val=0.67":0.0,"rouge__min_val=0.67|max_val=1.0":0.0,"safety_concern=high":0.0,"safety_concern=low":0.0,"safety_concern=moderate":0,"safety_concern=safe":0.0,"subject_of_expertise=Agriculture":0.0,"subject_of_expertise=Business":0.0,"subject_of_expertise=Computer_sciences":0,"subject_of_expertise=Earth_sciences":0,"subject_of_expertise=Economics":0,"subject_of_expertise=Electrical_engineering":0,"subject_of_expertise=Family_and_consumer_science":0,"subject_of_expertise=Geography":0.0,"subject_of_expertise=Human_physical_performance_and_recreation":0.0,"subject_of_expertise=Linguistics_and_language":0.0,"subject_of_expertise=Literature":0,"subject_of_expertise=Materials_science_and_engineering":0.0,"subject_of_expertise=Mechanical_engineering":0,"subject_of_expertise=Media_studies_and_communication":0,"subject_of_expertise=Medicine_and_health":0.0,"subject_of_expertise=Military_sciences":0.0,"subject_of_expertise=Philosophy":0,"subject_of_expertise=Political_science":0.0,"subject_of_expertise=Social_work":0.0,"subject_of_expertise=Sociology":0,"subject_of_expertise=Space_sciences":0,"subject_of_expertise=System_science":0,"token_len_difference__min_val=0.0|max_val=0.33":0.0,"token_len_difference__min_val=0.33|max_val=0.67":0.0,"token_len_difference__min_val=0.67|max_val=1.0":0.0064285714,"type_of_in_context_material=1":0.0,"num_swaps":67,"Chat":0.7625698324,"Chat Hard":0.6535087719,"Safety":0.4570913303,"Reasoning":0.8537233317,"Overall":0.6817233166,"alpacaeval-easy":0.74,"alpacaeval-hard":0.7368421053,"alpacaeval-length":0.7368421053,"donotanswer":0.4485294118,"hep-cpp":0.8353658537,"hep-go":0.8231707317,"hep-java":0.8780487805,"hep-js":0.8109756098,"hep-python":0.8536585366,"hep-rust":0.7682926829,"llmbar-adver-GPTInst":0.652173913,"llmbar-adver-GPTOut":0.4255319149,"llmbar-adver-manual":0.5869565217,"llmbar-adver-neighbor":0.6641791045,"llmbar-natural":0.78,"math-prm":0.8791946309,"mt-bench-easy":0.8928571429,"mt-bench-hard":0.6486486486,"mt-bench-med":0.85,"refusals-dangerous":0.25,"refusals-offensive":0.28,"xstest-should-refuse":0.3376623377,"xstest-should-respond":0.908,"label":1}
{"hash":"be03c44b9131fcdc22570e2160a4a515","model_type":"Seq. Classifier","chat_template":"tulu","bertscore__min_val=0.33|max_val=0.67":0.0,"bertscore__min_val=0.67|max_val=1.0":0.0,"bertscore_length__min_val=0.0|max_val=0.33":0.0,"bertscore_length__min_val=0.33|max_val=0.67":0.0,"bertscore_length__min_val=0.67|max_val=1.0":0.0,"complexity_of_intents=complex":0.0,"complexity_of_intents=moderate":0.0,"complexity_of_intents=simple":0.0,"cosine_sim__min_val=0.0|max_val=0.33":0.0,"cosine_sim__min_val=0.33|max_val=0.67":0.0,"cosine_sim__min_val=0.67|max_val=1.0":0.0,"entity_sim__min_val=0.0|max_val=0.33":0.0,"entity_sim__min_val=0.33|max_val=0.67":0.0,"entity_sim__min_val=0.67|max_val=1.0":0.0,"expertise_level=basic_domain_knowledge":0.0,"expertise_level=expert_domain_knowledge":0.0,"expertise_level=general_public":0.0,"format_constraints=1":0.0,"languages=English":0.0,"len_longer__min_val=0.0|max_val=0.33":0.0,"len_longer__min_val=0.33|max_val=0.67":0.0,"len_longer__min_val=0.67|max_val=1.0":0.0,"len_shorter__min_val=0.0|max_val=0.33":0.0,"len_shorter__min_val=0.33|max_val=0.67":0.0,"len_shorter__min_val=0.67|max_val=1.0":0.0,"open_endedness=high":0.4,"open_endedness=low":0.0,"open_endedness=moderate":0.0,"open_endedness=no":0.0,"prompt_len__min_val=0.0|max_val=0.33":0.0,"prompt_len__min_val=0.33|max_val=0.67":0.0,"prompt_len__min_val=0.67|max_val=1.0":0.0,"rouge__min_val=0.0|max_val=0.33":0.0,"rouge__min_val=0.33|max_val=0.67":0.0,"rouge__min_val=0.67|max_val=1.0":0.0,"safety_concern=high":0.0,"safety_concern=low":0.0,"safety_concern=moderate":0,"safety_concern=safe":0.0,"subject_of_expertise=Agriculture":0.0,"subject_of_expertise=Business":0.0,"subject_of_expertise=Computer_sciences":0,"subject_of_expertise=Earth_sciences":0,"subject_of_expertise=Economics":0,"subject_of_expertise=Electrical_engineering":0,"subject_of_expertise=Family_and_consumer_science":0,"subject_of_expertise=Geography":0.0,"subject_of_expertise=Human_physical_performance_and_recreation":0.0,"subject_of_expertise=Linguistics_and_language":0.0,"subject_of_expertise=Literature":0,"subject_of_expertise=Materials_science_and_engineering":0.0,"subject_of_expertise=Mechanical_engineering":0,"subject_of_expertise=Media_studies_and_communication":0,"subject_of_expertise=Medicine_and_health":0.0,"subject_of_expertise=Military_sciences":0.0,"subject_of_expertise=Philosophy":0,"subject_of_expertise=Political_science":0.0,"subject_of_expertise=Social_work":0.0,"subject_of_expertise=Sociology":0,"subject_of_expertise=Space_sciences":0,"subject_of_expertise=System_science":0,"token_len_difference__min_val=0.0|max_val=0.33":0.0,"token_len_difference__min_val=0.33|max_val=0.67":0.0,"token_len_difference__min_val=0.67|max_val=1.0":0.0,"type_of_in_context_material=1":0.0,"num_swaps":2800,"Chat":0.7960893855,"Chat Hard":0.6271929825,"Safety":0.8270871183,"Reasoning":0.8410234899,"Overall":0.772848244,"alpacaeval-easy":0.78,"alpacaeval-hard":0.7894736842,"alpacaeval-length":0.8,"donotanswer":0.7573529412,"hep-cpp":0.762195122,"hep-go":0.7682926829,"hep-java":0.8414634146,"hep-js":0.8292682927,"hep-python":0.7682926829,"hep-rust":0.7804878049,"llmbar-adver-GPTInst":0.6739130435,"llmbar-adver-GPTOut":0.3191489362,"llmbar-adver-manual":0.5869565217,"llmbar-adver-neighbor":0.6119402985,"llmbar-natural":0.77,"math-prm":0.8903803132,"mt-bench-easy":0.8571428571,"mt-bench-hard":0.6216216216,"mt-bench-med":0.8,"refusals-dangerous":0.83,"refusals-offensive":0.81,"xstest-should-refuse":0.8701298701,"xstest-should-respond":0.828,"label":1}
{"hash":"c3d049483cffd16f50d72acb91467706","model_type":"Seq. Classifier","chat_template":"tulu","bertscore__min_val=0.33|max_val=0.67":0.0,"bertscore__min_val=0.67|max_val=1.0":0.0,"bertscore_length__min_val=0.0|max_val=0.33":0.0,"bertscore_length__min_val=0.33|max_val=0.67":0.369,"bertscore_length__min_val=0.67|max_val=1.0":0.0,"complexity_of_intents=complex":0.0,"complexity_of_intents=moderate":0.0,"complexity_of_intents=simple":0.0,"cosine_sim__min_val=0.0|max_val=0.33":0.0,"cosine_sim__min_val=0.33|max_val=0.67":0.0,"cosine_sim__min_val=0.67|max_val=1.0":0.0,"entity_sim__min_val=0.0|max_val=0.33":0.0,"entity_sim__min_val=0.33|max_val=0.67":0.0,"entity_sim__min_val=0.67|max_val=1.0":0.2832857143,"expertise_level=basic_domain_knowledge":0.0,"expertise_level=expert_domain_knowledge":0.0,"expertise_level=general_public":0.0,"format_constraints=1":0.0,"languages=English":0.0,"len_longer__min_val=0.0|max_val=0.33":0.0,"len_longer__min_val=0.33|max_val=0.67":0.0,"len_longer__min_val=0.67|max_val=1.0":0.0,"len_shorter__min_val=0.0|max_val=0.33":0.0,"len_shorter__min_val=0.33|max_val=0.67":0.0,"len_shorter__min_val=0.67|max_val=1.0":0.0012857143,"open_endedness=high":0.0,"open_endedness=low":0.0,"open_endedness=moderate":0.0,"open_endedness=no":0.0,"prompt_len__min_val=0.0|max_val=0.33":0.0,"prompt_len__min_val=0.33|max_val=0.67":0.0,"prompt_len__min_val=0.67|max_val=1.0":0.0,"rouge__min_val=0.0|max_val=0.33":0.0,"rouge__min_val=0.33|max_val=0.67":0.0,"rouge__min_val=0.67|max_val=1.0":0.0,"safety_concern=high":0.0,"safety_concern=low":0.0,"safety_concern=moderate":0,"safety_concern=safe":0.0,"subject_of_expertise=Agriculture":0.0,"subject_of_expertise=Business":0.0,"subject_of_expertise=Computer_sciences":0,"subject_of_expertise=Earth_sciences":0,"subject_of_expertise=Economics":0,"subject_of_expertise=Electrical_engineering":0,"subject_of_expertise=Family_and_consumer_science":0,"subject_of_expertise=Geography":0.0,"subject_of_expertise=Human_physical_performance_and_recreation":0.0,"subject_of_expertise=Linguistics_and_language":0.0,"subject_of_expertise=Literature":0,"subject_of_expertise=Materials_science_and_engineering":0.0,"subject_of_expertise=Mechanical_engineering":0,"subject_of_expertise=Media_studies_and_communication":0,"subject_of_expertise=Medicine_and_health":0.0,"subject_of_expertise=Military_sciences":0.0,"subject_of_expertise=Philosophy":0,"subject_of_expertise=Political_science":0.0,"subject_of_expertise=Social_work":0.0,"subject_of_expertise=Sociology":0,"subject_of_expertise=Space_sciences":0,"subject_of_expertise=System_science":0,"token_len_difference__min_val=0.0|max_val=0.33":0.0018571429,"token_len_difference__min_val=0.33|max_val=0.67":0.0,"token_len_difference__min_val=0.67|max_val=1.0":0.0,"type_of_in_context_material=1":0.0,"num_swaps":681,"Chat":0.8044692737,"Chat Hard":0.6513157895,"Safety":0.4399553528,"Reasoning":0.8301379113,"Overall":0.6814695818,"alpacaeval-easy":0.83,"alpacaeval-hard":0.8,"alpacaeval-length":0.7473684211,"donotanswer":0.4558823529,"hep-cpp":0.8109756098,"hep-go":0.756097561,"hep-java":0.8414634146,"hep-js":0.7926829268,"hep-python":0.8170731707,"hep-rust":0.8292682927,"llmbar-adver-GPTInst":0.5326086957,"llmbar-adver-GPTOut":0.5319148936,"llmbar-adver-manual":0.6086956522,"llmbar-adver-neighbor":0.6492537313,"llmbar-natural":0.86,"math-prm":0.8523489933,"mt-bench-easy":0.8571428571,"mt-bench-hard":0.5945945946,"mt-bench-med":0.85,"refusals-dangerous":0.1,"refusals-offensive":0.24,"xstest-should-refuse":0.3441558442,"xstest-should-respond":0.932,"label":1}
