{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "from ast import literal_eval\n",
    "from pathlib import Path\n",
    "from functools import reduce\n",
    "\n",
    "from src.utils import find_meta_category\n",
    "from src.feature_extractor import sample_feature_combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download prerequisite files\n",
    "\n",
    "Fetch all the results and feature values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching experiments list...\n",
      "Downloading dataset \u001b[36m01J6KF3JRCATRJQ9CPJTRV5VBM\u001b[0m to \u001b[32m.\u001b[0m\n",
      "Files: 0          ⠋  \n",
      "Bytes: 0 B        ⠋  \n",
      "\u001b[2A\u001b[JFiles: 1          ⠙  \n",
      "Bytes: 73.77 KiB  ⠙  \n",
      "\u001b[2A\u001b[JFiles: 1          ✔  \n",
      "Bytes: 73.77 KiB  ✔  \n",
      "\u001b[2A\u001b[JFiles: 1          ✔  \n",
      "Bytes: 73.77 KiB  ✔  \n",
      "Completed in 100ms: 439.4 KiB/s, 6 files/s\n",
      "Fetching extracted features...\n",
      "mkdir: features/: File exists\n",
      "Downloading dataset \u001b[36m01J6KF3JRCATRJQ9CPJTRV5VBM\u001b[0m to \u001b[32m.\u001b[0m\n",
      "Files: 0          ⠋  \n",
      "Bytes: 0 B        ⠋  \n",
      "\u001b[2A\u001b[JFiles: 2          ⠙  \n",
      "Bytes: 75.1 MiB   ⠙  \n",
      "\u001b[2A\u001b[JFiles: 9          ⠹  \n",
      "Bytes: 339 MiB    ⠹  \n",
      "\u001b[2A\u001b[JFiles: 16         ⠸  \n",
      "Bytes: 602.9 MiB  ⠸  \n",
      "\u001b[2A\u001b[JFiles: 16         ✔  \n",
      "Bytes: 602.9 MiB  ✔  \n",
      "\u001b[2A\u001b[JFiles: 16         ✔  \n",
      "Bytes: 602.9 MiB  ✔  \n",
      "Completed in 400ms: 1.23 GiB/s, 33 files/s\n",
      "Fetching helpsteer2 dataset\n",
      "Downloading dataset \u001b[36m01J6KBM2VCM9EQ7MER26VBXCCM\u001b[0m to \u001b[32m.\u001b[0m\n",
      "Files: 0          ⠋  \n",
      "Bytes: 0 B        ⠋  \n",
      "\u001b[2A\u001b[JFiles: 1          ⠙  \n",
      "Bytes: 70.58 MiB  ⠙  \n",
      "\u001b[2A\u001b[JFiles: 1          ✔  \n",
      "Bytes: 70.58 MiB  ✔  \n",
      "\u001b[2A\u001b[JFiles: 1          ✔  \n",
      "Bytes: 70.58 MiB  ✔  \n",
      "Completed in 100ms: 362.2 MiB/s, 5 files/s\n",
      "Collating all evaluation results\n",
      "2024-09-02 12:59:53 - INFO - root - Logged-in as ljm (ljm@allenai.org)\n",
      "2024-09-02 12:59:54 - INFO - root - Found 194 experiments that match 'rm-eval-helpsteer2'\n",
      "2024-09-02 13:00:18 - INFO - root - Computing category scores...\n",
      "2024-09-02 13:00:18 - INFO - root - Deriving features from the experiments file: experiments.txt\n",
      "2024-09-02 13:00:18 - INFO - root - Will attempt merge via feature hash\n",
      "2024-09-02 13:00:18 - INFO - root - Creating labels in column 'label' with GPT-4 threshold '0.658'\n",
      "2024-09-02 13:00:18 - INFO - root - Saving 62 results to results.csv\n",
      "2024-09-02 13:00:18 - INFO - root - Saved on results.csv\n"
     ]
    }
   ],
   "source": [
    "# You can get the experiments file here: 01J6KF3JRCATRJQ9CPJTRV5VBM (https://beaker.org/ds/01J6KF3JRCATRJQ9CPJTRV5VBM/details)\n",
    "!echo \"Fetching experiments list...\"\n",
    "!beaker dataset fetch 01J6KF3JRCATRJQ9CPJTRV5VBM --prefix experiments.txt\n",
    "!echo \"Fetching extracted features...\"\n",
    "!mkdir features/\n",
    "!beaker dataset fetch 01J6KF3JRCATRJQ9CPJTRV5VBM --prefix features/ \n",
    "#!beaker dataset fetch 01J6KFVCRCTYHCZDR0XNK0G9HT --prefix features/\n",
    "!echo \"Fetching helpsteer2 dataset\"\n",
    "!beaker dataset fetch 01J6KBM2VCM9EQ7MER26VBXCCM\n",
    "!echo \"Collating all evaluation results\"\n",
    "%run ../../scripts/fetch_evals_rewardbench.py --output_file results.csv --gpt4_threshold_score 0.658 --experiment_prefix rm-eval-helpsteer2 --experiments_file experiments.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collate feature set for all instances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEXICAL_FEATS_PATH = Path(\"features\")\n",
    "DATASET_PATH = Path(\"helpsteer2_human_vs_gpt4_weighted_for_llama.jsonl\")\n",
    "\n",
    "\n",
    "def get_dataset_features(\n",
    "    feature_path=LEXICAL_FEATS_PATH, dataset_path=DATASET_PATH\n",
    ") -> \"pd.DataFrame\":\n",
    "    lexical_features = [\n",
    "        \"rouge\",\n",
    "        \"bertscore\",\n",
    "        \"bertscore_length\",\n",
    "        \"entity_sim\",\n",
    "        \"cosine_sim\",\n",
    "        \"prompt_len\",\n",
    "        \"len_longer\",\n",
    "        \"len_shorter\",\n",
    "        \"token_len_difference\",\n",
    "    ]\n",
    "    lexical_feature_files = [\n",
    "        file\n",
    "        for file in feature_path.glob(\"*.jsonl\")\n",
    "        if any(file.stem in feat for feat in lexical_features)\n",
    "    ]\n",
    "    lexical_feats_df = reduce(\n",
    "        lambda left, right: left.merge(\n",
    "            right, on=[\"id\", \"prompt\", \"completion_a\", \"completion_b\"], how=\"outer\"\n",
    "        ),\n",
    "        [pd.read_json(file, lines=True) for file in lexical_feature_files],\n",
    "    )\n",
    "\n",
    "    df = pd.read_json(dataset_path, lines=True).rename(columns={\"prompt_hash\": \"id\"})\n",
    "    finaldf = df.merge(lexical_feats_df, how=\"left\", on=\"id\").drop(\n",
    "        columns=[\"prompt\", \"completion_a\", \"completion_b\"]\n",
    "    )\n",
    "\n",
    "    # Hacky way for token_len_difference\n",
    "    finaldf = finaldf.rename(columns={\"token_len_diff\": \"token_len_difference\"})\n",
    "    return finaldf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.read_csv(\"results.csv\").dropna()\n",
    "features_df = get_dataset_features()\n",
    "print(len(results_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get proportion of instances that fulfill the conditions\n",
    "\n",
    "1. For each row, get features that were activated\n",
    "2. Then for each activated feature, we get the proportion by looking at the feature dataframe.\n",
    "3. The proportion is computed as: `number_of_instance_that_fulfill_a_single_condition` / `total_number_of_instances`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>expertise_level</th>\n",
       "      <th>format_constraints</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1317</th>\n",
       "      <td>expert domain knowledge</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4613</th>\n",
       "      <td>basic domain knowledge</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4734</th>\n",
       "      <td>general public</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              expertise_level format_constraints\n",
       "289                      None                 []\n",
       "1317  expert domain knowledge               None\n",
       "4613   basic domain knowledge               None\n",
       "4734           general public               None"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect nan columns\n",
    "rows_with_nan = features_df[features_df.isna().any(axis=1)]\n",
    "nan_columns = rows_with_nan.columns[rows_with_nan.isna().any()]\n",
    "df_nan_columns = rows_with_nan[nan_columns]\n",
    "df_nan_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what you're going to do instead, is to take the binary_cols, and then for each element of that binary_cols, you compute the \"weight\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_instances(feat: str, features_df: \"pd.DataFrame\") -> float:\n",
    "    total = len(features_df)\n",
    "    lexical_features = [\n",
    "        \"rouge\",\n",
    "        \"bertscore\",\n",
    "        \"bertscore_length\",\n",
    "        \"entity_sim\",\n",
    "        \"cosine_sim\",\n",
    "        \"prompt_len\",\n",
    "        \"len_longer\",\n",
    "        \"len_shorter\",\n",
    "        \"token_len_difference\",\n",
    "    ]\n",
    "\n",
    "    if feat.split(\"__\")[0] in lexical_features:\n",
    "        feat_name, value = feat.split(\"__\")\n",
    "        min_val_str, max_val_str = value.split(\"|\")\n",
    "        min_val, max_val = float(min_val_str.split(\"=\")[1]), float(\n",
    "            max_val_str.split(\"=\")[1]\n",
    "        )\n",
    "        return features_df[feat_name].between(min_val, max_val).mean()\n",
    "    else:\n",
    "        # Parse the feature\n",
    "        feat_name, value = feat.split(\"=\")\n",
    "        meta_category = find_meta_category(feat_name)\n",
    "        if meta_category == \"scalar\":\n",
    "            v = value.replace(\"_\", \" \")\n",
    "            return features_df[feat_name].value_counts().get(v) / total\n",
    "        elif meta_category == \"closed_set\":\n",
    "            v = value.replace(\"_\", \" \")\n",
    "            list_of_values = features_df[feat_name].tolist()\n",
    "            return sum([1 if v in listval else 0 for listval in list_of_values]) / total\n",
    "        elif meta_category == \"open_set\":\n",
    "            list_of_values = features_df[feat_name].tolist()\n",
    "            return sum([1 if listval else 0 for listval in list_of_values]) / total\n",
    "\n",
    "        return find_meta_category(feat_name)\n",
    "\n",
    "\n",
    "feats = results_df.columns[results_df.isin([0, 1]).all()]  # get binary columns\n",
    "feat_map = {\n",
    "    feat: compute_instances(feat, features_df) for feat in feats if feat != \"label\"\n",
    "}\n",
    "\n",
    "ratio_df = results_df.apply(\n",
    "    lambda row: row.map(lambda x: feat_map.get(row.name, 1) if x == 1 else x)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressor training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LightGBM regressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 55, test size: 7\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 55, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score 0.695253\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "Mean Squared Error: 0.0013998446533237798\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"objective\": \"regression\",\n",
    "    \"metric\": \"mse\",\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"num_leaves\": 31,\n",
    "}\n",
    "\n",
    "# Train the model\n",
    "binary = False\n",
    "X = ratio_df[list(feat_map.keys())]\n",
    "if binary:\n",
    "    X = (X > 0).astype(int)\n",
    "y = ratio_df[\"Overall\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.1, random_state=42\n",
    ")\n",
    "print(f\"Train size: {len(X_train)}, test size: {len(X_test)}\")\n",
    "\n",
    "\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
    "model = lgb.train(params, train_data, valid_sets=[test_data])\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  Feature  Importance\n",
      "0    bertscore__min_val=0.33|max_val=0.67         0.0\n",
      "33       rouge__min_val=0.33|max_val=0.67         0.0\n",
      "35                    safety_concern=high         0.0\n",
      "36                     safety_concern=low         0.0\n",
      "37                safety_concern=moderate         0.0\n",
      "..                                    ...         ...\n",
      "27                open_endedness=moderate         0.0\n",
      "28                      open_endedness=no         0.0\n",
      "29   prompt_len__min_val=0.0|max_val=0.33         0.0\n",
      "30  prompt_len__min_val=0.33|max_val=0.67         0.0\n",
      "64          type_of_in_context_material=1         0.0\n",
      "\n",
      "[65 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "importances = model.feature_importance(importance_type=\"gain\")  # ['split', 'gain']\n",
    "\n",
    "# Create a DataFrame to view feature importances\n",
    "feature_importance_df = pd.DataFrame(\n",
    "    {\"Feature\": X.columns, \"Importance\": importances}\n",
    ").sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "print(feature_importance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LinearRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 49, test size: 13\n",
      "Feature names: ['bertscore__min_val=0.33|max_val=0.67'\n",
      " 'bertscore__min_val=0.67|max_val=1.0'\n",
      " 'bertscore_length__min_val=0.0|max_val=0.33' ...\n",
      " 'token_len_difference__min_val=0.67|max_val=1.0^2'\n",
      " 'token_len_difference__min_val=0.67|max_val=1.0 type_of_in_context_material=1'\n",
      " 'type_of_in_context_material=1^2']\n",
      "Mean Squared Error: 0.0012591021224480326\n",
      "Coeeficients: [-4.07087444e-01  2.64518756e-02  7.84504190e-02 ...  1.13884130e-03\n",
      "  0.00000000e+00  3.17208325e-04]\n",
      "Intercept: 0.7141910311097194\n"
     ]
    }
   ],
   "source": [
    "polyfit = True\n",
    "binary = False\n",
    "\n",
    "X = ratio_df[list(feat_map.keys())]\n",
    "y = ratio_df[\"Overall\"]\n",
    "if binary:\n",
    "    X = (X > 0).astype(int)\n",
    "\n",
    "if polyfit:\n",
    "    poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "    X_poly = poly.fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_poly, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "else:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "\n",
    "print(f\"Train size: {len(X_train)}, test size: {len(X_test)}\")\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Feature names: {poly.get_feature_names_out(X.columns)}\")\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"Coeeficients: {model.coef_}\")\n",
    "print(f\"Intercept: {model.intercept_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance is not possible with polynomial features (hard to interpret)\n"
     ]
    }
   ],
   "source": [
    "if not polyfit:\n",
    "    feature_importance = pd.DataFrame(\n",
    "        {\"Feature\": X.columns, \"Coefficient\": model.coef_}\n",
    "    )\n",
    "\n",
    "    # Calculate absolute importance for easier comparison\n",
    "    feature_importance[\"Absolute_Coefficient\"] = np.abs(\n",
    "        feature_importance[\"Coefficient\"]\n",
    "    )\n",
    "\n",
    "    # Sort by absolute coefficient value\n",
    "    feature_importance = feature_importance.sort_values(\n",
    "        by=\"Absolute_Coefficient\", ascending=False\n",
    "    )\n",
    "    feature_importance.head(10)\n",
    "else:\n",
    "    print(\n",
    "        \"Feature importance is not possible with polynomial features (hard to interpret)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:00, 46500.04it/s]\n",
      "45it [00:00, 68609.12it/s]\n",
      "120it [00:00, 62859.56it/s]\n",
      "210it [00:00, 50467.19it/s]\n",
      "252it [00:00, 49643.73it/s]\n",
      "210it [00:00, 41807.66it/s]\n",
      "120it [00:00, 34911.32it/s]\n",
      "45it [00:00, 30795.18it/s]\n",
      "10it [00:00, 20794.76it/s]\n",
      "1it [00:00, 8924.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-02 13:00:21 - INFO - root - Adding meta analyzer features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "10it [00:00, 110960.42it/s]\n",
      "45it [00:00, 106936.93it/s]\n",
      "120it [00:00, 75527.68it/s]\n",
      "210it [00:00, 62015.34it/s]\n",
      "252it [00:00, 48155.48it/s]\n",
      "210it [00:00, 41389.21it/s]\n",
      "120it [00:00, 35037.69it/s]\n",
      "45it [00:00, 29217.29it/s]\n",
      "10it [00:00, 21421.37it/s]\n",
      "1it [00:00, 15650.39it/s]\n"
     ]
    }
   ],
   "source": [
    "_, combinations = sample_feature_combinations(\n",
    "    meta_analyzer_n_samples=2000, max_number=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hx/gk2rs0792pn5p8hkj4nkhdm80000gp/T/ipykernel_36621/4023816742.py:2: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for idx, combination in tqdm_notebook(enumerate(combinations), total=len(combinations)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "866653ce3db44a49b1fd1da6d53de371",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4069 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sim_df = pd.DataFrame(0, index=np.arange(len(combinations)), columns=X.columns)\n",
    "for idx, combination in tqdm_notebook(enumerate(combinations), total=len(combinations)):\n",
    "    activated_feats = []\n",
    "    for feat in combination:\n",
    "        if \"analyzer\" in feat:\n",
    "            feature_name_str, value_str = feat.split(\"::\")[1].split(\"|\")\n",
    "            feature_name, value = (\n",
    "                feature_name_str.split(\"=\")[-1],\n",
    "                value_str.split(\"=\")[-1],\n",
    "            )\n",
    "            activated_feats.append(f\"{feature_name}={value}\")\n",
    "        else:\n",
    "            activated_feats.append(feat.replace(\"::\", \"__\"))\n",
    "    sim_df.loc[idx, activated_feats] = 1\n",
    "sim_df = sim_df.apply(\n",
    "    lambda row: row.map(lambda x: feat_map.get(row.name, 1) if x == 1 else x)\n",
    ").dropna(axis=1, how=\"any\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activated_features</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[bertscore__min_val=0.67|max_val=1.0, complexi...</td>\n",
       "      <td>0.812256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[bertscore_length__min_val=0.0|max_val=0.33, e...</td>\n",
       "      <td>0.810126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[complexity_of_intents=simple, safety_concern=...</td>\n",
       "      <td>0.800891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[bertscore__min_val=0.67|max_val=1.0, cosine_s...</td>\n",
       "      <td>0.799207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[bertscore__min_val=0.67|max_val=1.0, cosine_s...</td>\n",
       "      <td>0.798044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[bertscore_length__min_val=0.33|max_val=0.67, ...</td>\n",
       "      <td>0.792909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[complexity_of_intents=simple, format_constrai...</td>\n",
       "      <td>0.791309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[complexity_of_intents=simple, format_constrai...</td>\n",
       "      <td>0.791309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[complexity_of_intents=simple, format_constrai...</td>\n",
       "      <td>0.790966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[complexity_of_intents=simple, format_constrai...</td>\n",
       "      <td>0.790966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[complexity_of_intents=simple, format_constrai...</td>\n",
       "      <td>0.790787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[complexity_of_intents=simple, open_endedness=...</td>\n",
       "      <td>0.789959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[bertscore_length__min_val=0.0|max_val=0.33, c...</td>\n",
       "      <td>0.788190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[bertscore__min_val=0.67|max_val=1.0, cosine_s...</td>\n",
       "      <td>0.784764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[bertscore__min_val=0.33|max_val=0.67, bertsco...</td>\n",
       "      <td>0.782252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[bertscore__min_val=0.67|max_val=1.0, complexi...</td>\n",
       "      <td>0.782118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[bertscore__min_val=0.67|max_val=1.0, entity_s...</td>\n",
       "      <td>0.781140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[complexity_of_intents=simple, entity_sim__min...</td>\n",
       "      <td>0.779665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[bertscore_length__min_val=0.0|max_val=0.33, l...</td>\n",
       "      <td>0.779619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[bertscore_length__min_val=0.0|max_val=0.33, e...</td>\n",
       "      <td>0.779271</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   activated_features      pred\n",
       "0   [bertscore__min_val=0.67|max_val=1.0, complexi...  0.812256\n",
       "1   [bertscore_length__min_val=0.0|max_val=0.33, e...  0.810126\n",
       "2   [complexity_of_intents=simple, safety_concern=...  0.800891\n",
       "3   [bertscore__min_val=0.67|max_val=1.0, cosine_s...  0.799207\n",
       "4   [bertscore__min_val=0.67|max_val=1.0, cosine_s...  0.798044\n",
       "5   [bertscore_length__min_val=0.33|max_val=0.67, ...  0.792909\n",
       "6   [complexity_of_intents=simple, format_constrai...  0.791309\n",
       "7   [complexity_of_intents=simple, format_constrai...  0.791309\n",
       "8   [complexity_of_intents=simple, format_constrai...  0.790966\n",
       "9   [complexity_of_intents=simple, format_constrai...  0.790966\n",
       "10  [complexity_of_intents=simple, format_constrai...  0.790787\n",
       "11  [complexity_of_intents=simple, open_endedness=...  0.789959\n",
       "12  [bertscore_length__min_val=0.0|max_val=0.33, c...  0.788190\n",
       "13  [bertscore__min_val=0.67|max_val=1.0, cosine_s...  0.784764\n",
       "14  [bertscore__min_val=0.33|max_val=0.67, bertsco...  0.782252\n",
       "15  [bertscore__min_val=0.67|max_val=1.0, complexi...  0.782118\n",
       "16  [bertscore__min_val=0.67|max_val=1.0, entity_s...  0.781140\n",
       "17  [complexity_of_intents=simple, entity_sim__min...  0.779665\n",
       "18  [bertscore_length__min_val=0.0|max_val=0.33, l...  0.779619\n",
       "19  [bertscore_length__min_val=0.0|max_val=0.33, e...  0.779271"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_results = sim_df.copy(deep=True)\n",
    "sim_results[\"activated_features\"] = sim_results.apply(\n",
    "    lambda row: [col for col in sim_results.columns if row[col] != 0], axis=1\n",
    ")\n",
    "sim_results[\"pred\"] = model.predict(poly.transform(sim_df))\n",
    "sim_results = sim_results.sort_values(by=\"pred\", ascending=False).reset_index(drop=True)\n",
    "sim_results[[\"activated_features\", \"pred\"]].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['bertscore__min_val=0.67|max_val=1.0', 'complexity_of_intents=simple', 'entity_sim__min_val=0.33|max_val=0.67', 'token_len_difference__min_val=0.67|max_val=1.0', 'type_of_in_context_material=1'], ['bertscore_length__min_val=0.0|max_val=0.33', 'entity_sim__min_val=0.33|max_val=0.67', 'open_endedness=moderate', 'prompt_len__min_val=0.67|max_val=1.0', 'safety_concern=safe'], ['complexity_of_intents=simple', 'safety_concern=safe'], ['bertscore__min_val=0.67|max_val=1.0', 'cosine_sim__min_val=0.67|max_val=1.0', 'entity_sim__min_val=0.0|max_val=0.33', 'token_len_difference__min_val=0.67|max_val=1.0'], ['bertscore__min_val=0.67|max_val=1.0', 'cosine_sim__min_val=0.67|max_val=1.0', 'entity_sim__min_val=0.0|max_val=0.33'], ['bertscore_length__min_val=0.33|max_val=0.67', 'complexity_of_intents=simple', 'format_constraints=1', 'open_endedness=no', 'rouge__min_val=0.0|max_val=0.33', 'safety_concern=safe', 'type_of_in_context_material=1'], ['complexity_of_intents=simple', 'format_constraints=1', 'open_endedness=high', 'safety_concern=safe', 'type_of_in_context_material=1'], ['complexity_of_intents=simple', 'format_constraints=1', 'safety_concern=safe', 'type_of_in_context_material=1'], ['complexity_of_intents=simple', 'format_constraints=1', 'safety_concern=safe'], ['complexity_of_intents=simple', 'format_constraints=1', 'safety_concern=safe']]\n"
     ]
    }
   ],
   "source": [
    "top_combinations = sim_results.activated_features.head(10).to_list()\n",
    "print(top_combinations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: So now you have determined 10 feature combinations that seem to work well. The next step is to train RMs and evaluate them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bertscore::min_val=0.67|max_val=1.0 scalar::feature_name=complexity_of_intents|value=simple entity_sim::min_val=0.33|max_val=0.67 token_len_difference::min_val=0.67|max_val=1.0 open_set::feature_name=type_of_in_context_material|check_for_existence=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:00, 77528.72it/s]\n",
      "45it [00:00, 103251.47it/s]\n",
      "120it [00:00, 75054.65it/s]\n",
      "210it [00:00, 59189.83it/s]\n",
      "252it [00:00, 51212.01it/s]\n",
      "210it [00:00, 41105.28it/s]\n",
      "120it [00:00, 35263.54it/s]\n",
      "45it [00:00, 32699.88it/s]\n",
      "10it [00:00, 22623.00it/s]\n",
      "1it [00:00, 9532.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-02 13:07:07 - INFO - root - Adding meta analyzer features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "10it [00:00, 83886.08it/s]\n",
      "45it [00:00, 113632.56it/s]\n",
      "120it [00:00, 78581.81it/s]\n",
      "210it [00:00, 60123.13it/s]\n",
      "252it [00:00, 49209.21it/s]\n",
      "210it [00:00, 39666.91it/s]\n",
      "120it [00:00, 35713.93it/s]\n",
      "45it [00:00, 29602.21it/s]\n",
      "10it [00:00, 22121.86it/s]\n",
      "1it [00:00, 11881.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-02 13:07:08 - INFO - root - Found 10160 prompts with cols: ['prompt_hash', 'text', 'response_a', 'response_b', 'pref_human', 'pref_gpt4', 'rating_human', 'rating_gpt4', 'completions', 'subject_of_expertise', 'expertise_level', 'languages', 'open_endedness', 'safety_concern', 'complexity_of_intents', 'type_of_in_context_material', 'format_constraints']\n",
      "2024-09-02 13:07:08 - INFO - root - Extracting features\n",
      "2024-09-02 13:07:08 - INFO - root - Extracting 'bertscore' with params: {'min_val': 0.67, 'max_val': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ljvm/Documents/Work/human-pref-datamodel/venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "419e19feb3c4452fa9ac82d237814d56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  86%|########6 | 231M/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-02 13:07:11 - ERROR - root - Error encountered for bertscore ({'min_val': 0.67, 'max_val': 1.0}): Torch not compiled with CUDA enabled\n",
      "2024-09-02 13:07:11 - INFO - root - Skipping this feature because skip_if_error=True\n",
      "2024-09-02 13:07:11 - INFO - root - Extracting 'entity_sim' with params: {'min_val': 0.33, 'max_val': 0.67}\n",
      "2024-09-02 13:07:11 - ERROR - root - Error encountered for entity_sim ({'min_val': 0.33, 'max_val': 0.67}): [E050] Can't find model 'en_core_web_lg'. It doesn't seem to be a Python package or a valid path to a data directory.\n",
      "2024-09-02 13:07:11 - INFO - root - Skipping this feature because skip_if_error=True\n",
      "2024-09-02 13:07:11 - INFO - root - Getting instances. Needs at least 0/0 to swap to human preferences.\n",
      "2024-09-02 13:07:11 - INFO - root - Didn't find any features for this combination. Will return GPT-4 preferences\n",
      "2024-09-02 13:07:11 - INFO - root - Converting annotations into DPO training format\n",
      "2024-09-02 13:07:11 - INFO - root - Number of instances after selection: 8767\n",
      "2024-09-02 13:07:11 - INFO - root - Sampled 7000 instances from the total.\n",
      "2024-09-02 13:07:11 - INFO - root - Tag set 'bertscore__min_val-0.67|max_val-1.0___scalar__feature_name-complexity_of_intents|value-simple___entity_sim__min_val-0.33|max_val-0.67___token_len_difference__min_val-0.67|max_val-1.0___open_set__feature_name-type_of_in_context_material|check_for_existence-1' resulted into 0 swaps! Skipping\n",
      "bertscore_length::min_val=0.0|max_val=0.33 entity_sim::min_val=0.33|max_val=0.67 scalar::feature_name=open_endedness|value=moderate prompt_len::min_val=0.67|max_val=1.0 scalar::feature_name=safety_concern|value=safe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:00, 94466.31it/s]\n",
      "45it [00:00, 113701.01it/s]\n",
      "120it [00:00, 68487.75it/s]\n",
      "210it [00:00, 61405.73it/s]\n",
      "252it [00:00, 48477.94it/s]\n",
      "210it [00:00, 43229.64it/s]\n",
      "120it [00:00, 37676.21it/s]\n",
      "45it [00:00, 28498.22it/s]\n",
      "10it [00:00, 21366.81it/s]\n",
      "1it [00:00, 11214.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-02 13:07:11 - INFO - root - Adding meta analyzer features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "10it [00:00, 92794.34it/s]\n",
      "45it [00:00, 91445.58it/s]\n",
      "120it [00:00, 75054.65it/s]\n",
      "210it [00:00, 60569.65it/s]\n",
      "252it [00:00, 50432.51it/s]\n",
      "210it [00:00, 37926.45it/s]\n",
      "120it [00:00, 34285.86it/s]\n",
      "45it [00:00, 32899.37it/s]\n",
      "10it [00:00, 22982.49it/s]\n",
      "1it [00:00, 10894.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-02 13:07:11 - INFO - root - Found 10160 prompts with cols: ['prompt_hash', 'text', 'response_a', 'response_b', 'pref_human', 'pref_gpt4', 'rating_human', 'rating_gpt4', 'completions', 'subject_of_expertise', 'expertise_level', 'languages', 'open_endedness', 'safety_concern', 'complexity_of_intents', 'type_of_in_context_material', 'format_constraints']\n",
      "2024-09-02 13:07:11 - INFO - root - Extracting features\n",
      "2024-09-02 13:07:11 - INFO - root - Extracting 'bertscore_length' with params: {'min_val': 0.0, 'max_val': 0.33}\n",
      "2024-09-02 13:07:13 - ERROR - root - Error encountered for bertscore_length ({'min_val': 0.0, 'max_val': 0.33}): Torch not compiled with CUDA enabled\n",
      "2024-09-02 13:07:13 - INFO - root - Skipping this feature because skip_if_error=True\n",
      "2024-09-02 13:07:13 - INFO - root - Extracting 'entity_sim' with params: {'min_val': 0.33, 'max_val': 0.67}\n",
      "2024-09-02 13:07:13 - ERROR - root - Error encountered for entity_sim ({'min_val': 0.33, 'max_val': 0.67}): [E050] Can't find model 'en_core_web_lg'. It doesn't seem to be a Python package or a valid path to a data directory.\n",
      "2024-09-02 13:07:13 - INFO - root - Skipping this feature because skip_if_error=True\n",
      "2024-09-02 13:07:13 - INFO - root - Extracting 'prompt_len' with params: {'min_val': 0.67, 'max_val': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ljvm/Documents/Work/human-pref-datamodel/venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-02 13:07:27 - INFO - root - Filtering instances where score falls in [0.67, 1.0]\n",
      "2024-09-02 13:07:29 - INFO - root - Getting instances. Needs at least 1/1 to swap to human preferences.\n",
      "2024-09-02 13:07:29 - INFO - root - Swapping 3353 samples with human preferences.\n",
      "2024-09-02 13:07:29 - INFO - root - Converting annotations into DPO training format\n",
      "2024-09-02 13:07:29 - INFO - root - Number of instances after selection: 9019\n",
      "2024-09-02 13:07:29 - INFO - root - Sampled 7000 instances from the total.\n",
      "2024-09-02 13:07:29 - INFO - root - Saved to human_datamodel_7000_FEATS_765fa07442161345c4795770cfafc16f_SWAPS_2424.jsonl\n",
      "2024-09-02 13:07:29 - INFO - root - top_features_experiments.txt not found. Generating new and appending experiment human_datamodel_7000_FEATS_765fa07442161345c4795770cfafc16f_SWAPS_2424\n",
      "scalar::feature_name=complexity_of_intents|value=simple scalar::feature_name=safety_concern|value=safe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:00, 97997.76it/s]\n",
      "45it [00:00, 110635.22it/s]\n",
      "120it [00:00, 65922.26it/s]\n",
      "210it [00:00, 56407.55it/s]\n",
      "252it [00:00, 50952.79it/s]\n",
      "210it [00:00, 12816.54it/s]\n",
      "120it [00:00, 8920.57it/s]\n",
      "45it [00:00, 28373.97it/s]\n",
      "10it [00:00, 24556.81it/s]\n",
      "1it [00:00, 8943.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-02 13:07:29 - INFO - root - Adding meta analyzer features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "10it [00:00, 86125.34it/s]\n",
      "45it [00:00, 111947.62it/s]\n",
      "120it [00:00, 64445.13it/s]\n",
      "210it [00:00, 62499.39it/s]\n",
      "252it [00:00, 50408.46it/s]\n",
      "210it [00:00, 41781.88it/s]\n",
      "120it [00:00, 32127.95it/s]\n",
      "45it [00:00, 29408.49it/s]\n",
      "10it [00:00, 22721.04it/s]\n",
      "1it [00:00, 10754.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-02 13:07:30 - INFO - root - Found 10160 prompts with cols: ['prompt_hash', 'text', 'response_a', 'response_b', 'pref_human', 'pref_gpt4', 'rating_human', 'rating_gpt4', 'completions', 'subject_of_expertise', 'expertise_level', 'languages', 'open_endedness', 'safety_concern', 'complexity_of_intents', 'type_of_in_context_material', 'format_constraints']\n",
      "2024-09-02 13:07:30 - INFO - root - Extracting features\n",
      "2024-09-02 13:07:30 - INFO - root - Getting instances. Needs at least 0/0 to swap to human preferences.\n",
      "2024-09-02 13:07:30 - INFO - root - Didn't find any features for this combination. Will return GPT-4 preferences\n",
      "2024-09-02 13:07:30 - INFO - root - Converting annotations into DPO training format\n",
      "2024-09-02 13:07:30 - INFO - root - Number of instances after selection: 8767\n",
      "2024-09-02 13:07:30 - INFO - root - Sampled 7000 instances from the total.\n",
      "2024-09-02 13:07:30 - INFO - root - Tag set 'scalar__feature_name-complexity_of_intents|value-simple___scalar__feature_name-safety_concern|value-safe' resulted into 0 swaps! Skipping\n",
      "bertscore::min_val=0.67|max_val=1.0 cosine_sim::min_val=0.67|max_val=1.0 entity_sim::min_val=0.0|max_val=0.33 token_len_difference::min_val=0.67|max_val=1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:00, 106454.42it/s]\n",
      "45it [00:00, 104857.60it/s]\n",
      "120it [00:00, 80595.11it/s]\n",
      "210it [00:00, 58073.70it/s]\n",
      "252it [00:00, 49268.85it/s]\n",
      "210it [00:00, 44503.02it/s]\n",
      "120it [00:00, 36261.99it/s]\n",
      "45it [00:00, 28881.97it/s]\n",
      "10it [00:00, 24385.49it/s]\n",
      "1it [00:00, 4476.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-02 13:07:30 - INFO - root - Adding meta analyzer features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "10it [00:00, 108942.96it/s]\n",
      "45it [00:00, 104857.60it/s]\n",
      "120it [00:00, 81088.53it/s]\n",
      "210it [00:00, 60866.83it/s]\n",
      "252it [00:00, 45843.36it/s]\n",
      "210it [00:00, 43462.15it/s]\n",
      "120it [00:00, 34891.96it/s]\n",
      "45it [00:00, 30700.01it/s]\n",
      "10it [00:00, 21151.31it/s]\n",
      "1it [00:00, 15887.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-02 13:07:30 - INFO - root - Found 10160 prompts with cols: ['prompt_hash', 'text', 'response_a', 'response_b', 'pref_human', 'pref_gpt4', 'rating_human', 'rating_gpt4', 'completions', 'subject_of_expertise', 'expertise_level', 'languages', 'open_endedness', 'safety_concern', 'complexity_of_intents', 'type_of_in_context_material', 'format_constraints']\n",
      "2024-09-02 13:07:30 - INFO - root - Extracting features\n",
      "2024-09-02 13:07:30 - INFO - root - Extracting 'bertscore' with params: {'min_val': 0.67, 'max_val': 1.0}\n",
      "2024-09-02 13:07:31 - ERROR - root - Error encountered for bertscore ({'min_val': 0.67, 'max_val': 1.0}): Torch not compiled with CUDA enabled\n",
      "2024-09-02 13:07:31 - INFO - root - Skipping this feature because skip_if_error=True\n",
      "2024-09-02 13:07:31 - INFO - root - Extracting 'cosine_sim' with params: {'min_val': 0.67, 'max_val': 1.0}\n",
      "2024-09-02 13:07:31 - INFO - sentence_transformers.SentenceTransformer - Load pretrained SentenceTransformer: all-distilroberta-v1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ljvm/Documents/Work/human-pref-datamodel/venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-02 13:07:32 - ERROR - root - Error encountered for cosine_sim ({'min_val': 0.67, 'max_val': 1.0}): Torch not compiled with CUDA enabled\n",
      "2024-09-02 13:07:32 - INFO - root - Skipping this feature because skip_if_error=True\n",
      "2024-09-02 13:07:32 - INFO - root - Extracting 'entity_sim' with params: {'min_val': 0.0, 'max_val': 0.33}\n",
      "2024-09-02 13:07:32 - ERROR - root - Error encountered for entity_sim ({'min_val': 0.0, 'max_val': 0.33}): [E050] Can't find model 'en_core_web_lg'. It doesn't seem to be a Python package or a valid path to a data directory.\n",
      "2024-09-02 13:07:32 - INFO - root - Skipping this feature because skip_if_error=True\n",
      "2024-09-02 13:07:32 - INFO - root - Getting instances. Needs at least 0/0 to swap to human preferences.\n",
      "2024-09-02 13:07:32 - INFO - root - Didn't find any features for this combination. Will return GPT-4 preferences\n",
      "2024-09-02 13:07:32 - INFO - root - Converting annotations into DPO training format\n",
      "2024-09-02 13:07:32 - INFO - root - Number of instances after selection: 8767\n",
      "2024-09-02 13:07:32 - INFO - root - Sampled 7000 instances from the total.\n",
      "2024-09-02 13:07:32 - INFO - root - Tag set 'bertscore__min_val-0.67|max_val-1.0___cosine_sim__min_val-0.67|max_val-1.0___entity_sim__min_val-0.0|max_val-0.33___token_len_difference__min_val-0.67|max_val-1.0' resulted into 0 swaps! Skipping\n",
      "bertscore::min_val=0.67|max_val=1.0 cosine_sim::min_val=0.67|max_val=1.0 entity_sim::min_val=0.0|max_val=0.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:00, 98922.26it/s]\n",
      "45it [00:00, 94893.76it/s]\n",
      "120it [00:00, 68264.81it/s]\n",
      "210it [00:00, 56414.77it/s]\n",
      "252it [00:00, 46179.86it/s]\n",
      "210it [00:00, 42311.76it/s]\n",
      "120it [00:00, 35160.08it/s]\n",
      "45it [00:00, 27354.16it/s]\n",
      "10it [00:00, 16027.15it/s]\n",
      "1it [00:00, 13888.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-02 13:07:32 - INFO - root - Adding meta analyzer features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "10it [00:00, 129854.61it/s]\n",
      "45it [00:00, 98509.23it/s]\n",
      "120it [00:00, 78130.47it/s]\n",
      "210it [00:00, 53645.40it/s]\n",
      "252it [00:00, 46934.49it/s]\n",
      "210it [00:00, 43586.89it/s]\n",
      "120it [00:00, 36464.28it/s]\n",
      "45it [00:00, 26959.53it/s]\n",
      "10it [00:00, 26749.39it/s]\n",
      "1it [00:00, 16710.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-02 13:07:32 - INFO - root - Found 10160 prompts with cols: ['prompt_hash', 'text', 'response_a', 'response_b', 'pref_human', 'pref_gpt4', 'rating_human', 'rating_gpt4', 'completions', 'subject_of_expertise', 'expertise_level', 'languages', 'open_endedness', 'safety_concern', 'complexity_of_intents', 'type_of_in_context_material', 'format_constraints']\n",
      "2024-09-02 13:07:32 - INFO - root - Extracting features\n",
      "2024-09-02 13:07:32 - INFO - root - Extracting 'bertscore' with params: {'min_val': 0.67, 'max_val': 1.0}\n",
      "2024-09-02 13:07:33 - ERROR - root - Error encountered for bertscore ({'min_val': 0.67, 'max_val': 1.0}): Torch not compiled with CUDA enabled\n",
      "2024-09-02 13:07:33 - INFO - root - Skipping this feature because skip_if_error=True\n",
      "2024-09-02 13:07:33 - INFO - root - Extracting 'cosine_sim' with params: {'min_val': 0.67, 'max_val': 1.0}\n",
      "2024-09-02 13:07:33 - INFO - sentence_transformers.SentenceTransformer - Load pretrained SentenceTransformer: all-distilroberta-v1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ljvm/Documents/Work/human-pref-datamodel/venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-02 13:07:34 - ERROR - root - Error encountered for cosine_sim ({'min_val': 0.67, 'max_val': 1.0}): Torch not compiled with CUDA enabled\n",
      "2024-09-02 13:07:34 - INFO - root - Skipping this feature because skip_if_error=True\n",
      "2024-09-02 13:07:34 - INFO - root - Extracting 'entity_sim' with params: {'min_val': 0.0, 'max_val': 0.33}\n",
      "2024-09-02 13:07:34 - ERROR - root - Error encountered for entity_sim ({'min_val': 0.0, 'max_val': 0.33}): [E050] Can't find model 'en_core_web_lg'. It doesn't seem to be a Python package or a valid path to a data directory.\n",
      "2024-09-02 13:07:34 - INFO - root - Skipping this feature because skip_if_error=True\n",
      "2024-09-02 13:07:34 - INFO - root - Getting instances. Needs at least 0/0 to swap to human preferences.\n",
      "2024-09-02 13:07:34 - INFO - root - Didn't find any features for this combination. Will return GPT-4 preferences\n",
      "2024-09-02 13:07:34 - INFO - root - Converting annotations into DPO training format\n",
      "2024-09-02 13:07:34 - INFO - root - Number of instances after selection: 8767\n",
      "2024-09-02 13:07:34 - INFO - root - Sampled 7000 instances from the total.\n",
      "2024-09-02 13:07:34 - INFO - root - Tag set 'bertscore__min_val-0.67|max_val-1.0___cosine_sim__min_val-0.67|max_val-1.0___entity_sim__min_val-0.0|max_val-0.33' resulted into 0 swaps! Skipping\n",
      "bertscore_length::min_val=0.33|max_val=0.67 scalar::feature_name=complexity_of_intents|value=simple open_set::feature_name=format_constraints|check_for_existence=1 scalar::feature_name=open_endedness|value=no rouge::min_val=0.0|max_val=0.33 scalar::feature_name=safety_concern|value=safe open_set::feature_name=type_of_in_context_material|check_for_existence=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:00, 85423.71it/s]\n",
      "45it [00:00, 108162.57it/s]\n",
      "120it [00:00, 73167.10it/s]\n",
      "210it [00:00, 61819.47it/s]\n",
      "252it [00:00, 16802.29it/s]\n",
      "210it [00:00, 11809.71it/s]\n",
      "120it [00:00, 32146.42it/s]\n",
      "45it [00:00, 22900.23it/s]\n",
      "10it [00:00, 25010.76it/s]\n",
      "1it [00:00, 11915.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-02 13:07:34 - INFO - root - Adding meta analyzer features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "10it [00:00, 129453.83it/s]\n",
      "45it [00:00, 91667.64it/s]\n",
      "120it [00:00, 74587.50it/s]\n",
      "210it [00:00, 61478.60it/s]\n",
      "252it [00:00, 48611.72it/s]\n",
      "210it [00:00, 40781.73it/s]\n",
      "120it [00:00, 33689.19it/s]\n",
      "45it [00:00, 30570.73it/s]\n",
      "10it [00:00, 23590.01it/s]\n",
      "1it [00:00, 12336.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-02 13:07:34 - INFO - root - Found 10160 prompts with cols: ['prompt_hash', 'text', 'response_a', 'response_b', 'pref_human', 'pref_gpt4', 'rating_human', 'rating_gpt4', 'completions', 'subject_of_expertise', 'expertise_level', 'languages', 'open_endedness', 'safety_concern', 'complexity_of_intents', 'type_of_in_context_material', 'format_constraints']\n",
      "2024-09-02 13:07:34 - INFO - root - Extracting features\n",
      "2024-09-02 13:07:34 - INFO - root - Extracting 'bertscore_length' with params: {'min_val': 0.33, 'max_val': 0.67}\n",
      "2024-09-02 13:07:36 - ERROR - root - Error encountered for bertscore_length ({'min_val': 0.33, 'max_val': 0.67}): Torch not compiled with CUDA enabled\n",
      "2024-09-02 13:07:36 - INFO - root - Skipping this feature because skip_if_error=True\n",
      "2024-09-02 13:07:36 - INFO - root - Extracting 'rouge' with params: {'min_val': 0.0, 'max_val': 0.33}\n",
      "2024-09-02 13:07:36 - INFO - absl - Using default tokenizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ljvm/Documents/Work/human-pref-datamodel/venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10160 [00:00<?, ?it/s]\n",
      "  0%|          | 41/10160 [00:00<00:25, 398.48it/s]\n",
      "  1%|          | 90/10160 [00:00<00:22, 445.44it/s]\n",
      "  1%|▏         | 135/10160 [00:00<00:22, 443.72it/s]\n",
      "  2%|▏         | 180/10160 [00:00<00:22, 438.86it/s]\n",
      "  2%|▏         | 224/10160 [00:00<00:24, 413.20it/s]\n",
      "  3%|▎         | 270/10160 [00:00<00:23, 427.65it/s]\n",
      "  3%|▎         | 313/10160 [00:00<00:24, 409.68it/s]\n",
      "  4%|▎         | 361/10160 [00:00<00:22, 429.24it/s]\n",
      "  4%|▍         | 406/10160 [00:00<00:22, 433.96it/s]\n",
      "  4%|▍         | 450/10160 [00:01<00:22, 426.88it/s]\n",
      "  5%|▍         | 493/10160 [00:01<00:23, 418.89it/s]\n",
      "  5%|▌         | 540/10160 [00:01<00:22, 432.09it/s]\n",
      "  6%|▌         | 584/10160 [00:01<00:23, 410.74it/s]\n",
      "  6%|▋         | 635/10160 [00:01<00:21, 437.04it/s]\n",
      "  7%|▋         | 683/10160 [00:01<00:21, 447.86it/s]\n",
      "  7%|▋         | 729/10160 [00:01<00:21, 438.86it/s]\n",
      "  8%|▊         | 775/10160 [00:01<00:21, 434.73it/s]\n",
      "  8%|▊         | 819/10160 [00:01<00:21, 435.35it/s]\n",
      "  9%|▊         | 867/10160 [00:02<00:20, 444.30it/s]\n",
      "  9%|▉         | 912/10160 [00:02<00:21, 425.51it/s]\n",
      "  9%|▉         | 955/10160 [00:02<00:21, 421.86it/s]\n",
      " 10%|▉         | 998/10160 [00:02<00:22, 409.19it/s]\n",
      " 10%|█         | 1040/10160 [00:02<00:22, 409.43it/s]\n",
      " 11%|█         | 1093/10160 [00:02<00:20, 440.88it/s]\n",
      " 11%|█         | 1138/10160 [00:02<00:21, 426.19it/s]\n",
      " 12%|█▏        | 1185/10160 [00:02<00:20, 430.41it/s]\n",
      " 12%|█▏        | 1229/10160 [00:02<00:20, 427.17it/s]\n",
      " 13%|█▎        | 1273/10160 [00:02<00:20, 430.16it/s]\n",
      " 13%|█▎        | 1317/10160 [00:03<00:20, 429.34it/s]\n",
      " 13%|█▎        | 1360/10160 [00:03<00:21, 401.85it/s]\n",
      " 14%|█▍        | 1403/10160 [00:03<00:21, 408.40it/s]\n",
      " 14%|█▍        | 1445/10160 [00:03<00:21, 409.62it/s]\n",
      " 15%|█▍        | 1489/10160 [00:03<00:20, 418.22it/s]\n",
      " 15%|█▌        | 1531/10160 [00:03<00:20, 413.89it/s]\n",
      " 16%|█▌        | 1576/10160 [00:03<00:20, 422.30it/s]\n",
      " 16%|█▌        | 1619/10160 [00:03<00:20, 419.98it/s]\n",
      " 16%|█▋        | 1668/10160 [00:03<00:19, 439.93it/s]\n",
      " 17%|█▋        | 1713/10160 [00:04<00:19, 426.01it/s]\n",
      " 17%|█▋        | 1757/10160 [00:04<00:19, 428.57it/s]\n",
      " 18%|█▊        | 1800/10160 [00:04<00:19, 423.17it/s]\n",
      " 18%|█▊        | 1843/10160 [00:04<00:20, 404.53it/s]\n",
      " 19%|█▊        | 1897/10160 [00:04<00:18, 442.59it/s]\n",
      " 19%|█▉        | 1945/10160 [00:04<00:18, 450.48it/s]\n",
      " 20%|█▉        | 1991/10160 [00:04<00:18, 438.39it/s]\n",
      " 20%|██        | 2036/10160 [00:04<00:19, 422.01it/s]\n",
      " 21%|██        | 2083/10160 [00:04<00:18, 434.51it/s]\n",
      " 21%|██        | 2127/10160 [00:04<00:19, 414.98it/s]\n",
      " 21%|██▏       | 2179/10160 [00:05<00:18, 441.75it/s]\n",
      " 22%|██▏       | 2230/10160 [00:05<00:17, 459.59it/s]\n",
      " 22%|██▏       | 2278/10160 [00:05<00:17, 462.92it/s]\n",
      " 23%|██▎       | 2325/10160 [00:05<00:17, 437.03it/s]\n",
      " 23%|██▎       | 2370/10160 [00:05<00:18, 421.75it/s]\n",
      " 24%|██▍       | 2417/10160 [00:05<00:17, 431.60it/s]\n",
      " 24%|██▍       | 2461/10160 [00:05<00:18, 425.19it/s]\n",
      " 25%|██▍       | 2507/10160 [00:05<00:17, 428.71it/s]\n",
      " 25%|██▌       | 2558/10160 [00:05<00:16, 449.67it/s]\n",
      " 26%|██▌       | 2604/10160 [00:06<00:17, 434.57it/s]\n",
      " 26%|██▌       | 2655/10160 [00:06<00:16, 454.11it/s]\n",
      " 27%|██▋       | 2701/10160 [00:06<00:16, 442.55it/s]\n",
      " 27%|██▋       | 2746/10160 [00:06<00:17, 426.24it/s]\n",
      " 28%|██▊       | 2794/10160 [00:06<00:16, 439.78it/s]\n",
      " 28%|██▊       | 2839/10160 [00:06<00:17, 428.43it/s]\n",
      " 28%|██▊       | 2883/10160 [00:06<00:17, 426.20it/s]\n",
      " 29%|██▉       | 2928/10160 [00:06<00:16, 432.31it/s]\n",
      " 29%|██▉       | 2972/10160 [00:06<00:17, 416.29it/s]\n",
      " 30%|██▉       | 3014/10160 [00:07<00:17, 410.55it/s]\n",
      " 30%|███       | 3056/10160 [00:07<00:17, 408.77it/s]\n",
      " 31%|███       | 3099/10160 [00:07<00:17, 414.00it/s]\n",
      " 31%|███       | 3148/10160 [00:07<00:16, 434.35it/s]\n",
      " 31%|███▏      | 3192/10160 [00:07<00:16, 419.65it/s]\n",
      " 32%|███▏      | 3235/10160 [00:07<00:16, 417.77it/s]\n",
      " 32%|███▏      | 3277/10160 [00:07<00:16, 414.37it/s]\n",
      " 33%|███▎      | 3319/10160 [00:07<00:16, 414.45it/s]\n",
      " 33%|███▎      | 3362/10160 [00:07<00:16, 417.87it/s]\n",
      " 34%|███▎      | 3408/10160 [00:07<00:15, 428.07it/s]\n",
      " 34%|███▍      | 3451/10160 [00:08<00:16, 417.50it/s]\n",
      " 34%|███▍      | 3497/10160 [00:08<00:15, 428.92it/s]\n",
      " 35%|███▍      | 3545/10160 [00:08<00:14, 442.10it/s]\n",
      " 35%|███▌      | 3590/10160 [00:08<00:15, 437.37it/s]\n",
      " 36%|███▌      | 3634/10160 [00:08<00:14, 435.20it/s]\n",
      " 36%|███▌      | 3678/10160 [00:08<00:15, 431.15it/s]\n",
      " 37%|███▋      | 3722/10160 [00:08<00:15, 427.71it/s]\n",
      " 37%|███▋      | 3765/10160 [00:08<00:15, 413.12it/s]\n",
      " 38%|███▊      | 3815/10160 [00:08<00:14, 434.88it/s]\n",
      " 38%|███▊      | 3865/10160 [00:09<00:13, 451.34it/s]\n",
      " 38%|███▊      | 3911/10160 [00:09<00:14, 418.45it/s]\n",
      " 39%|███▉      | 3954/10160 [00:09<00:14, 414.41it/s]\n",
      " 39%|███▉      | 3996/10160 [00:09<00:15, 407.85it/s]\n",
      " 40%|███▉      | 4046/10160 [00:09<00:14, 432.97it/s]\n",
      " 40%|████      | 4090/10160 [00:09<00:14, 428.29it/s]\n",
      " 41%|████      | 4134/10160 [00:09<00:14, 424.83it/s]\n",
      " 41%|████      | 4182/10160 [00:09<00:13, 440.53it/s]\n",
      " 42%|████▏     | 4227/10160 [00:09<00:13, 428.54it/s]\n",
      " 42%|████▏     | 4276/10160 [00:09<00:13, 445.61it/s]\n",
      " 43%|████▎     | 4321/10160 [00:10<00:13, 427.92it/s]\n",
      " 43%|████▎     | 4365/10160 [00:10<00:13, 418.71it/s]\n",
      " 43%|████▎     | 4408/10160 [00:10<00:13, 415.43it/s]\n",
      " 44%|████▍     | 4450/10160 [00:10<00:14, 404.49it/s]\n",
      " 44%|████▍     | 4491/10160 [00:10<00:14, 401.42it/s]\n",
      " 45%|████▍     | 4534/10160 [00:10<00:13, 407.84it/s]\n",
      " 45%|████▌     | 4582/10160 [00:10<00:13, 428.11it/s]\n",
      " 46%|████▌     | 4627/10160 [00:10<00:12, 431.52it/s]\n",
      " 46%|████▌     | 4673/10160 [00:10<00:12, 439.19it/s]\n",
      " 46%|████▋     | 4717/10160 [00:11<00:12, 428.55it/s]\n",
      " 47%|████▋     | 4763/10160 [00:11<00:12, 435.80it/s]\n",
      " 47%|████▋     | 4807/10160 [00:11<00:12, 414.10it/s]\n",
      " 48%|████▊     | 4849/10160 [00:11<00:13, 406.27it/s]\n",
      " 48%|████▊     | 4898/10160 [00:11<00:12, 428.47it/s]\n",
      " 49%|████▊     | 4942/10160 [00:11<00:12, 428.58it/s]\n",
      " 49%|████▉     | 4987/10160 [00:11<00:11, 432.68it/s]\n",
      " 50%|████▉     | 5031/10160 [00:11<00:12, 397.90it/s]\n",
      " 50%|████▉     | 5079/10160 [00:11<00:12, 418.16it/s]\n",
      " 50%|█████     | 5122/10160 [00:12<00:12, 419.67it/s]\n",
      " 51%|█████     | 5165/10160 [00:12<00:12, 411.69it/s]\n",
      " 51%|█████▏    | 5209/10160 [00:12<00:11, 419.18it/s]\n",
      " 52%|█████▏    | 5254/10160 [00:12<00:11, 423.81it/s]\n",
      " 52%|█████▏    | 5297/10160 [00:12<00:11, 418.30it/s]\n",
      " 53%|█████▎    | 5345/10160 [00:12<00:11, 431.76it/s]\n",
      " 53%|█████▎    | 5393/10160 [00:12<00:10, 441.87it/s]\n",
      " 54%|█████▎    | 5438/10160 [00:12<00:11, 428.77it/s]\n",
      " 54%|█████▍    | 5483/10160 [00:12<00:10, 432.73it/s]\n",
      " 54%|█████▍    | 5536/10160 [00:12<00:10, 458.02it/s]\n",
      " 55%|█████▍    | 5582/10160 [00:13<00:10, 448.32it/s]\n",
      " 55%|█████▌    | 5627/10160 [00:13<00:10, 441.77it/s]\n",
      " 56%|█████▌    | 5681/10160 [00:13<00:09, 468.34it/s]\n",
      " 56%|█████▋    | 5728/10160 [00:13<00:09, 461.51it/s]\n",
      " 57%|█████▋    | 5776/10160 [00:13<00:09, 464.79it/s]\n",
      " 57%|█████▋    | 5823/10160 [00:13<00:09, 436.00it/s]\n",
      " 58%|█████▊    | 5867/10160 [00:13<00:10, 422.43it/s]\n",
      " 58%|█████▊    | 5910/10160 [00:13<00:10, 407.84it/s]\n",
      " 59%|█████▊    | 5954/10160 [00:13<00:10, 413.75it/s]\n",
      " 59%|█████▉    | 5997/10160 [00:14<00:09, 416.40it/s]\n",
      " 59%|█████▉    | 6039/10160 [00:14<00:09, 415.30it/s]\n",
      " 60%|█████▉    | 6081/10160 [00:14<00:09, 415.99it/s]\n",
      " 60%|██████    | 6131/10160 [00:14<00:09, 438.81it/s]\n",
      " 61%|██████    | 6175/10160 [00:14<00:09, 404.77it/s]\n",
      " 61%|██████    | 6218/10160 [00:14<00:09, 408.18it/s]\n",
      " 62%|██████▏   | 6264/10160 [00:14<00:09, 420.31it/s]\n",
      " 62%|██████▏   | 6307/10160 [00:14<00:09, 409.90it/s]\n",
      " 63%|██████▎   | 6353/10160 [00:14<00:09, 419.10it/s]\n",
      " 63%|██████▎   | 6396/10160 [00:14<00:09, 409.44it/s]\n",
      " 63%|██████▎   | 6440/10160 [00:15<00:08, 417.56it/s]\n",
      " 64%|██████▍   | 6482/10160 [00:15<00:08, 410.32it/s]\n",
      " 64%|██████▍   | 6524/10160 [00:15<00:08, 406.46it/s]\n",
      " 65%|██████▍   | 6565/10160 [00:15<00:09, 398.76it/s]\n",
      " 65%|██████▌   | 6612/10160 [00:15<00:08, 416.62it/s]\n",
      " 65%|██████▌   | 6654/10160 [00:15<00:08, 415.11it/s]\n",
      " 66%|██████▌   | 6696/10160 [00:15<00:08, 414.15it/s]\n",
      " 66%|██████▋   | 6738/10160 [00:15<00:08, 411.87it/s]\n",
      " 67%|██████▋   | 6783/10160 [00:15<00:08, 419.85it/s]\n",
      " 67%|██████▋   | 6826/10160 [00:16<00:07, 420.56it/s]\n",
      " 68%|██████▊   | 6869/10160 [00:16<00:08, 406.27it/s]\n",
      " 68%|██████▊   | 6910/10160 [00:16<00:08, 404.27it/s]\n",
      " 68%|██████▊   | 6955/10160 [00:16<00:07, 416.65it/s]\n",
      " 69%|██████▉   | 6997/10160 [00:16<00:07, 413.81it/s]\n",
      " 69%|██████▉   | 7039/10160 [00:16<00:07, 396.49it/s]\n",
      " 70%|██████▉   | 7079/10160 [00:16<00:07, 389.16it/s]\n",
      " 70%|███████   | 7121/10160 [00:16<00:07, 395.08it/s]\n",
      " 71%|███████   | 7163/10160 [00:16<00:07, 401.59it/s]\n",
      " 71%|███████   | 7209/10160 [00:16<00:07, 413.49it/s]\n",
      " 71%|███████▏  | 7251/10160 [00:17<00:07, 414.38it/s]\n",
      " 72%|███████▏  | 7293/10160 [00:17<00:06, 411.42it/s]\n",
      " 72%|███████▏  | 7335/10160 [00:17<00:06, 410.73it/s]\n",
      " 73%|███████▎  | 7377/10160 [00:17<00:06, 403.39it/s]\n",
      " 73%|███████▎  | 7421/10160 [00:17<00:06, 412.89it/s]\n",
      " 73%|███████▎  | 7463/10160 [00:17<00:06, 405.30it/s]\n",
      " 74%|███████▍  | 7512/10160 [00:17<00:06, 429.32it/s]\n",
      " 74%|███████▍  | 7556/10160 [00:17<00:06, 431.60it/s]\n",
      " 75%|███████▍  | 7601/10160 [00:17<00:05, 436.67it/s]\n",
      " 75%|███████▌  | 7648/10160 [00:18<00:05, 444.56it/s]\n",
      " 76%|███████▌  | 7693/10160 [00:18<00:05, 430.87it/s]\n",
      " 76%|███████▌  | 7740/10160 [00:18<00:05, 435.61it/s]\n",
      " 77%|███████▋  | 7788/10160 [00:18<00:05, 443.54it/s]\n",
      " 77%|███████▋  | 7835/10160 [00:18<00:05, 447.34it/s]\n",
      " 78%|███████▊  | 7881/10160 [00:18<00:05, 447.33it/s]\n",
      " 78%|███████▊  | 7926/10160 [00:18<00:05, 436.24it/s]\n",
      " 78%|███████▊  | 7970/10160 [00:18<00:05, 430.73it/s]\n",
      " 79%|███████▉  | 8016/10160 [00:18<00:04, 436.72it/s]\n",
      " 79%|███████▉  | 8060/10160 [00:18<00:04, 424.56it/s]\n",
      " 80%|███████▉  | 8103/10160 [00:19<00:04, 414.49it/s]\n",
      " 80%|████████  | 8145/10160 [00:19<00:04, 413.87it/s]\n",
      " 81%|████████  | 8189/10160 [00:19<00:04, 421.14it/s]\n",
      " 81%|████████  | 8234/10160 [00:19<00:04, 426.04it/s]\n",
      " 81%|████████▏ | 8277/10160 [00:19<00:04, 415.72it/s]\n",
      " 82%|████████▏ | 8319/10160 [00:19<00:04, 402.91it/s]\n",
      " 82%|████████▏ | 8364/10160 [00:19<00:04, 412.22it/s]\n",
      " 83%|████████▎ | 8409/10160 [00:19<00:04, 422.52it/s]\n",
      " 83%|████████▎ | 8452/10160 [00:19<00:04, 404.85it/s]\n",
      " 84%|████████▎ | 8497/10160 [00:20<00:04, 413.16it/s]\n",
      " 84%|████████▍ | 8542/10160 [00:20<00:03, 419.27it/s]\n",
      " 84%|████████▍ | 8585/10160 [00:20<00:03, 413.99it/s]\n",
      " 85%|████████▍ | 8627/10160 [00:20<00:03, 396.06it/s]\n",
      " 85%|████████▌ | 8667/10160 [00:20<00:03, 396.35it/s]\n",
      " 86%|████████▌ | 8711/10160 [00:20<00:03, 406.68it/s]\n",
      " 86%|████████▌ | 8752/10160 [00:20<00:03, 398.49it/s]\n",
      " 87%|████████▋ | 8792/10160 [00:20<00:03, 387.23it/s]\n",
      " 87%|████████▋ | 8834/10160 [00:20<00:03, 392.85it/s]\n",
      " 87%|████████▋ | 8882/10160 [00:20<00:03, 417.44it/s]\n",
      " 88%|████████▊ | 8924/10160 [00:21<00:03, 405.69it/s]\n",
      " 88%|████████▊ | 8967/10160 [00:21<00:02, 408.83it/s]\n",
      " 89%|████████▊ | 9009/10160 [00:21<00:02, 410.20it/s]\n",
      " 89%|████████▉ | 9051/10160 [00:21<00:02, 412.23it/s]\n",
      " 90%|████████▉ | 9099/10160 [00:21<00:02, 430.04it/s]\n",
      " 90%|█████████ | 9144/10160 [00:21<00:02, 434.30it/s]\n",
      " 90%|█████████ | 9188/10160 [00:21<00:02, 429.91it/s]\n",
      " 91%|█████████ | 9235/10160 [00:21<00:02, 440.18it/s]\n",
      " 91%|█████████▏| 9280/10160 [00:21<00:02, 437.54it/s]\n",
      " 92%|█████████▏| 9324/10160 [00:22<00:02, 407.83it/s]\n",
      " 92%|█████████▏| 9366/10160 [00:22<00:02, 394.61it/s]\n",
      " 93%|█████████▎| 9406/10160 [00:22<00:01, 389.90it/s]\n",
      " 93%|█████████▎| 9453/10160 [00:22<00:01, 409.28it/s]\n",
      " 94%|█████████▎| 9500/10160 [00:22<00:01, 422.26it/s]\n",
      " 94%|█████████▍| 9543/10160 [00:22<00:01, 418.80it/s]\n",
      " 94%|█████████▍| 9591/10160 [00:22<00:01, 433.34it/s]\n",
      " 95%|█████████▍| 9635/10160 [00:22<00:01, 387.17it/s]\n",
      " 95%|█████████▌| 9675/10160 [00:22<00:01, 388.02it/s]\n",
      " 96%|█████████▌| 9717/10160 [00:23<00:01, 392.51it/s]\n",
      " 96%|█████████▌| 9760/10160 [00:23<00:00, 401.96it/s]\n",
      " 97%|█████████▋| 9806/10160 [00:23<00:00, 417.89it/s]\n",
      " 97%|█████████▋| 9849/10160 [00:23<00:00, 412.42it/s]\n",
      " 97%|█████████▋| 9891/10160 [00:23<00:00, 404.04it/s]\n",
      " 98%|█████████▊| 9932/10160 [00:23<00:00, 404.80it/s]\n",
      " 98%|█████████▊| 9973/10160 [00:23<00:00, 396.52it/s]\n",
      " 99%|█████████▊| 10013/10160 [00:23<00:00, 391.64it/s]\n",
      " 99%|█████████▉| 10059/10160 [00:23<00:00, 409.65it/s]\n",
      " 99%|█████████▉| 10101/10160 [00:23<00:00, 409.05it/s]\n",
      "100%|█████████▉| 10143/10160 [00:24<00:00, 411.60it/s]\n",
      "100%|██████████| 10160/10160 [00:24<00:00, 421.73it/s]\n",
      "\n",
      "2024-09-02 13:08:00 - INFO - root - Filtering instances where score falls in [0.0, 0.33]\n",
      "2024-09-02 13:08:00 - INFO - root - Getting instances. Needs at least 1/1 to swap to human preferences.\n",
      "2024-09-02 13:08:00 - INFO - root - Swapping 2518 samples with human preferences.\n",
      "2024-09-02 13:08:00 - INFO - root - Converting annotations into DPO training format\n",
      "2024-09-02 13:08:00 - INFO - root - Number of instances after selection: 8758\n",
      "2024-09-02 13:08:00 - INFO - root - Sampled 7000 instances from the total.\n",
      "2024-09-02 13:08:01 - INFO - root - Saved to human_datamodel_7000_FEATS_db38031c79d6d7cadb7f76d451ccb02a_SWAPS_1960.jsonl\n",
      "2024-09-02 13:08:01 - INFO - root - Appending human_datamodel_7000_FEATS_db38031c79d6d7cadb7f76d451ccb02a_SWAPS_1960 to top_features_experiments.txt\n",
      "scalar::feature_name=complexity_of_intents|value=simple open_set::feature_name=format_constraints|check_for_existence=1 scalar::feature_name=open_endedness|value=high scalar::feature_name=safety_concern|value=safe open_set::feature_name=type_of_in_context_material|check_for_existence=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:00, 100102.72it/s]\n",
      "45it [00:00, 89114.11it/s]\n",
      "120it [00:00, 27835.22it/s]\n",
      "210it [00:00, 52174.14it/s]\n",
      "252it [00:00, 47049.39it/s]\n",
      "210it [00:00, 42596.18it/s]\n",
      "120it [00:00, 35787.58it/s]\n",
      "45it [00:00, 31963.37it/s]\n",
      "10it [00:00, 24571.20it/s]\n",
      "1it [00:00, 11096.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-02 13:08:01 - INFO - root - Adding meta analyzer features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "10it [00:00, 121927.44it/s]\n",
      "45it [00:00, 102689.71it/s]\n",
      "120it [00:00, 75800.67it/s]\n",
      "210it [00:00, 59453.52it/s]\n",
      "252it [00:00, 51169.86it/s]\n",
      "210it [00:00, 43335.98it/s]\n",
      "120it [00:00, 34464.29it/s]\n",
      "45it [00:00, 31212.78it/s]\n",
      "10it [00:00, 22453.45it/s]\n",
      "1it [00:00, 10106.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-02 13:08:01 - INFO - root - Found 10160 prompts with cols: ['prompt_hash', 'text', 'response_a', 'response_b', 'pref_human', 'pref_gpt4', 'rating_human', 'rating_gpt4', 'completions', 'subject_of_expertise', 'expertise_level', 'languages', 'open_endedness', 'safety_concern', 'complexity_of_intents', 'type_of_in_context_material', 'format_constraints']\n",
      "2024-09-02 13:08:01 - INFO - root - Extracting features\n",
      "2024-09-02 13:08:01 - INFO - root - Getting instances. Needs at least 0/0 to swap to human preferences.\n",
      "2024-09-02 13:08:01 - INFO - root - Didn't find any features for this combination. Will return GPT-4 preferences\n",
      "2024-09-02 13:08:01 - INFO - root - Converting annotations into DPO training format\n",
      "2024-09-02 13:08:01 - INFO - root - Number of instances after selection: 8767\n",
      "2024-09-02 13:08:01 - INFO - root - Sampled 7000 instances from the total.\n",
      "2024-09-02 13:08:01 - INFO - root - Tag set 'scalar__feature_name-complexity_of_intents|value-simple___open_set__feature_name-format_constraints|check_for_existence-1___scalar__feature_name-open_endedness|value-high___scalar__feature_name-safety_concern|value-safe___open_set__feature_name-type_of_in_context_material|check_for_existence-1' resulted into 0 swaps! Skipping\n",
      "scalar::feature_name=complexity_of_intents|value=simple open_set::feature_name=format_constraints|check_for_existence=1 scalar::feature_name=safety_concern|value=safe open_set::feature_name=type_of_in_context_material|check_for_existence=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:00, 106454.42it/s]\n",
      "45it [00:00, 102522.37it/s]\n",
      "120it [00:00, 65896.37it/s]\n",
      "210it [00:00, 59776.30it/s]\n",
      "252it [00:00, 19551.33it/s]\n",
      "210it [00:00, 11281.80it/s]\n",
      "120it [00:00, 35224.05it/s]\n",
      "45it [00:00, 21058.09it/s]\n",
      "10it [00:00, 22477.51it/s]\n",
      "1it [00:00, 7570.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-02 13:08:01 - INFO - root - Adding meta analyzer features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "10it [00:00, 124830.48it/s]\n",
      "45it [00:00, 98509.23it/s]\n",
      "120it [00:00, 76771.89it/s]\n",
      "210it [00:00, 55821.27it/s]\n",
      "252it [00:00, 51478.89it/s]\n",
      "210it [00:00, 40337.23it/s]\n",
      "120it [00:00, 37346.33it/s]\n",
      "45it [00:00, 33758.48it/s]\n",
      "10it [00:00, 22322.00it/s]\n",
      "1it [00:00, 14266.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-02 13:08:01 - INFO - root - Found 10160 prompts with cols: ['prompt_hash', 'text', 'response_a', 'response_b', 'pref_human', 'pref_gpt4', 'rating_human', 'rating_gpt4', 'completions', 'subject_of_expertise', 'expertise_level', 'languages', 'open_endedness', 'safety_concern', 'complexity_of_intents', 'type_of_in_context_material', 'format_constraints']\n",
      "2024-09-02 13:08:01 - INFO - root - Extracting features\n",
      "2024-09-02 13:08:01 - INFO - root - Getting instances. Needs at least 0/0 to swap to human preferences.\n",
      "2024-09-02 13:08:01 - INFO - root - Didn't find any features for this combination. Will return GPT-4 preferences\n",
      "2024-09-02 13:08:01 - INFO - root - Converting annotations into DPO training format\n",
      "2024-09-02 13:08:01 - INFO - root - Number of instances after selection: 8767\n",
      "2024-09-02 13:08:01 - INFO - root - Sampled 7000 instances from the total.\n",
      "2024-09-02 13:08:01 - INFO - root - Tag set 'scalar__feature_name-complexity_of_intents|value-simple___open_set__feature_name-format_constraints|check_for_existence-1___scalar__feature_name-safety_concern|value-safe___open_set__feature_name-type_of_in_context_material|check_for_existence-1' resulted into 0 swaps! Skipping\n",
      "scalar::feature_name=complexity_of_intents|value=simple open_set::feature_name=format_constraints|check_for_existence=1 scalar::feature_name=safety_concern|value=safe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:00, 106454.42it/s]\n",
      "45it [00:00, 115439.56it/s]\n",
      "120it [00:00, 80595.11it/s]\n",
      "210it [00:00, 65829.88it/s]\n",
      "252it [00:00, 51125.31it/s]\n",
      "210it [00:00, 42701.50it/s]\n",
      "120it [00:00, 37738.36it/s]\n",
      "45it [00:00, 33335.16it/s]\n",
      "10it [00:00, 20410.24it/s]\n",
      "1it [00:00, 12826.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-02 13:08:01 - INFO - root - Adding meta analyzer features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "10it [00:00, 131482.88it/s]\n",
      "45it [00:00, 82420.82it/s]\n",
      "120it [00:00, 79993.08it/s]\n",
      "210it [00:00, 66541.05it/s]\n",
      "252it [00:00, 50031.46it/s]\n",
      "210it [00:00, 42161.88it/s]\n",
      "120it [00:00, 36064.52it/s]\n",
      "45it [00:00, 27691.27it/s]\n",
      "10it [00:00, 28320.76it/s]\n",
      "1it [00:00, 9341.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-02 13:08:02 - INFO - root - Found 10160 prompts with cols: ['prompt_hash', 'text', 'response_a', 'response_b', 'pref_human', 'pref_gpt4', 'rating_human', 'rating_gpt4', 'completions', 'subject_of_expertise', 'expertise_level', 'languages', 'open_endedness', 'safety_concern', 'complexity_of_intents', 'type_of_in_context_material', 'format_constraints']\n",
      "2024-09-02 13:08:02 - INFO - root - Extracting features\n",
      "2024-09-02 13:08:02 - INFO - root - Getting instances. Needs at least 0/0 to swap to human preferences.\n",
      "2024-09-02 13:08:02 - INFO - root - Didn't find any features for this combination. Will return GPT-4 preferences\n",
      "2024-09-02 13:08:02 - INFO - root - Converting annotations into DPO training format\n",
      "2024-09-02 13:08:02 - INFO - root - Number of instances after selection: 8767\n",
      "2024-09-02 13:08:02 - INFO - root - Sampled 7000 instances from the total.\n",
      "2024-09-02 13:08:02 - INFO - root - Tag set 'scalar__feature_name-complexity_of_intents|value-simple___open_set__feature_name-format_constraints|check_for_existence-1___scalar__feature_name-safety_concern|value-safe' resulted into 0 swaps! Skipping\n",
      "scalar::feature_name=complexity_of_intents|value=simple open_set::feature_name=format_constraints|check_for_existence=1 scalar::feature_name=safety_concern|value=safe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:00, 93414.34it/s]\n",
      "45it [00:00, 89324.98it/s]\n",
      "120it [00:00, 8883.57it/s]\n",
      "210it [00:00, 13904.43it/s]\n",
      "252it [00:00, 48829.56it/s]\n",
      "210it [00:00, 38299.15it/s]\n",
      "120it [00:00, 34699.52it/s]\n",
      "45it [00:00, 30526.23it/s]\n",
      "10it [00:00, 21788.59it/s]\n",
      "1it [00:00, 11491.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-02 13:08:02 - INFO - root - Adding meta analyzer features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "10it [00:00, 96199.63it/s]\n",
      "45it [00:00, 118632.11it/s]\n",
      "120it [00:00, 70919.61it/s]\n",
      "210it [00:00, 58743.75it/s]\n",
      "252it [00:00, 46919.90it/s]\n",
      "210it [00:00, 41160.98it/s]\n",
      "120it [00:00, 35734.22it/s]\n",
      "45it [00:00, 31604.77it/s]\n",
      "10it [00:00, 24951.24it/s]\n",
      "1it [00:00, 12520.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-02 13:08:02 - INFO - root - Found 10160 prompts with cols: ['prompt_hash', 'text', 'response_a', 'response_b', 'pref_human', 'pref_gpt4', 'rating_human', 'rating_gpt4', 'completions', 'subject_of_expertise', 'expertise_level', 'languages', 'open_endedness', 'safety_concern', 'complexity_of_intents', 'type_of_in_context_material', 'format_constraints']\n",
      "2024-09-02 13:08:02 - INFO - root - Extracting features\n",
      "2024-09-02 13:08:02 - INFO - root - Getting instances. Needs at least 0/0 to swap to human preferences.\n",
      "2024-09-02 13:08:02 - INFO - root - Didn't find any features for this combination. Will return GPT-4 preferences\n",
      "2024-09-02 13:08:02 - INFO - root - Converting annotations into DPO training format\n",
      "2024-09-02 13:08:02 - INFO - root - Number of instances after selection: 8767\n",
      "2024-09-02 13:08:02 - INFO - root - Sampled 7000 instances from the total.\n",
      "2024-09-02 13:08:02 - INFO - root - Tag set 'scalar__feature_name-complexity_of_intents|value-simple___open_set__feature_name-format_constraints|check_for_existence-1___scalar__feature_name-safety_concern|value-safe' resulted into 0 swaps! Skipping\n"
     ]
    }
   ],
   "source": [
    "for combination in top_combinations:\n",
    "    feats_to_run = []\n",
    "    for feat in combination:\n",
    "        if \"min_val\" in feat:\n",
    "            feats_to_run.append(feat.replace(\"__\", \"::\"))\n",
    "        else:\n",
    "            feat_name, value = feat.split(\"=\")\n",
    "            category = find_meta_category(feat_name)\n",
    "            if category == \"closed_set\":\n",
    "                key = \"constraints\"\n",
    "            elif category == \"scalar\":\n",
    "                key = \"value\"\n",
    "            elif category == \"open_set\":\n",
    "                key = \"check_for_existence\"\n",
    "            feats_to_run.append(f\"{category}::feature_name={feat_name}|{key}={value}\")\n",
    "    features_str = \" \".join(feats_to_run)\n",
    "    print(features_str)\n",
    "    %run ../../scripts/apply_data_model.py single --input_path helpsteer2_human_vs_gpt4_weighted_for_llama.jsonl --output_dir . --append_to_experiments_file top_features_experiments.txt --features {features_str}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
