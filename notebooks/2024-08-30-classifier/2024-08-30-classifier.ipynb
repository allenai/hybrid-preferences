{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "from ast import literal_eval\n",
    "from pathlib import Path\n",
    "from functools import reduce\n",
    "\n",
    "from src.utils import find_meta_category\n",
    "from src.feature_extractor import sample_feature_combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download prerequisite files\n",
    "\n",
    "Fetch all the results and feature values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching experiments list...\n",
      "Downloading dataset \u001b[36m01J6KF3JRCATRJQ9CPJTRV5VBM\u001b[0m to \u001b[32m.\u001b[0m\n",
      "Files: 0          ⠋  \n",
      "Bytes: 0 B        ⠋  \n",
      "\u001b[2A\u001b[JFiles: 1          ⠙  \n",
      "Bytes: 73.77 KiB  ⠙  \n",
      "\u001b[2A\u001b[JFiles: 1          ✔  \n",
      "Bytes: 73.77 KiB  ✔  \n",
      "\u001b[2A\u001b[JFiles: 1          ✔  \n",
      "Bytes: 73.77 KiB  ✔  \n",
      "Completed in 100ms: 439.4 KiB/s, 6 files/s\n",
      "Fetching extracted features...\n",
      "mkdir: features/: File exists\n",
      "Downloading dataset \u001b[36m01J6KF3JRCATRJQ9CPJTRV5VBM\u001b[0m to \u001b[32m.\u001b[0m\n",
      "Files: 0          ⠋  \n",
      "Bytes: 0 B        ⠋  \n",
      "\u001b[2A\u001b[JFiles: 2          ⠙  \n",
      "Bytes: 75.1 MiB   ⠙  \n",
      "\u001b[2A\u001b[JFiles: 9          ⠹  \n",
      "Bytes: 339 MiB    ⠹  \n",
      "\u001b[2A\u001b[JFiles: 16         ⠸  \n",
      "Bytes: 602.9 MiB  ⠸  \n",
      "\u001b[2A\u001b[JFiles: 16         ✔  \n",
      "Bytes: 602.9 MiB  ✔  \n",
      "\u001b[2A\u001b[JFiles: 16         ✔  \n",
      "Bytes: 602.9 MiB  ✔  \n",
      "Completed in 400ms: 1.23 GiB/s, 33 files/s\n",
      "Fetching helpsteer2 dataset\n",
      "Downloading dataset \u001b[36m01J6KBM2VCM9EQ7MER26VBXCCM\u001b[0m to \u001b[32m.\u001b[0m\n",
      "Files: 0          ⠋  \n",
      "Bytes: 0 B        ⠋  \n",
      "\u001b[2A\u001b[JFiles: 1          ⠙  \n",
      "Bytes: 70.58 MiB  ⠙  \n",
      "\u001b[2A\u001b[JFiles: 1          ✔  \n",
      "Bytes: 70.58 MiB  ✔  \n",
      "\u001b[2A\u001b[JFiles: 1          ✔  \n",
      "Bytes: 70.58 MiB  ✔  \n",
      "Completed in 100ms: 362.2 MiB/s, 5 files/s\n",
      "Collating all evaluation results\n",
      "2024-09-02 12:59:53 - INFO - root - Logged-in as ljm (ljm@allenai.org)\n",
      "2024-09-02 12:59:54 - INFO - root - Found 194 experiments that match 'rm-eval-helpsteer2'\n",
      "2024-09-02 13:00:18 - INFO - root - Computing category scores...\n",
      "2024-09-02 13:00:18 - INFO - root - Deriving features from the experiments file: experiments.txt\n",
      "2024-09-02 13:00:18 - INFO - root - Will attempt merge via feature hash\n",
      "2024-09-02 13:00:18 - INFO - root - Creating labels in column 'label' with GPT-4 threshold '0.658'\n",
      "2024-09-02 13:00:18 - INFO - root - Saving 62 results to results.csv\n",
      "2024-09-02 13:00:18 - INFO - root - Saved on results.csv\n"
     ]
    }
   ],
   "source": [
    "# You can get the experiments file here: 01J6KF3JRCATRJQ9CPJTRV5VBM (https://beaker.org/ds/01J6KF3JRCATRJQ9CPJTRV5VBM/details)\n",
    "!echo \"Fetching experiments list...\"\n",
    "!beaker dataset fetch 01J6KF3JRCATRJQ9CPJTRV5VBM --prefix experiments.txt\n",
    "!echo \"Fetching extracted features...\"\n",
    "!mkdir features/\n",
    "!beaker dataset fetch 01J6KF3JRCATRJQ9CPJTRV5VBM --prefix features/ \n",
    "#!beaker dataset fetch 01J6KFVCRCTYHCZDR0XNK0G9HT --prefix features/\n",
    "!echo \"Fetching helpsteer2 dataset\"\n",
    "!beaker dataset fetch 01J6KBM2VCM9EQ7MER26VBXCCM\n",
    "!echo \"Collating all evaluation results\"\n",
    "%run ../../scripts/fetch_evals_rewardbench.py --output_file results.csv --gpt4_threshold_score 0.658 --experiment_prefix rm-eval-helpsteer2 --experiments_file experiments.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collate feature set for all instances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEXICAL_FEATS_PATH = Path(\"features\")\n",
    "DATASET_PATH = Path(\"helpsteer2_human_vs_gpt4_weighted_for_llama.jsonl\")\n",
    "\n",
    "\n",
    "def get_dataset_features(\n",
    "    feature_path=LEXICAL_FEATS_PATH, dataset_path=DATASET_PATH\n",
    ") -> \"pd.DataFrame\":\n",
    "    lexical_features = [\n",
    "        \"rouge\",\n",
    "        \"bertscore\",\n",
    "        \"bertscore_length\",\n",
    "        \"entity_sim\",\n",
    "        \"cosine_sim\",\n",
    "        \"prompt_len\",\n",
    "        \"len_longer\",\n",
    "        \"len_shorter\",\n",
    "        \"token_len_difference\",\n",
    "    ]\n",
    "    lexical_feature_files = [\n",
    "        file\n",
    "        for file in feature_path.glob(\"*.jsonl\")\n",
    "        if any(file.stem in feat for feat in lexical_features)\n",
    "    ]\n",
    "    lexical_feats_df = reduce(\n",
    "        lambda left, right: left.merge(\n",
    "            right, on=[\"id\", \"prompt\", \"completion_a\", \"completion_b\"], how=\"outer\"\n",
    "        ),\n",
    "        [pd.read_json(file, lines=True) for file in lexical_feature_files],\n",
    "    )\n",
    "\n",
    "    df = pd.read_json(dataset_path, lines=True).rename(columns={\"prompt_hash\": \"id\"})\n",
    "    finaldf = df.merge(lexical_feats_df, how=\"left\", on=\"id\").drop(\n",
    "        columns=[\"prompt\", \"completion_a\", \"completion_b\"]\n",
    "    )\n",
    "\n",
    "    # Hacky way for token_len_difference\n",
    "    finaldf = finaldf.rename(columns={\"token_len_diff\": \"token_len_difference\"})\n",
    "    return finaldf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.read_csv(\"results.csv\").dropna()\n",
    "features_df = get_dataset_features()\n",
    "print(len(results_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get proportion of instances that fulfill the conditions\n",
    "\n",
    "1. For each row, get features that were activated\n",
    "2. Then for each activated feature, we get the proportion by looking at the feature dataframe.\n",
    "3. The proportion is computed as: `number_of_instance_that_fulfill_a_single_condition` / `total_number_of_instances`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>expertise_level</th>\n",
       "      <th>format_constraints</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1317</th>\n",
       "      <td>expert domain knowledge</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4613</th>\n",
       "      <td>basic domain knowledge</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4734</th>\n",
       "      <td>general public</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              expertise_level format_constraints\n",
       "289                      None                 []\n",
       "1317  expert domain knowledge               None\n",
       "4613   basic domain knowledge               None\n",
       "4734           general public               None"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect nan columns\n",
    "rows_with_nan = features_df[features_df.isna().any(axis=1)]\n",
    "nan_columns = rows_with_nan.columns[rows_with_nan.isna().any()]\n",
    "df_nan_columns = rows_with_nan[nan_columns]\n",
    "df_nan_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what you're going to do instead, is to take the binary_cols, and then for each element of that binary_cols, you compute the \"weight\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_instances(feat: str, features_df: \"pd.DataFrame\") -> float:\n",
    "    total = len(features_df)\n",
    "    lexical_features = [\n",
    "        \"rouge\",\n",
    "        \"bertscore\",\n",
    "        \"bertscore_length\",\n",
    "        \"entity_sim\",\n",
    "        \"cosine_sim\",\n",
    "        \"prompt_len\",\n",
    "        \"len_longer\",\n",
    "        \"len_shorter\",\n",
    "        \"token_len_difference\",\n",
    "    ]\n",
    "\n",
    "    if feat.split(\"__\")[0] in lexical_features:\n",
    "        feat_name, value = feat.split(\"__\")\n",
    "        min_val_str, max_val_str = value.split(\"|\")\n",
    "        min_val, max_val = float(min_val_str.split(\"=\")[1]), float(\n",
    "            max_val_str.split(\"=\")[1]\n",
    "        )\n",
    "        return features_df[feat_name].between(min_val, max_val).mean()\n",
    "    else:\n",
    "        # Parse the feature\n",
    "        feat_name, value = feat.split(\"=\")\n",
    "        meta_category = find_meta_category(feat_name)\n",
    "        if meta_category == \"scalar\":\n",
    "            v = value.replace(\"_\", \" \")\n",
    "            return features_df[feat_name].value_counts().get(v) / total\n",
    "        elif meta_category == \"closed_set\":\n",
    "            v = value.replace(\"_\", \" \")\n",
    "            list_of_values = features_df[feat_name].tolist()\n",
    "            return sum([1 if v in listval else 0 for listval in list_of_values]) / total\n",
    "        elif meta_category == \"open_set\":\n",
    "            list_of_values = features_df[feat_name].tolist()\n",
    "            return sum([1 if listval else 0 for listval in list_of_values]) / total\n",
    "\n",
    "        return find_meta_category(feat_name)\n",
    "\n",
    "\n",
    "feats = results_df.columns[results_df.isin([0, 1]).all()]  # get binary columns\n",
    "feat_map = {\n",
    "    feat: compute_instances(feat, features_df) for feat in feats if feat != \"label\"\n",
    "}\n",
    "\n",
    "ratio_df = results_df.apply(\n",
    "    lambda row: row.map(lambda x: feat_map.get(row.name, 1) if x == 1 else x)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressor training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LightGBM regressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 55, test size: 7\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 55, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score 0.695253\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "Mean Squared Error: 0.0013998446533237798\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"objective\": \"regression\",\n",
    "    \"metric\": \"mse\",\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"num_leaves\": 31,\n",
    "}\n",
    "\n",
    "# Train the model\n",
    "binary = False\n",
    "X = ratio_df[list(feat_map.keys())]\n",
    "if binary:\n",
    "    X = (X > 0).astype(int)\n",
    "y = ratio_df[\"Overall\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.1, random_state=42\n",
    ")\n",
    "print(f\"Train size: {len(X_train)}, test size: {len(X_test)}\")\n",
    "\n",
    "\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
    "model = lgb.train(params, train_data, valid_sets=[test_data])\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  Feature  Importance\n",
      "0    bertscore__min_val=0.33|max_val=0.67         0.0\n",
      "33       rouge__min_val=0.33|max_val=0.67         0.0\n",
      "35                    safety_concern=high         0.0\n",
      "36                     safety_concern=low         0.0\n",
      "37                safety_concern=moderate         0.0\n",
      "..                                    ...         ...\n",
      "27                open_endedness=moderate         0.0\n",
      "28                      open_endedness=no         0.0\n",
      "29   prompt_len__min_val=0.0|max_val=0.33         0.0\n",
      "30  prompt_len__min_val=0.33|max_val=0.67         0.0\n",
      "64          type_of_in_context_material=1         0.0\n",
      "\n",
      "[65 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "importances = model.feature_importance(importance_type=\"gain\")  # ['split', 'gain']\n",
    "\n",
    "# Create a DataFrame to view feature importances\n",
    "feature_importance_df = pd.DataFrame(\n",
    "    {\"Feature\": X.columns, \"Importance\": importances}\n",
    ").sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "print(feature_importance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LinearRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 49, test size: 13\n",
      "Feature names: ['bertscore__min_val=0.33|max_val=0.67'\n",
      " 'bertscore__min_val=0.67|max_val=1.0'\n",
      " 'bertscore_length__min_val=0.0|max_val=0.33' ...\n",
      " 'token_len_difference__min_val=0.67|max_val=1.0^2'\n",
      " 'token_len_difference__min_val=0.67|max_val=1.0 type_of_in_context_material=1'\n",
      " 'type_of_in_context_material=1^2']\n",
      "Mean Squared Error: 0.0012591021224480326\n",
      "Coeeficients: [-4.07087444e-01  2.64518756e-02  7.84504190e-02 ...  1.13884130e-03\n",
      "  0.00000000e+00  3.17208325e-04]\n",
      "Intercept: 0.7141910311097194\n"
     ]
    }
   ],
   "source": [
    "polyfit = True\n",
    "binary = False\n",
    "\n",
    "X = ratio_df[list(feat_map.keys())]\n",
    "y = ratio_df[\"Overall\"]\n",
    "if binary:\n",
    "    X = (X > 0).astype(int)\n",
    "\n",
    "if polyfit:\n",
    "    poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "    X_poly = poly.fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_poly, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "else:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "\n",
    "print(f\"Train size: {len(X_train)}, test size: {len(X_test)}\")\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Feature names: {poly.get_feature_names_out(X.columns)}\")\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"Coeeficients: {model.coef_}\")\n",
    "print(f\"Intercept: {model.intercept_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance is not possible with polynomial features (hard to interpret)\n"
     ]
    }
   ],
   "source": [
    "if not polyfit:\n",
    "    feature_importance = pd.DataFrame(\n",
    "        {\"Feature\": X.columns, \"Coefficient\": model.coef_}\n",
    "    )\n",
    "\n",
    "    # Calculate absolute importance for easier comparison\n",
    "    feature_importance[\"Absolute_Coefficient\"] = np.abs(\n",
    "        feature_importance[\"Coefficient\"]\n",
    "    )\n",
    "\n",
    "    # Sort by absolute coefficient value\n",
    "    feature_importance = feature_importance.sort_values(\n",
    "        by=\"Absolute_Coefficient\", ascending=False\n",
    "    )\n",
    "    feature_importance.head(10)\n",
    "else:\n",
    "    print(\n",
    "        \"Feature importance is not possible with polynomial features (hard to interpret)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:00, 46500.04it/s]\n",
      "45it [00:00, 68609.12it/s]\n",
      "120it [00:00, 62859.56it/s]\n",
      "210it [00:00, 50467.19it/s]\n",
      "252it [00:00, 49643.73it/s]\n",
      "210it [00:00, 41807.66it/s]\n",
      "120it [00:00, 34911.32it/s]\n",
      "45it [00:00, 30795.18it/s]\n",
      "10it [00:00, 20794.76it/s]\n",
      "1it [00:00, 8924.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-02 13:00:21 - INFO - root - Adding meta analyzer features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "10it [00:00, 110960.42it/s]\n",
      "45it [00:00, 106936.93it/s]\n",
      "120it [00:00, 75527.68it/s]\n",
      "210it [00:00, 62015.34it/s]\n",
      "252it [00:00, 48155.48it/s]\n",
      "210it [00:00, 41389.21it/s]\n",
      "120it [00:00, 35037.69it/s]\n",
      "45it [00:00, 29217.29it/s]\n",
      "10it [00:00, 21421.37it/s]\n",
      "1it [00:00, 15650.39it/s]\n"
     ]
    }
   ],
   "source": [
    "_, combinations = sample_feature_combinations(\n",
    "    meta_analyzer_n_samples=2000, max_number=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hx/gk2rs0792pn5p8hkj4nkhdm80000gp/T/ipykernel_36621/4023816742.py:2: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for idx, combination in tqdm_notebook(enumerate(combinations), total=len(combinations)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "866653ce3db44a49b1fd1da6d53de371",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4069 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sim_df = pd.DataFrame(0, index=np.arange(len(combinations)), columns=X.columns)\n",
    "for idx, combination in tqdm_notebook(enumerate(combinations), total=len(combinations)):\n",
    "    activated_feats = []\n",
    "    for feat in combination:\n",
    "        if \"analyzer\" in feat:\n",
    "            feature_name_str, value_str = feat.split(\"::\")[1].split(\"|\")\n",
    "            feature_name, value = (\n",
    "                feature_name_str.split(\"=\")[-1],\n",
    "                value_str.split(\"=\")[-1],\n",
    "            )\n",
    "            activated_feats.append(f\"{feature_name}={value}\")\n",
    "        else:\n",
    "            activated_feats.append(feat.replace(\"::\", \"__\"))\n",
    "    sim_df.loc[idx, activated_feats] = 1\n",
    "sim_df = sim_df.apply(\n",
    "    lambda row: row.map(lambda x: feat_map.get(row.name, 1) if x == 1 else x)\n",
    ").dropna(axis=1, how=\"any\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activated_features</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[bertscore__min_val=0.67|max_val=1.0, complexi...</td>\n",
       "      <td>0.812256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[bertscore_length__min_val=0.0|max_val=0.33, e...</td>\n",
       "      <td>0.810126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[complexity_of_intents=simple, safety_concern=...</td>\n",
       "      <td>0.800891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[bertscore__min_val=0.67|max_val=1.0, cosine_s...</td>\n",
       "      <td>0.799207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[bertscore__min_val=0.67|max_val=1.0, cosine_s...</td>\n",
       "      <td>0.798044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[bertscore_length__min_val=0.33|max_val=0.67, ...</td>\n",
       "      <td>0.792909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[complexity_of_intents=simple, format_constrai...</td>\n",
       "      <td>0.791309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[complexity_of_intents=simple, format_constrai...</td>\n",
       "      <td>0.791309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[complexity_of_intents=simple, format_constrai...</td>\n",
       "      <td>0.790966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[complexity_of_intents=simple, format_constrai...</td>\n",
       "      <td>0.790966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[complexity_of_intents=simple, format_constrai...</td>\n",
       "      <td>0.790787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[complexity_of_intents=simple, open_endedness=...</td>\n",
       "      <td>0.789959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[bertscore_length__min_val=0.0|max_val=0.33, c...</td>\n",
       "      <td>0.788190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[bertscore__min_val=0.67|max_val=1.0, cosine_s...</td>\n",
       "      <td>0.784764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[bertscore__min_val=0.33|max_val=0.67, bertsco...</td>\n",
       "      <td>0.782252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[bertscore__min_val=0.67|max_val=1.0, complexi...</td>\n",
       "      <td>0.782118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[bertscore__min_val=0.67|max_val=1.0, entity_s...</td>\n",
       "      <td>0.781140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[complexity_of_intents=simple, entity_sim__min...</td>\n",
       "      <td>0.779665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[bertscore_length__min_val=0.0|max_val=0.33, l...</td>\n",
       "      <td>0.779619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[bertscore_length__min_val=0.0|max_val=0.33, e...</td>\n",
       "      <td>0.779271</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   activated_features      pred\n",
       "0   [bertscore__min_val=0.67|max_val=1.0, complexi...  0.812256\n",
       "1   [bertscore_length__min_val=0.0|max_val=0.33, e...  0.810126\n",
       "2   [complexity_of_intents=simple, safety_concern=...  0.800891\n",
       "3   [bertscore__min_val=0.67|max_val=1.0, cosine_s...  0.799207\n",
       "4   [bertscore__min_val=0.67|max_val=1.0, cosine_s...  0.798044\n",
       "5   [bertscore_length__min_val=0.33|max_val=0.67, ...  0.792909\n",
       "6   [complexity_of_intents=simple, format_constrai...  0.791309\n",
       "7   [complexity_of_intents=simple, format_constrai...  0.791309\n",
       "8   [complexity_of_intents=simple, format_constrai...  0.790966\n",
       "9   [complexity_of_intents=simple, format_constrai...  0.790966\n",
       "10  [complexity_of_intents=simple, format_constrai...  0.790787\n",
       "11  [complexity_of_intents=simple, open_endedness=...  0.789959\n",
       "12  [bertscore_length__min_val=0.0|max_val=0.33, c...  0.788190\n",
       "13  [bertscore__min_val=0.67|max_val=1.0, cosine_s...  0.784764\n",
       "14  [bertscore__min_val=0.33|max_val=0.67, bertsco...  0.782252\n",
       "15  [bertscore__min_val=0.67|max_val=1.0, complexi...  0.782118\n",
       "16  [bertscore__min_val=0.67|max_val=1.0, entity_s...  0.781140\n",
       "17  [complexity_of_intents=simple, entity_sim__min...  0.779665\n",
       "18  [bertscore_length__min_val=0.0|max_val=0.33, l...  0.779619\n",
       "19  [bertscore_length__min_val=0.0|max_val=0.33, e...  0.779271"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_results = sim_df.copy(deep=True)\n",
    "sim_results[\"activated_features\"] = sim_results.apply(\n",
    "    lambda row: [col for col in sim_results.columns if row[col] != 0], axis=1\n",
    ")\n",
    "sim_results[\"pred\"] = model.predict(poly.transform(sim_df))\n",
    "sim_results = sim_results.sort_values(by=\"pred\", ascending=False).reset_index(drop=True)\n",
    "sim_results[[\"activated_features\", \"pred\"]].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['bertscore__min_val=0.67|max_val=1.0', 'complexity_of_intents=simple', 'entity_sim__min_val=0.33|max_val=0.67', 'token_len_difference__min_val=0.67|max_val=1.0', 'type_of_in_context_material=1'], ['bertscore_length__min_val=0.0|max_val=0.33', 'entity_sim__min_val=0.33|max_val=0.67', 'open_endedness=moderate', 'prompt_len__min_val=0.67|max_val=1.0', 'safety_concern=safe'], ['complexity_of_intents=simple', 'safety_concern=safe'], ['bertscore__min_val=0.67|max_val=1.0', 'cosine_sim__min_val=0.67|max_val=1.0', 'entity_sim__min_val=0.0|max_val=0.33', 'token_len_difference__min_val=0.67|max_val=1.0'], ['bertscore__min_val=0.67|max_val=1.0', 'cosine_sim__min_val=0.67|max_val=1.0', 'entity_sim__min_val=0.0|max_val=0.33'], ['bertscore_length__min_val=0.33|max_val=0.67', 'complexity_of_intents=simple', 'format_constraints=1', 'open_endedness=no', 'rouge__min_val=0.0|max_val=0.33', 'safety_concern=safe', 'type_of_in_context_material=1'], ['complexity_of_intents=simple', 'format_constraints=1', 'open_endedness=high', 'safety_concern=safe', 'type_of_in_context_material=1'], ['complexity_of_intents=simple', 'format_constraints=1', 'safety_concern=safe', 'type_of_in_context_material=1'], ['complexity_of_intents=simple', 'format_constraints=1', 'safety_concern=safe'], ['complexity_of_intents=simple', 'format_constraints=1', 'safety_concern=safe']]\n"
     ]
    }
   ],
   "source": [
    "top_combinations = sim_results.activated_features.head(10).to_list()\n",
    "print(top_combinations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: So now you have determined 10 feature combinations that seem to work well. The next step is to train RMs and evaluate them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bertscore::min_val=0.67|max_val=1.0 scalar::feature_name=complexity_of_intents|value=simple entity_sim::min_val=0.33|max_val=0.67 token_len_difference::min_val=0.67|max_val=1.0 open_set::feature_name=type_of_in_context_material|check_for_existence=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:00, 88674.50it/s]\n",
      "45it [00:00, 98663.71it/s]\n",
      "120it [00:00, 80750.28it/s]\n",
      "210it [00:00, 59225.65it/s]\n",
      "252it [00:00, 51895.94it/s]\n",
      "210it [00:00, 41160.98it/s]\n",
      "120it [00:00, 35565.04it/s]\n",
      "45it [00:00, 32728.23it/s]\n",
      "10it [00:00, 20784.46it/s]\n",
      "1it [00:00, 15141.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-02 13:12:15 - INFO - root - Adding meta analyzer features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "10it [00:00, 91779.08it/s]\n",
      "45it [00:00, 104857.60it/s]\n",
      "120it [00:00, 74400.07it/s]\n",
      "210it [00:00, 59658.89it/s]\n",
      "252it [00:00, 49774.65it/s]\n",
      "210it [00:00, 40439.09it/s]\n",
      "120it [00:00, 33831.85it/s]\n",
      "45it [00:00, 29376.45it/s]\n",
      "10it [00:00, 22721.04it/s]\n",
      "1it [00:00, 15141.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-02 13:12:15 - INFO - root - Found 10160 prompts with cols: ['prompt_hash', 'text', 'response_a', 'response_b', 'pref_human', 'pref_gpt4', 'rating_human', 'rating_gpt4', 'completions', 'subject_of_expertise', 'expertise_level', 'languages', 'open_endedness', 'safety_concern', 'complexity_of_intents', 'type_of_in_context_material', 'format_constraints']\n",
      "2024-09-02 13:12:15 - INFO - root - Using device: cpu\n",
      "2024-09-02 13:12:15 - INFO - root - Extracting features\n",
      "2024-09-02 13:12:15 - INFO - root - Extracting 'bertscore' with params: {'min_val': 0.67, 'max_val': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ljvm/Documents/Work/human-pref-datamodel/venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b8baa81c9d3483d962b0716896fe0c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/317 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for combination in top_combinations:\n",
    "    feats_to_run = []\n",
    "    for feat in combination:\n",
    "        if \"min_val\" in feat:\n",
    "            feats_to_run.append(feat.replace(\"__\", \"::\"))\n",
    "        else:\n",
    "            feat_name, value = feat.split(\"=\")\n",
    "            category = find_meta_category(feat_name)\n",
    "            if category == \"closed_set\":\n",
    "                key = \"constraints\"\n",
    "            elif category == \"scalar\":\n",
    "                key = \"value\"\n",
    "            elif category == \"open_set\":\n",
    "                key = \"check_for_existence\"\n",
    "            feats_to_run.append(f\"{category}::feature_name={feat_name}|{key}={value}\")\n",
    "    features_str = \" \".join(feats_to_run)\n",
    "    print(features_str)\n",
    "    %run ../../scripts/apply_data_model.py single --input_path helpsteer2_human_vs_gpt4_weighted_for_llama.jsonl --output_dir . --append_to_experiments_file top_features_experiments.txt --features {features_str}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
