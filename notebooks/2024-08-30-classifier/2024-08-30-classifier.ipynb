{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "from pathlib import Path\n",
    "import hashlib\n",
    "from functools import reduce\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from src.utils import find_meta_category\n",
    "from src.feature_extractor import sample_feature_combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download prerequisite files\n",
    "\n",
    "Fetch all the results and feature values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_download = True\n",
    "if not skip_download:\n",
    "# You can get the experiments file here: 01J6KF3JRCATRJQ9CPJTRV5VBM (https://beaker.org/ds/01J6KF3JRCATRJQ9CPJTRV5VBM/details)\n",
    "    !echo \"Fetching experiments list...\"\n",
    "    !beaker dataset fetch 01J6KF3JRCATRJQ9CPJTRV5VBM --prefix experiments.txt -q\n",
    "    !echo \"Fetching extracted features...\"\n",
    "    !mkdir -p features/\n",
    "    !beaker dataset fetch 01J6KF3JRCATRJQ9CPJTRV5VBM --prefix features/ -q\n",
    "    #!beaker dataset fetch 01J6KFVCRCTYHCZDR0XNK0G9HT --prefix features/\n",
    "    !echo \"Fetching helpsteer2 dataset...\"\n",
    "    !beaker dataset fetch 01J6KBM2VCM9EQ7MER26VBXCCM\n",
    "    !echo \"Fetching extracted subsets... (this will take ~10 minutes)\"\n",
    "    !beaker dataset fetch 01J6KF3JRCATRJQ9CPJTRV5VBM --prefix data/ -q\n",
    "    \n",
    "!echo \"Collating all evaluation results\"\n",
    "%run ../../scripts/fetch_evals_rewardbench.py --output_file results.csv --gpt4_threshold_score 0.658 --experiment_prefix rm-eval-helpsteer2 --experiments_file experiments.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls data | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collate feature set for all instances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEXICAL_FEATS_PATH = Path(\"features\")\n",
    "DATASET_PATH = Path(\"helpsteer2_human_vs_gpt4_weighted_for_llama.jsonl\")\n",
    "\n",
    "\n",
    "def get_dataset_features(\n",
    "    feature_path=LEXICAL_FEATS_PATH, dataset_path=DATASET_PATH\n",
    ") -> \"pd.DataFrame\":\n",
    "    lexical_features = [\n",
    "        \"rouge\",\n",
    "        \"bertscore\",\n",
    "        \"bertscore_length\",\n",
    "        \"entity_sim\",\n",
    "        \"cosine_sim\",\n",
    "        \"prompt_len\",\n",
    "        \"len_longer\",\n",
    "        \"len_shorter\",\n",
    "        \"token_len_difference\",\n",
    "    ]\n",
    "    lexical_feature_files = [\n",
    "        file\n",
    "        for file in feature_path.glob(\"*.jsonl\")\n",
    "        if any(file.stem in feat for feat in lexical_features)\n",
    "    ]\n",
    "    lexical_feats_df = reduce(\n",
    "        lambda left, right: left.merge(\n",
    "            right, on=[\"id\", \"prompt\", \"completion_a\", \"completion_b\"], how=\"outer\"\n",
    "        ),\n",
    "        [pd.read_json(file, lines=True) for file in lexical_feature_files],\n",
    "    )\n",
    "\n",
    "    df = pd.read_json(dataset_path, lines=True).rename(columns={\"prompt_hash\": \"id\"})\n",
    "    finaldf = df.merge(lexical_feats_df, how=\"left\", on=\"id\").drop(\n",
    "        columns=[\"prompt\", \"completion_a\", \"completion_b\"]\n",
    "    )\n",
    "\n",
    "    # Hacky way for token_len_difference\n",
    "    # finaldf = finaldf.rename(columns={\"token_len_diff\": \"token_len_difference\"})\n",
    "    return finaldf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_csv(\"results.csv\").dropna()\n",
    "features_df = get_dataset_features()\n",
    "features_df.to_csv(\"helpsteer2_all_features.csv\", index=False)\n",
    "# print(len(results_df)), print(len(features_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.to_json(\"features.jsonl\", lines=True, orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataframe contains the features for instances in the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get proportion of instances that fulfill the conditions\n",
    "\n",
    "1. For each row, get features that were activated\n",
    "2. Then for each activated feature, we get the proportion by looking at the feature dataframe.\n",
    "3. The proportion is computed as: `number_of_instance_that_fulfill_a_single_condition` / `total_number_of_instances`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect nan columns\n",
    "rows_with_nan = features_df[features_df.isna().any(axis=1)]\n",
    "nan_columns = rows_with_nan.columns[rows_with_nan.isna().any()]\n",
    "df_nan_columns = rows_with_nan[nan_columns]\n",
    "df_nan_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what you're going to do instead, is to take the binary_cols, and then for each element of that binary_cols, you compute the \"weight\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_instances(feat: str, features_df: \"pd.DataFrame\") -> float:\n",
    "    \"\"\"Compute the ratio of instances that fulfill a given feature 'feat' vs. the total dataset 'len(features_df)'\"\"\"\n",
    "    total = len(features_df)\n",
    "    lexical_features = [\n",
    "        \"rouge\",\n",
    "        \"bertscore\",\n",
    "        \"bertscore_length\",\n",
    "        \"entity_sim\",\n",
    "        \"cosine_sim\",\n",
    "        \"prompt_len\",\n",
    "        \"len_longer\",\n",
    "        \"len_shorter\",\n",
    "        \"token_len_difference\",\n",
    "    ]\n",
    "\n",
    "    if feat.split(\"__\")[0] in lexical_features:\n",
    "        feat_name, value = feat.split(\"__\")\n",
    "        min_val_str, max_val_str = value.split(\"|\")\n",
    "        min_val, max_val = float(min_val_str.split(\"=\")[1]), float(\n",
    "            max_val_str.split(\"=\")[1]\n",
    "        )\n",
    "        return features_df[feat_name].between(min_val, max_val).mean()\n",
    "    else:\n",
    "        # Parse the feature\n",
    "        feat_name, value = feat.split(\"=\")\n",
    "        meta_category = find_meta_category(feat_name)\n",
    "        if meta_category == \"scalar\":\n",
    "            v = value.replace(\"_\", \" \")\n",
    "            return features_df[feat_name].value_counts().get(v) / total\n",
    "        elif meta_category == \"closed_set\":\n",
    "            v = value.replace(\"_\", \" \")\n",
    "            list_of_values = features_df[feat_name].tolist()\n",
    "            return sum([1 if v in listval else 0 for listval in list_of_values]) / total\n",
    "        elif meta_category == \"open_set\":\n",
    "            list_of_values = features_df[feat_name].tolist()\n",
    "            return sum([1 if listval else 0 for listval in list_of_values]) / total\n",
    "\n",
    "        return find_meta_category(feat_name)\n",
    "\n",
    "\n",
    "# feats = results_df.columns[results_df.isin([0, 1]).all()]  # get binary columns\n",
    "# feat_map = {\n",
    "#    feat: compute_instances(feat, features_df) for feat in feats if feat != \"label\"\n",
    "# }\n",
    "\n",
    "# ratio_df = results_df.apply(\n",
    "#    lambda row: row.map(lambda x: feat_map.get(row.name, 1) if x == 1 else x)\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each result, we get the `hash`, find the extracted subset (because they were randomly-sampled) from `data`, and compute the ratio from there.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "get_per_hash_ratios = True\n",
    "\n",
    "\n",
    "def extract_hash(string):\n",
    "    match = re.search(r\"FEATS_(.*?)_SWAPS\", string)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "\n",
    "result_hashes = results_df[\"hash\"].to_list()\n",
    "subsets = {extract_hash(str(file)): file for file in Path(\"data\").glob(\"*.jsonl\")}\n",
    "feats = results_df.columns[results_df.isin([0, 1]).all()]  # get binary collumns\n",
    "\n",
    "hash_ratios = {}\n",
    "for result_hash in tqdm_notebook(result_hashes):\n",
    "    if result_hash in subsets:\n",
    "        sampled_features_df = pd.read_json(subsets[result_hash], lines=True)\n",
    "        sampled_features_df[\"id\"] = sampled_features_df[\"prompt\"].apply(\n",
    "            lambda x: hashlib.md5(x.encode(\"utf-8\")).hexdigest()\n",
    "        )\n",
    "        # Get the features from features_df based on the existing prompt_hashes in sampled_features_df\n",
    "        sdf = features_df[features_df[\"id\"].isin(sampled_features_df[\"id\"].to_list())]\n",
    "        hash_ratios[result_hash] = {\n",
    "            feat: compute_instances(feat, sdf) for feat in feats if feat != \"label\"\n",
    "        }\n",
    "\n",
    "\n",
    "def replace_values(row):\n",
    "    feat_map = hash_ratios.get(row[\"hash\"], {})\n",
    "    for col in feat_map:\n",
    "        if row[col] == 1 and col in feat_map:\n",
    "            row[col] = feat_map[col]\n",
    "    return row\n",
    "\n",
    "\n",
    "ratio_df = results_df.apply(replace_values, axis=1)\n",
    "\n",
    "\n",
    "# Get feat_map with default counts\n",
    "feats = results_df.columns[results_df.isin([0, 1]).all()]  # get binary columns\n",
    "feat_map = {\n",
    "    feat: compute_instances(feat, features_df) for feat in feats if feat != \"label\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressor training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, root_mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "force_new_split = False\n",
    "\n",
    "feat_names = list(list(hash_ratios.values())[0].keys())\n",
    "if Path(\"validation_set.jsonl\").exists() and not force_new_split:\n",
    "    print(\"Reusing existing validation set\")\n",
    "    val_df = pd.read_json(\"validation_set.jsonl\", lines=True)\n",
    "    train_df = ratio_df[~ratio_df[\"hash\"].isin(val_df[\"hash\"])]\n",
    "    X_train = train_df[feat_names]\n",
    "    y_train = train_df[\"Overall\"]\n",
    "    X_test = val_df[feat_names]\n",
    "    y_test = val_df[\"Overall\"]\n",
    "else:\n",
    "    X = ratio_df[feat_names]\n",
    "    y = ratio_df[\"Overall\"]\n",
    "    X_train, X_test, y_train, y_test, train_idx, test_idx = train_test_split(\n",
    "        X, y, ratio_df.index, test_size=0.2, random_state=42\n",
    "    )\n",
    "    # Save the validation set\n",
    "    validation_set = ratio_df.loc[test_idx]\n",
    "    validation_set.to_json(\"validation_set.jsonl\", lines=True, orient=\"records\")\n",
    "\n",
    "print(f\"Train size: {len(X_train)}, test size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train LinearRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_linear_regressor(X_train, X_test, y_train, y_test):\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = root_mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    return model, {\"mse\": mse, \"rmse\": rmse}\n",
    "\n",
    "\n",
    "# print(f\"Feature names: {poly.get_feature_names_out(X.columns)}\")\n",
    "model, scores = train_linear_regressor(X_train, X_test, y_train, y_test)\n",
    "print(scores)\n",
    "print(f\"intercept: {model.intercept_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_of_train = [0.25, 0.50, 0.75, 1]\n",
    "for pct in pct_of_train:\n",
    "    num_train = int(len(X_train) * pct)\n",
    "    _, scores = train_linear_regressor(\n",
    "        X_train[:num_train], X_test, y_train[:num_train], y_test\n",
    "    )\n",
    "    print(num_train, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train LightGBM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "def train_lightgbm(X_train, X_test, y_train, y_test):\n",
    "    train_data = lgb.Dataset(X_train, label=y_train, params={\"verbose\": -1})\n",
    "    test_data = lgb.Dataset(\n",
    "        X_test, label=y_test, reference=train_data, params={\"verbose\": -1}\n",
    "    )\n",
    "    params = {\n",
    "        \"objective\": \"regression\",\n",
    "        \"metric\": \"rmse\",\n",
    "        \"boosting\": \"gbdt\",\n",
    "        \"learning_rate\": 0.1,\n",
    "        \"num_leaves\": 31,\n",
    "        \"scale_pos_weight\": 0.4,\n",
    "    }\n",
    "    # Train the model\n",
    "    model = lgb.train(params, train_data, valid_sets=[test_data])\n",
    "    # Predict and evaluate\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = root_mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    return model, {\"mse\": mse, \"rmse\": rmse}\n",
    "\n",
    "\n",
    "model, scores = train_lightgbm(X_train, X_test, y_train, y_test)\n",
    "print(scores)\n",
    "\n",
    "importances = model.feature_importance()\n",
    "importance_df = pd.DataFrame(\n",
    "    {\"feature\": X_train.columns, \"importance\": importances}\n",
    ").sort_values(by=\"importance\", ascending=False)\n",
    "importance_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_of_train = [0.25, 0.50, 0.75, 1]\n",
    "for pct in pct_of_train:\n",
    "    num_train = int(len(X_train) * pct)\n",
    "    _, scores = train_lightgbm(X_train[:num_train], X_test, y_train[:num_train], y_test)\n",
    "    print(num_train, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Polynomial Regressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def train_quadratic_regressor(X_train, X_test, y_train, y_test):\n",
    "    poly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=False)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    X_train_poly = poly.fit_transform(X_train_scaled)\n",
    "    X_test_poly = poly.transform(X_test_scaled)\n",
    "\n",
    "    print(X_train_poly)\n",
    "\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_poly, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test_poly)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    return model, {\"mse\": mse, \"rmse\": rmse}\n",
    "\n",
    "\n",
    "model, scores = train_quadratic_regressor(X_train, X_test, y_train, y_test)\n",
    "print(scores)\n",
    "print(f\"intercept: {model.intercept_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_of_train = [0.25, 0.50, 0.75, 1]\n",
    "for pct in pct_of_train:\n",
    "    num_train = int(len(X_train) * pct)\n",
    "    _, scores = train_quadratic_regressor(\n",
    "        X_train[:num_train], X_test, y_train[:num_train], y_test\n",
    "    )\n",
    "    print(num_train, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression (statsmodels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "def train_linear_regressor_statsmodels(X_train, X_test, y_train, y_test):\n",
    "    X_train_sm = sm.add_constant(X_train)  # Add a constant (intercept)\n",
    "    X_test_sm = sm.add_constant(X_test)\n",
    "\n",
    "    model = sm.OLS(y_train, X_train_sm).fit()  # Fit the model\n",
    "\n",
    "    y_pred = model.predict(X_test_sm)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    return model, {\"mse\": mse, \"rmse\": rmse}\n",
    "\n",
    "\n",
    "model, scores = train_linear_regressor_statsmodels(X_train, X_test, y_train, y_test)\n",
    "print(scores)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def calculate_vif(X):\n",
    "    # Add a constant (intercept) if needed\n",
    "    X_with_const = sm.add_constant(X)\n",
    "\n",
    "    # Create a DataFrame for VIF values\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"feature\"] = X_with_const.columns\n",
    "\n",
    "    # Calculate VIF for each feature\n",
    "    vif_data[\"VIF\"] = [\n",
    "        variance_inflation_factor(X_with_const.values, i)\n",
    "        for i in range(X_with_const.shape[1])\n",
    "    ]\n",
    "\n",
    "    return vif_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vif_df = calculate_vif(X_train)\n",
    "print(vif_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vif_df.sort_values(by=\"VIF\", ascending=False).dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "def train_ridge_regression(X_train, X_test, y_train, y_test, alpha=1.0):\n",
    "    \"\"\"\n",
    "    Train a Ridge regression model with data scaling and evaluation.\n",
    "\n",
    "    Args:\n",
    "        X_train (array-like): Training feature matrix.\n",
    "        X_test (array-like): Testing feature matrix.\n",
    "        y_train (array-like): Training target variable.\n",
    "        y_test (array-like): Testing target variable.\n",
    "        alpha (float): Regularization strength (default=1.0).\n",
    "\n",
    "    Returns:\n",
    "        model: Trained Ridge regression model.\n",
    "        results: Dictionary with model performance metrics.\n",
    "    \"\"\"\n",
    "    # Standardize the features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Initialize and train the Ridge regression model\n",
    "    model = Ridge(alpha=alpha)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Make predictions and evaluate the model\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    results = {\n",
    "        \"mse\": mse,\n",
    "        \"r2\": r2,\n",
    "        \"coefficients\": model.coef_,\n",
    "        \"intercept\": model.intercept_,\n",
    "    }\n",
    "\n",
    "    return model, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, results = train_ridge_regression(X_train, X_test, y_train, y_test, alpha=0.5)\n",
    "\n",
    "print(\"Model performance metrics:\")\n",
    "print(f\"MSE: {results['mse']:.4f}\")\n",
    "print(f\"R^2: {results['r2']:.4f}\")\n",
    "print(f\"Coefficients: {results['coefficients']}\")\n",
    "print(f\"Intercept: {results['intercept']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, combinations = sample_feature_combinations(\n",
    "    meta_analyzer_n_samples=2000, max_number=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_df = pd.DataFrame(0, index=np.arange(len(combinations)), columns=X_train.columns)\n",
    "for idx, combination in tqdm_notebook(enumerate(combinations), total=len(combinations)):\n",
    "    activated_feats = []\n",
    "    for feat in combination:\n",
    "        if \"analyzer\" in feat:\n",
    "            feature_name_str, value_str = feat.split(\"::\")[1].split(\"|\")\n",
    "            feature_name, value = (\n",
    "                feature_name_str.split(\"=\")[-1],\n",
    "                value_str.split(\"=\")[-1],\n",
    "            )\n",
    "            activated_feats.append(f\"{feature_name}={value}\")\n",
    "        else:\n",
    "            activated_feats.append(feat.replace(\"::\", \"__\"))\n",
    "    sim_df.loc[idx, activated_feats] = 1\n",
    "sim_df = sim_df.apply(\n",
    "    lambda row: row.map(lambda x: feat_map.get(row.name, 1) if x == 1 else x)\n",
    ").dropna(axis=1, how=\"any\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_results = sim_df.copy(deep=True)\n",
    "sim_results[\"activated_features\"] = sim_results.apply(\n",
    "    lambda row: [col for col in sim_results.columns if row[col] != 0], axis=1\n",
    ")\n",
    "sim_results[\"pred\"] = model.predict(sim_df)\n",
    "sim_results = sim_results.sort_values(by=\"pred\", ascending=False).reset_index(drop=True)\n",
    "sim_results[\"hash\"] = sim_results[\"activated_features\"].apply(\n",
    "    lambda x: hashlib.md5(\"___\".join(x).encode(\"utf-8\")).hexdigest()\n",
    ")\n",
    "sim_results = sim_results.drop_duplicates(subset=[\"hash\"]).reset_index(drop=True)\n",
    "sim_results[[\"activated_features\", \"pred\"]].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do some evals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_csv(\"../../data/top.csv\")\n",
    "actual_scores = results_df[\"Overall\"]\n",
    "\n",
    "results_df = results_df.head(16)\n",
    "\n",
    "\n",
    "def is_binary(series):\n",
    "    return set(series.dropna().unique()).issubset({0, 1})\n",
    "\n",
    "\n",
    "def update_name(name):\n",
    "    if \"feature_name\" in name:\n",
    "        _, feature = name.split(\"__\")\n",
    "        name_str, val_str = feature.split(\"|\")\n",
    "        _, x = name_str.split(\"=\")\n",
    "        _, y = val_str.split(\"=\")\n",
    "        return f\"{x}={y}\"\n",
    "\n",
    "    else:\n",
    "        return name\n",
    "\n",
    "\n",
    "binary_columns = [\n",
    "    col for col in results_df.columns if is_binary(results_df[col]) and col != \"label\"\n",
    "]\n",
    "results_df = results_df[binary_columns]\n",
    "results_df = results_df.rename(\n",
    "    columns={col: update_name(col) for col in binary_columns}\n",
    ")\n",
    "results_df = results_df.rename(\n",
    "    columns={\n",
    "        \"token_len_diff__min_val=0.0|max_val=0.33\": \"token_len_difference__min_val=0.0|max_val=0.33\",\n",
    "        \"token_len_diff__min_val=0.33|max_val=0.67\": \"token_len_difference__min_val=0.33|max_val=0.67\",\n",
    "        \"token_len_diff__min_val=0.67|max_val=1.0\": \"token_len_difference__min_val=0.67|max_val=1.0\",\n",
    "    }\n",
    ")\n",
    "\n",
    "results_df = results_df.apply(\n",
    "    lambda row: row.map(lambda x: feat_map.get(row.name, 1) if x == 1 else x)\n",
    ").dropna(axis=1, how=\"any\")\n",
    "\n",
    "\n",
    "for unused_feat in list(set(sim_df.columns) - set(results_df.columns)):\n",
    "    results_df[unused_feat] = 0\n",
    "\n",
    "results_df = results_df.reindex(sorted(results_df.columns), axis=1)\n",
    "\n",
    "results_df[\"predicted_scores\"] = model.predict(results_df)\n",
    "\n",
    "results_df[\"activated_features\"] = results_df.apply(\n",
    "    lambda row: [\n",
    "        col\n",
    "        for col in results_df.columns\n",
    "        if row[col] != 0 and col not in (\"predicted_scores\")\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "results_df[\"actual_scores\"] = actual_scores\n",
    "\n",
    "results_df[\"actual_scores_rank\"] = results_df[\"actual_scores\"].rank(ascending=False)\n",
    "results_df[\"predicted_scores_rank\"] = results_df[\"predicted_scores\"].rank(\n",
    "    ascending=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[\n",
    "    [\n",
    "        \"activated_features\",\n",
    "        \"actual_scores\",\n",
    "        \"actual_scores_rank\",\n",
    "        \"predicted_scores\",\n",
    "        \"predicted_scores_rank\",\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mse = mean_squared_error(results_df[\"actual_scores\"], results_df[\"predicted_scores\"])\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# Compute Spearman's rho\n",
    "rho, p_value = spearmanr(results_df[\"actual_scores\"], results_df[\"predicted_scores\"])\n",
    "\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"Spearman's rho: {rho:.4f}\")\n",
    "print(f\"p-value: {p_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = 100\n",
    "human_score = 0.715\n",
    "better_than_humans = sim_results[sim_results[\"pred\"] > human_score]\n",
    "top_combinations = (\n",
    "    better_than_humans.activated_features.head(top_n).drop_duplicates().to_list()\n",
    ")\n",
    "print(top_combinations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now you have determined 10 feature combinations that seem to work well. The next step is to train RMs and evaluate them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from beaker import Beaker, ExperimentSpec\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_beaker_experiments(\n",
    "    combinations, *, template=\"../../beaker/template.yml\", output_file=\"experiments.yml\"\n",
    "):\n",
    "    spec = ExperimentSpec.from_file(template)\n",
    "    exp_spec = deepcopy(spec)\n",
    "    template_task = exp_spec.tasks.pop(0)\n",
    "\n",
    "    new_tasks = []\n",
    "    for idx, combination in enumerate(combinations):\n",
    "        feats_to_run = []\n",
    "        for feat in combination:\n",
    "            if \"min_val\" in feat:\n",
    "                if \"token_len_difference\" in feat:\n",
    "                    feat = feat.replace(\"difference\", \"diff\")\n",
    "                feats_to_run.append(feat.replace(\"__\", \"::\"))\n",
    "            else:\n",
    "                feat_name, value = feat.split(\"=\")\n",
    "                category = find_meta_category(feat_name)\n",
    "                if category == \"closed_set\":\n",
    "                    key = \"constraints\"\n",
    "                elif category == \"scalar\":\n",
    "                    key = \"value\"\n",
    "                elif category == \"open_set\":\n",
    "                    key = \"check_for_existence\"\n",
    "                feats_to_run.append(\n",
    "                    f\"{category}::feature_name={feat_name}|{key}={value}\"\n",
    "                )\n",
    "        # Create beaker task\n",
    "        task = deepcopy(template_task)\n",
    "        task.name = f\"get-features-datamodel-{idx}\"\n",
    "        task.arguments.extend([\"--features\"] + feats_to_run)\n",
    "        new_tasks.append(task)\n",
    "\n",
    "    exp_spec.tasks = new_tasks\n",
    "    exp_spec.validate()\n",
    "    exp_spec.to_file(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_beaker_experiments(top_combinations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get finished jobs and download the subsets and create an `experiments.txt` file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_experiments_file(\n",
    "    beaker_experiment_id: str, output_path: Path, cache_dir: Path\n",
    "):\n",
    "    beaker = Beaker.from_env(\"ai2/ljm-oe-adapt\")\n",
    "    experiment = beaker.experiment.get(beaker_experiment_id)\n",
    "\n",
    "    experiment_ids = []\n",
    "    for job in tqdm_notebook(experiment.jobs):\n",
    "        if job.is_done:\n",
    "            # Get output\n",
    "            dataset_id = job.execution.result.beaker\n",
    "            beaker.dataset.fetch(\n",
    "                dataset_id,\n",
    "                force=True,\n",
    "                target=cache_dir,\n",
    "                prefix=\"data/\",\n",
    "                quiet=True,\n",
    "            )\n",
    "\n",
    "            beaker.dataset.fetch(\n",
    "                dataset_id,\n",
    "                force=True,\n",
    "                target=cache_dir,\n",
    "                prefix=\"experiments.txt\",\n",
    "                quiet=True,\n",
    "            )\n",
    "\n",
    "            if (cache_dir / \"experiments.txt\").exists():\n",
    "                with open(cache_dir / \"experiments.txt\", \"r\") as f:\n",
    "                    data = f.read().splitlines()\n",
    "                    if data:\n",
    "                        id = data[0]\n",
    "                        experiment_ids.append(id)\n",
    "                    else:\n",
    "                        print(f\"No data found in cache for {job}\")\n",
    "\n",
    "    print(experiment_ids)\n",
    "    with open(output_path, \"a\") as f:\n",
    "        for id in set(experiment_ids):\n",
    "            f.write(\"\\n\" + id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment_id = \"01J6TS47Q2KNKYRCYHC8A0DE4B\"\n",
    "# experiment_id = \"01J6WDKDPQCM92REXJ1VCNJ0NW\"\n",
    "# experiment_id = \"01J6XJSWMSAM2ARJ1WXAP6PV8T\"\n",
    "experiment_id = \"01J6Z8EX5D9M0EBER108Z4HJ5B\"\n",
    "top_subsets_dir = Path(\"top_n_subsets\")\n",
    "top_subsets_dir.mkdir(parents=True, exist_ok=True)\n",
    "experiments_file = top_subsets_dir / \"top_n_subsets_experiments.txt\"\n",
    "\n",
    "create_experiments_file(experiment_id, experiments_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the feature combination that maximizes the score\n",
    "\n",
    "Here, we try different approaches to get the best features. Let's see how that works.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = (3, 5, 7, 9, 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive Feature Elimination\n",
    "\n",
    "> Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "rfe = RFECV(\n",
    "    model,\n",
    "    step=1,\n",
    "    cv=4,  # StratifiedKFold(5),\n",
    "    scoring=\"r2\",\n",
    "    min_features_to_select=3,\n",
    ")\n",
    "rfe.fit(X_train, y_train)\n",
    "rfe_features = list(X_train.columns[rfe.support_])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select features according to the k highest scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n",
    "\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "selections_kbest = {}\n",
    "for n in n_features:\n",
    "    selector = SelectKBest(score_func=f_regression, k=n)\n",
    "    selector.fit(X_train, y_train)\n",
    "    selected_features = np.array(X_train.columns)[selector.get_support()]\n",
    "    selections_kbest[n] = selected_features\n",
    "\n",
    "selections_kbest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.feature_selection import SelectFromModel, SelectKBest\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"feature_selection\", SelectKBest(score_func=mutual_info_regression)),\n",
    "        # (\"feature_selection\", SelectFromModel(model)),\n",
    "        (\"model\", model),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# param_grid = {\n",
    "#     \"feature_selection__threshold\": [\"mean\", \"median\"]\n",
    "# }\n",
    "\n",
    "param_grid = {\n",
    "    \"feature_selection__k\": [2, 3, 5, 10, 15]  # Example values for top-n features\n",
    "}\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring=\"r2\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_k = grid_search.best_params_[\"feature_selection__k\"]\n",
    "best_features = grid_search.best_estimator_.named_steps[\n",
    "    \"feature_selection\"\n",
    "].get_support()\n",
    "selected_features = np.array(X_train.columns)[best_features]\n",
    "print(selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine all these top features into one experiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_combinations = (\n",
    "    [rfe_features]\n",
    "    # list(selections_rfe.values())\n",
    "    + list(selections_kbest.values())\n",
    "    + [selected_features]\n",
    ")\n",
    "\n",
    "create_beaker_experiments(\n",
    "    best_combinations,\n",
    "    output_file=\"experiments_best.yml\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment_id = \"01J6ZBPHM8GG5M560PPBFFYB28\"\n",
    "experiment_id = \"01J70785358MQTVAQYGVW0CY3X\"\n",
    "top_subsets_dir = Path(\"best_feature_combinations\")\n",
    "top_subsets_dir.mkdir(parents=True, exist_ok=True)\n",
    "experiments_file = top_subsets_dir / \"best_features_experiments.txt\"\n",
    "\n",
    "create_experiments_file(experiment_id, experiments_file, cache_dir=top_subsets_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(best_combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
